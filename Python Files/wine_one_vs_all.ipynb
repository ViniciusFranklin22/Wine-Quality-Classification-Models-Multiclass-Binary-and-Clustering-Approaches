{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_wine = 'C://Users//vinic//Documents//Python//Machine Learning//Projetos UCI ML//Wine Quality//wine+quality//winequality-red.csv'\n",
    "white_wine = 'C://Users//vinic//Documents//Python//Machine Learning//Projetos UCI ML//Wine Quality//wine+quality//winequality-white.csv'\n",
    "data_red_wine = pd.read_csv(red_wine,header=0,sep=';')\n",
    "data_white_wine = pd.read_csv(white_wine,header=0,sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_white_wine' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdata_white_wine\u001b[49m\u001b[38;5;241m.\u001b[39misna()\u001b[38;5;241m.\u001b[39msum()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data_white_wine' is not defined"
     ]
    }
   ],
   "source": [
    "data_white_wine.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 6, 7, 4, 8, 3], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_red_wine['quality'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1599"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_red_wine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 5, 7, 8, 4, 3, 9], dtype=int64)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_white_wine['quality'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4898"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_white_wine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Validation and test Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataset(dataframe,oversample = False):\n",
    "    X = dataframe[dataframe.columns[:-1]].values\n",
    "    y = dataframe[dataframe.columns[-1]].values\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    #Scale the number, so we dont have  huge discrepancy between columns, it affects the model\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    #the difference between the len of the values must not be huge, so we have to scale it\n",
    "    #oversample the one that has the least, taking more of the less class\n",
    "    \n",
    "    if oversample:\n",
    "        ros = RandomOverSampler(random_state=42)\n",
    "        X,y = ros.fit_resample(X,y)\n",
    "        \n",
    "    # concat 2 arrays, y is only one dimension so we have to make it 2 \n",
    "    #in this function using -1 is the same as len(y)\n",
    "    #same as concat in pandas put hstack in numpay\n",
    "    df = np.hstack((X,np.reshape(y,(-1,1))))\n",
    "    \n",
    "    return df,X,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## divide the data so we can do the one vs all model\n",
    "### errado, tem que fazer isso depois de obter o test, train dataset\n",
    "# red wine\n",
    "#data_red_wine_one_vs_all = {}\n",
    "#for i in np.sort(data_red_wine['quality'].unique()):\n",
    "#    data_red_wine_one_vs_all[f'{i}'] = {}\n",
    "#    temp = data_red_wine.copy()\n",
    "#    temp['quality'] = (temp['quality'] == i).astype(int)\n",
    "#    data_red_wine_one_vs_all[f'{i}']['data_frame'] = temp\n",
    "#\n",
    "## white wine\n",
    "#data_white_wine_one_vs_all = {}\n",
    "#for i in np.sort(data_white_wine['quality'].unique()):\n",
    "#    data_white_wine_one_vs_all[f'{i}'] = {}\n",
    "#    temp = data_white_wine.copy()\n",
    "#    temp['quality'] = (temp['quality'] == i).astype(int)\n",
    "#    data_white_wine_one_vs_all[f'{i}']['data_frame'] = temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vinic\\anaconda3\\envs\\myenv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "# red wine\n",
    "data_red_wine_one_vs_all = {}\n",
    "# 1. Split fixo único\n",
    "# Split fixo único baseado no conjunto completo\n",
    "red_wine_train, red_wine_valid, red_wine_test = np.split(\n",
    "    data_red_wine.sample(frac=1, random_state=42), \n",
    "    [int(0.6 * len(data_red_wine)), int(0.8 * len(data_red_wine))]\n",
    ")\n",
    "\n",
    "# 2. Gerar datasets one-vs-all a partir do mesmo conjunto\n",
    "for i in sorted(data_red_wine['quality'].unique()):\n",
    "    # Transforma o label em binário\n",
    "    train_bin = red_wine_train.copy()\n",
    "    valid_bin = red_wine_valid.copy()\n",
    "    test_bin = red_wine_test.copy()\n",
    "    \n",
    "    train_bin['quality'] = (train_bin['quality'] == i).astype(int)\n",
    "    valid_bin['quality'] = (valid_bin['quality'] == i).astype(int)\n",
    "    test_bin['quality'] = (test_bin['quality'] == i).astype(int)\n",
    "\n",
    "    # Escala e armazena\n",
    "    red_train, red_X_train, red_y_train = scale_dataset(train_bin, oversample=True)\n",
    "    red_valid, red_X_valid, red_y_valid = scale_dataset(valid_bin, oversample=False)\n",
    "    red_test, red_X_test, red_y_test = scale_dataset(test_bin, oversample=False)\n",
    "\n",
    "    data_red_wine_one_vs_all[f'{i}'] = {\n",
    "        'red_wine_train': red_train,\n",
    "        'red_wine_X_train': red_X_train,\n",
    "        'red_wine_y_train': red_y_train,\n",
    "        'red_wine_valid':red_valid,\n",
    "        'red_wine_X_valid':red_X_valid,\n",
    "        'red_wine_y_valid':red_y_valid,\n",
    "        'red_wine_test': red_test,\n",
    "        'red_wine_X_test': red_X_test,\n",
    "        'red_wine_y_test': red_y_test\n",
    "    }\n",
    "\n",
    "# white wine\n",
    "\n",
    "data_white_wine_one_vs_all = {}\n",
    "\n",
    "# 1. Split fixo único baseado no conjunto completo\n",
    "white_wine_train, white_wine_test = train_test_split(\n",
    "    data_white_wine, test_size=0.2, stratify=data_white_wine['quality'], random_state=42\n",
    ")\n",
    "\n",
    "# 2. Gerar datasets one-vs-all a partir do mesmo conjunto\n",
    "for i in sorted(data_white_wine['quality'].unique()):\n",
    "    # Transforma o label em binário\n",
    "    train_bin = white_wine_train.copy()\n",
    "    test_bin = white_wine_test.copy()\n",
    "    \n",
    "    train_bin['quality'] = (train_bin['quality'] == i).astype(int)\n",
    "    test_bin['quality'] = (test_bin['quality'] == i).astype(int)\n",
    "\n",
    "    # Escala e armazena\n",
    "    white_train, white_X_train, white_y_train = scale_dataset(train_bin, oversample=True)\n",
    "    white_test, white_X_test, white_y_test = scale_dataset(test_bin, oversample=False)\n",
    "\n",
    "    data_white_wine_one_vs_all[f'{i}'] = {\n",
    "        'white_wine_train': white_train,\n",
    "        'white_wine_X_train': white_X_train,\n",
    "        'white_wine_y_train': white_y_train,\n",
    "        'white_wine_test': white_test,\n",
    "        'white_wine_X_test': white_X_test,\n",
    "        'white_wine_y_test': white_y_test\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the Quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-nearest neighbors - KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 6, 7, 4, 8, 3], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_red_wine['quality'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10, 12, 14,  8, 16,  6], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_red_wine['quality'].unique()+data_red_wine['quality'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the Red Wine Knn para Qualidade-> 3 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       317\n",
      "           1       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.99       320\n",
      "   macro avg       0.50      0.50      0.50       320\n",
      "weighted avg       0.98      0.99      0.98       320\n",
      "\n",
      "Number of FP 1\n",
      "Number of TP 0\n",
      "Number of TP 0\n",
      "1\n",
      "Confusion Matrix para classe 3:\n",
      "[[316   1]\n",
      " [  3   0]]\n",
      "{'TN': 316, 'FP': 1, 'FN': 3, 'TP': 0}\n",
      "For the Red Wine Knn para Qualidade-> 4 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98       310\n",
      "           1       0.25      0.20      0.22        10\n",
      "\n",
      "    accuracy                           0.96       320\n",
      "   macro avg       0.61      0.59      0.60       320\n",
      "weighted avg       0.95      0.96      0.95       320\n",
      "\n",
      "Number of FP 6\n",
      "Number of TP 2\n",
      "Number of TP 2\n",
      "9\n",
      "Confusion Matrix para classe 4:\n",
      "[[304   6]\n",
      " [  8   2]]\n",
      "{'TN': 304, 'FP': 6, 'FN': 8, 'TP': 2}\n",
      "For the Red Wine Knn para Qualidade-> 5 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.75      0.77       193\n",
      "           1       0.64      0.70      0.67       127\n",
      "\n",
      "    accuracy                           0.73       320\n",
      "   macro avg       0.72      0.72      0.72       320\n",
      "weighted avg       0.73      0.73      0.73       320\n",
      "\n",
      "Number of FP 49\n",
      "Number of TP 89\n",
      "Number of TP 89\n",
      "147\n",
      "Confusion Matrix para classe 5:\n",
      "[[144  49]\n",
      " [ 38  89]]\n",
      "{'TN': 144, 'FP': 49, 'FN': 38, 'TP': 89}\n",
      "For the Red Wine Knn para Qualidade-> 6 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.74      0.73       179\n",
      "           1       0.65      0.62      0.64       141\n",
      "\n",
      "    accuracy                           0.69       320\n",
      "   macro avg       0.68      0.68      0.68       320\n",
      "weighted avg       0.69      0.69      0.69       320\n",
      "\n",
      "Number of FP 47\n",
      "Number of TP 88\n",
      "Number of TP 88\n",
      "282\n",
      "Confusion Matrix para classe 6:\n",
      "[[132  47]\n",
      " [ 53  88]]\n",
      "{'TN': 132, 'FP': 47, 'FN': 53, 'TP': 88}\n",
      "For the Red Wine Knn para Qualidade-> 7 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.93      0.92       282\n",
      "           1       0.43      0.42      0.43        38\n",
      "\n",
      "    accuracy                           0.87       320\n",
      "   macro avg       0.68      0.67      0.68       320\n",
      "weighted avg       0.86      0.87      0.86       320\n",
      "\n",
      "Number of FP 21\n",
      "Number of TP 16\n",
      "Number of TP 16\n",
      "319\n",
      "Confusion Matrix para classe 7:\n",
      "[[261  21]\n",
      " [ 22  16]]\n",
      "{'TN': 261, 'FP': 21, 'FN': 22, 'TP': 16}\n",
      "For the Red Wine Knn para Qualidade-> 8 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       319\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.99       320\n",
      "   macro avg       0.50      0.50      0.50       320\n",
      "weighted avg       0.99      0.99      0.99       320\n",
      "\n",
      "Number of FP 1\n",
      "Number of TP 0\n",
      "Number of TP 0\n",
      "320\n",
      "Confusion Matrix para classe 8:\n",
      "[[318   1]\n",
      " [  1   0]]\n",
      "{'TN': 318, 'FP': 1, 'FN': 1, 'TP': 0}\n",
      "Para a analise total temos as medias:\n",
      "Para macro avg ->  {'precision macro avg': 0.32986863675269473, 'recall macro avg': 0.3243255847218426, 'f1-score macro avg': 0.32637802691945433}\n",
      "Para weighted avg ->  {'precision weighted avg': 0.6023416895156026, 'recall weighted avg': 0.609375, 'f1-score weighted avg': 0.6051695606599217}\n",
      "Numero de amostrar que deram true em algum momento:  320\n",
      "Pela formula:  0\n",
      "Numero de Amostras que sao classificadas:  320\n",
      "Amostras repetidas pelo menos uma vez 0\n",
      "Numero real de amostras no dataset teste:  320\n",
      "VALORES GERAIS -> FP:  125 FN:  125 TN  1475 TP 195\n",
      "Overall-> Precision: 0.609375 | Recall: 0.609375|F1-Score: 0.609375\n",
      "Overall Accuracy:  0.8697916666666666\n"
     ]
    }
   ],
   "source": [
    "## red wine\n",
    "class_rep = {}\n",
    "number_samples_final = 0\n",
    "predicted_as_1_in_any_model = np.zeros_like(data_red_wine_one_vs_all['6']['red_wine_y_test'], dtype=int)\n",
    "fpp = 0\n",
    "fnn = 0\n",
    "tnn = 0\n",
    "tpp = 0\n",
    "\n",
    "for i in np.sort(data_red_wine['quality'].unique()):\n",
    "    red_wine_knn_model = KNeighborsClassifier(n_neighbors=1)\n",
    "    red_wine_knn_model.fit(data_red_wine_one_vs_all[f'{i}']['red_wine_X_train'],data_red_wine_one_vs_all[f'{i}']['red_wine_y_train'])\n",
    "    #use the knn model to try to predict the test dataset\n",
    "    red_wine_y_pred = red_wine_knn_model.predict(data_red_wine_one_vs_all[f'{i}']['red_wine_X_test'])\n",
    "    ## for better understanding of precition and recall\n",
    "    ## https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "    print('For the Red Wine Knn para Qualidade->',i,'\\n',classification_report(data_red_wine_one_vs_all[f'{i}']['red_wine_y_test'],red_wine_y_pred))\n",
    "    class_rep[f'{i}'] = classification_report(data_red_wine_one_vs_all[f'{i}']['red_wine_y_test'],red_wine_y_pred,output_dict=True)\n",
    "    \n",
    "    number_samples_final += np.sum(red_wine_y_pred == 1)\n",
    "    #print(red_wine_y_pred)\n",
    "    predicted_as_1_in_any_model = predicted_as_1_in_any_model + red_wine_y_pred\n",
    "    print(\"Number of FP\",np.sum((~data_red_wine_one_vs_all[f'{i}']['red_wine_y_test'] & red_wine_y_pred)==1))\n",
    "    print(\"Number of TP\",np.sum((data_red_wine_one_vs_all[f'{i}']['red_wine_y_test']+red_wine_y_pred)>1))\n",
    "    print(\"Number of TP\",np.sum((data_red_wine_one_vs_all[f'{i}']['red_wine_y_test']&red_wine_y_pred)==1))\n",
    "    #print(predicted_as_1_in_any_model)\n",
    "    print(number_samples_final)\n",
    "    cm = confusion_matrix(data_red_wine_one_vs_all[f'{i}']['red_wine_y_test'], red_wine_y_pred)\n",
    "    print(f\"Confusion Matrix para classe {i}:\\n{cm}\")\n",
    "    tn, fp, fn, tp = confusion_matrix(data_red_wine_one_vs_all[f'{i}']['red_wine_y_test'], red_wine_y_pred).ravel()\n",
    "    print({'TN': tn, 'FP': fp, 'FN': fn, 'TP': tp})\n",
    "    fpp = fpp + fp\n",
    "    fnn = fn + fnn\n",
    "    tnn = tn + tnn\n",
    "    tpp = tpp + tp\n",
    "\n",
    "\n",
    "\n",
    "## tabela juntando os valores previstos como verdadeiros\n",
    "macro_avgs = {\"precision macro avg\":0,\"recall macro avg\":0,\"f1-score macro avg\":0}\n",
    "weighted_avg = {\"precision weighted avg\":0,\"recall weighted avg\":0,\"f1-score weighted avg\":0}\n",
    "number_samples = 0\n",
    "calculo_numero_amostras  = 0\n",
    "\n",
    "for i in np.sort(data_red_wine['quality'].unique()):\n",
    "    macro_avgs[\"precision macro avg\"] = macro_avgs[\"precision macro avg\"] + class_rep[f'{i}'][\"1\"][\"precision\"]\n",
    "    weighted_avg[\"precision weighted avg\"] = weighted_avg[\"precision weighted avg\"] + class_rep[f'{i}'][\"1\"][\"precision\"]*class_rep[f'{i}'][\"1\"][\"support\"]\n",
    "    macro_avgs[\"recall macro avg\"] = macro_avgs[\"recall macro avg\"] + class_rep[f'{i}'][\"1\"][\"recall\"]\n",
    "    weighted_avg[\"recall weighted avg\"] = weighted_avg[\"recall weighted avg\"] + class_rep[f'{i}'][\"1\"][\"recall\"]*class_rep[f'{i}'][\"1\"][\"support\"]    \n",
    "    macro_avgs[\"f1-score macro avg\"] = macro_avgs[\"f1-score macro avg\"] + class_rep[f'{i}'][\"1\"][\"f1-score\"]\n",
    "    weighted_avg[\"f1-score weighted avg\"] = weighted_avg[\"f1-score weighted avg\"] + class_rep[f'{i}'][\"1\"][\"f1-score\"]*class_rep[f'{i}'][\"1\"][\"support\"]\n",
    "    number_samples = number_samples + class_rep[f'{i}'][\"1\"][\"support\"]\n",
    "#    ## pegar o numero de samples que deram true em algum momento\n",
    "#    tp = class_rep[f'{i}'][\"1\"][\"support\"]*class_rep[f'{i}'][\"1\"][\"recall\"]\n",
    "#    print(\"precision\",class_rep[f'{i}'][\"1\"][\"precision\"])\n",
    "#    print(\"recall\",class_rep[f'{i}'][\"1\"][\"recall\"])\n",
    "#    if class_rep[f'{i}'][\"1\"][\"precision\"]> 0:\n",
    "#        predicted_positive  = tp/(class_rep[f'{i}'][\"1\"][\"precision\"])\n",
    "#        print(predicted_positive)\n",
    "#        calculo_numero_amostras += predicted_positive\n",
    "#\n",
    "for i in macro_avgs:\n",
    "    macro_avgs[i] = macro_avgs[i]/len(np.sort(data_red_wine['quality'].unique()))\n",
    "\n",
    "for i in weighted_avg:\n",
    "    weighted_avg[i] = weighted_avg[i]/number_samples\n",
    "\n",
    "print(\"Para a analise total temos as medias:\")\n",
    "print(\"Para macro avg -> \",macro_avgs)\n",
    "print(\"Para weighted avg -> \",weighted_avg)\n",
    "print(\"Numero de amostrar que deram true em algum momento: \",number_samples_final)\n",
    "print(\"Pela formula: \",calculo_numero_amostras)\n",
    "print(\"Numero de Amostras que sao classificadas: \",tpp+fpp)\n",
    "print(\"Amostras repetidas pelo menos uma vez\",np.sum(predicted_as_1_in_any_model>1))\n",
    "print(\"Numero real de amostras no dataset teste: \",len(data_red_wine_one_vs_all['6']['red_wine_y_test']))\n",
    "print('VALORES GERAIS -> FP: ',fpp,'FN: ',fnn,'TN ',tnn,\"TP\",tpp)\n",
    "print(f\"Overall-> Precision: {tpp/(tpp+fpp)} | Recall: {tpp/(tpp+fnn)}|F1-Score: {2/((1/(tpp/(tpp+fpp))) + (1/(tpp/(tpp+fnn))))}\")\n",
    "print(\"Overall Accuracy: \",(tpp+tnn)/(tpp+tnn+fpp+fnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the White Wine Knn para Qualidade-> 3 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       976\n",
      "           1       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.99       980\n",
      "   macro avg       0.50      0.50      0.50       980\n",
      "weighted avg       0.99      0.99      0.99       980\n",
      "\n",
      "Number of FP 1\n",
      "Number of TP 0\n",
      "Number of TP 0\n",
      "1\n",
      "Confusion Matrix para classe 3:\n",
      "[[975   1]\n",
      " [  4   0]]\n",
      "{'TN': 975, 'FP': 1, 'FN': 4, 'TP': 0}\n",
      "For the White Wine Knn para Qualidade-> 4 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       947\n",
      "           1       0.37      0.30      0.33        33\n",
      "\n",
      "    accuracy                           0.96       980\n",
      "   macro avg       0.67      0.64      0.66       980\n",
      "weighted avg       0.96      0.96      0.96       980\n",
      "\n",
      "Number of FP 17\n",
      "Number of TP 10\n",
      "Number of TP 10\n",
      "28\n",
      "Confusion Matrix para classe 4:\n",
      "[[930  17]\n",
      " [ 23  10]]\n",
      "{'TN': 930, 'FP': 17, 'FN': 23, 'TP': 10}\n",
      "For the White Wine Knn para Qualidade-> 5 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.87      0.85       689\n",
      "           1       0.66      0.60      0.63       291\n",
      "\n",
      "    accuracy                           0.79       980\n",
      "   macro avg       0.75      0.74      0.74       980\n",
      "weighted avg       0.79      0.79      0.79       980\n",
      "\n",
      "Number of FP 89\n",
      "Number of TP 175\n",
      "Number of TP 175\n",
      "292\n",
      "Confusion Matrix para classe 5:\n",
      "[[600  89]\n",
      " [116 175]]\n",
      "{'TN': 600, 'FP': 89, 'FN': 116, 'TP': 175}\n",
      "For the White Wine Knn para Qualidade-> 6 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.70      0.71       540\n",
      "           1       0.65      0.67      0.66       440\n",
      "\n",
      "    accuracy                           0.69       980\n",
      "   macro avg       0.68      0.68      0.68       980\n",
      "weighted avg       0.69      0.69      0.69       980\n",
      "\n",
      "Number of FP 160\n",
      "Number of TP 293\n",
      "Number of TP 293\n",
      "745\n",
      "Confusion Matrix para classe 6:\n",
      "[[380 160]\n",
      " [147 293]]\n",
      "{'TN': 380, 'FP': 160, 'FN': 147, 'TP': 293}\n",
      "For the White Wine Knn para Qualidade-> 7 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.90      0.90       804\n",
      "           1       0.55      0.56      0.55       176\n",
      "\n",
      "    accuracy                           0.84       980\n",
      "   macro avg       0.73      0.73      0.73       980\n",
      "weighted avg       0.84      0.84      0.84       980\n",
      "\n",
      "Number of FP 82\n",
      "Number of TP 99\n",
      "Number of TP 99\n",
      "926\n",
      "Confusion Matrix para classe 7:\n",
      "[[722  82]\n",
      " [ 77  99]]\n",
      "{'TN': 722, 'FP': 82, 'FN': 77, 'TP': 99}\n",
      "For the White Wine Knn para Qualidade-> 8 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98       945\n",
      "           1       0.44      0.66      0.53        35\n",
      "\n",
      "    accuracy                           0.96       980\n",
      "   macro avg       0.71      0.81      0.75       980\n",
      "weighted avg       0.97      0.96      0.96       980\n",
      "\n",
      "Number of FP 29\n",
      "Number of TP 23\n",
      "Number of TP 23\n",
      "978\n",
      "Confusion Matrix para classe 8:\n",
      "[[916  29]\n",
      " [ 12  23]]\n",
      "{'TN': 916, 'FP': 29, 'FN': 12, 'TP': 23}\n",
      "For the White Wine Knn para Qualidade-> 9 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       979\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           1.00       980\n",
      "   macro avg       0.50      0.50      0.50       980\n",
      "weighted avg       1.00      1.00      1.00       980\n",
      "\n",
      "Number of FP 2\n",
      "Number of TP 0\n",
      "Number of TP 0\n",
      "980\n",
      "Confusion Matrix para classe 9:\n",
      "[[977   2]\n",
      " [  1   0]]\n",
      "{'TN': 977, 'FP': 2, 'FN': 1, 'TP': 0}\n",
      "Para a analise total temos as medias:\n",
      "Para macro avg ->  {'precision macro avg': 0.38133104193164197, 'recall macro avg': 0.3985652602184266, 'f1-score macro avg': 0.3862194929266388}\n",
      "Para weighted avg ->  {'precision weighted avg': 0.6137321669131768, 'recall weighted avg': 0.6122448979591837, 'f1-score weighted avg': 0.6115992943690891}\n",
      "Numero de amostrar que deram true em algum momento:  980\n",
      "Pela formula:  0\n",
      "Numero de Amostras que sao classificadas:  980\n",
      "Amostras repetidas pelo menos uma vez 0\n",
      "Numero real de amostras no dataset teste:  980\n",
      "VALORES GERAIS -> FP:  380 FN:  380 TN  5500 TP 600\n",
      "Overall-> Precision: 0.6122448979591837 | Recall: 0.6122448979591837|F1-Score: 0.6122448979591837\n",
      "Overall Accuracy:  0.8892128279883382\n"
     ]
    }
   ],
   "source": [
    "## white wine\n",
    "\n",
    "class_rep = {}\n",
    "number_samples_final = 0\n",
    "predicted_as_1_in_any_model = np.zeros_like(data_white_wine_one_vs_all['6']['white_wine_y_test'], dtype=int)\n",
    "fpp = 0\n",
    "fnn = 0\n",
    "tnn = 0\n",
    "tpp = 0\n",
    "\n",
    "for i in np.sort(data_white_wine['quality'].unique()):\n",
    "    white_wine_knn_model = KNeighborsClassifier(n_neighbors=1)\n",
    "    white_wine_knn_model.fit(data_white_wine_one_vs_all[f'{i}']['white_wine_X_train'],data_white_wine_one_vs_all[f'{i}']['white_wine_y_train'])\n",
    "    #use the knn model to try to pwhiteict the test dataset\n",
    "    white_wine_y_pred = white_wine_knn_model.predict(data_white_wine_one_vs_all[f'{i}']['white_wine_X_test'])\n",
    "    ## for better understanding of precition and recall\n",
    "    ## https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "    print('For the White Wine Knn para Qualidade->',i,'\\n',classification_report(data_white_wine_one_vs_all[f'{i}']['white_wine_y_test'],white_wine_y_pred))\n",
    "    class_rep[f'{i}'] = classification_report(data_white_wine_one_vs_all[f'{i}']['white_wine_y_test'],white_wine_y_pred,output_dict=True)\n",
    "    number_samples_final += np.sum(white_wine_y_pred == 1)\n",
    "    #print(white_wine_y_pred)\n",
    "    predicted_as_1_in_any_model = predicted_as_1_in_any_model + white_wine_y_pred\n",
    "    print(\"Number of FP\",np.sum((~data_white_wine_one_vs_all[f'{i}']['white_wine_y_test'] & white_wine_y_pred)==1))\n",
    "    print(\"Number of TP\",np.sum((data_white_wine_one_vs_all[f'{i}']['white_wine_y_test']+white_wine_y_pred)>1))\n",
    "    print(\"Number of TP\",np.sum((data_white_wine_one_vs_all[f'{i}']['white_wine_y_test']&white_wine_y_pred)==1))\n",
    "    #print(predicted_as_1_in_any_model)\n",
    "    print(number_samples_final)\n",
    "    cm = confusion_matrix(data_white_wine_one_vs_all[f'{i}']['white_wine_y_test'], white_wine_y_pred)\n",
    "    print(f\"Confusion Matrix para classe {i}:\\n{cm}\")\n",
    "    tn, fp, fn, tp = confusion_matrix(data_white_wine_one_vs_all[f'{i}']['white_wine_y_test'], white_wine_y_pred).ravel()\n",
    "    print({'TN': tn, 'FP': fp, 'FN': fn, 'TP': tp})\n",
    "    fpp = fpp + fp\n",
    "    fnn = fn + fnn\n",
    "    tnn = tn + tnn\n",
    "    tpp = tpp + tp    \n",
    "\n",
    "\n",
    "## tabela juntando os valores previstos como verdadeiros\n",
    "macro_avgs = {\"precision macro avg\":0,\"recall macro avg\":0,\"f1-score macro avg\":0}\n",
    "weighted_avg = {\"precision weighted avg\":0,\"recall weighted avg\":0,\"f1-score weighted avg\":0}\n",
    "number_samples= 0\n",
    "for i in np.sort(data_white_wine['quality'].unique()):\n",
    "    macro_avgs[\"precision macro avg\"] = macro_avgs[\"precision macro avg\"] + class_rep[f'{i}'][\"1\"][\"precision\"]\n",
    "    weighted_avg[\"precision weighted avg\"] = weighted_avg[\"precision weighted avg\"] + class_rep[f'{i}'][\"1\"][\"precision\"]*class_rep[f'{i}'][\"1\"][\"support\"]\n",
    "    macro_avgs[\"recall macro avg\"] = macro_avgs[\"recall macro avg\"] + class_rep[f'{i}'][\"1\"][\"recall\"]\n",
    "    weighted_avg[\"recall weighted avg\"] = weighted_avg[\"recall weighted avg\"] + class_rep[f'{i}'][\"1\"][\"recall\"]*class_rep[f'{i}'][\"1\"][\"support\"]    \n",
    "    macro_avgs[\"f1-score macro avg\"] = macro_avgs[\"f1-score macro avg\"] + class_rep[f'{i}'][\"1\"][\"f1-score\"]\n",
    "    weighted_avg[\"f1-score weighted avg\"] = weighted_avg[\"f1-score weighted avg\"] + class_rep[f'{i}'][\"1\"][\"f1-score\"]*class_rep[f'{i}'][\"1\"][\"support\"]\n",
    "    number_samples = number_samples + class_rep[f'{i}'][\"1\"][\"support\"]\n",
    "\n",
    "for i in macro_avgs:\n",
    "    macro_avgs[i] = macro_avgs[i]/len(np.sort(data_white_wine['quality'].unique()))\n",
    "\n",
    "for i in weighted_avg:\n",
    "    weighted_avg[i] = weighted_avg[i]/number_samples\n",
    "\n",
    "print(\"Para a analise total temos as medias:\")\n",
    "print(\"Para macro avg -> \",macro_avgs)\n",
    "print(\"Para weighted avg -> \",weighted_avg)\n",
    "print(\"Numero de amostrar que deram true em algum momento: \",number_samples_final)\n",
    "print(\"Pela formula: \",calculo_numero_amostras)\n",
    "print(\"Numero de Amostras que sao classificadas: \",tpp+fpp)\n",
    "print(\"Amostras repetidas pelo menos uma vez\",np.sum(predicted_as_1_in_any_model>1))\n",
    "print(\"Numero real de amostras no dataset teste: \",len(data_white_wine_one_vs_all['6']['white_wine_y_test']))\n",
    "print('VALORES GERAIS -> FP: ',fpp,'FN: ',fnn,'TN ',tnn,\"TP\",tpp)\n",
    "print(f\"Overall-> Precision: {tpp/(tpp+fpp)} | Recall: {tpp/(tpp+fnn)}|F1-Score: {2/((1/(tpp/(tpp+fpp))) + (1/(tpp/(tpp+fnn))))}\")\n",
    "print(\"Overall Accuracy: \",(tpp+tnn)/(tpp+tnn+fpp+fnn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the Red Wine NB para Qualidade-> 3 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.86      0.92       317\n",
      "           1       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.85       320\n",
      "   macro avg       0.49      0.43      0.46       320\n",
      "weighted avg       0.98      0.85      0.91       320\n",
      "\n",
      "Number of FP 45\n",
      "Number of TP 0\n",
      "Number of TP 0\n",
      "980.0\n",
      "Confusion Matrix para classe 3:\n",
      "[[272  45]\n",
      " [  3   0]]\n",
      "{'TN': 272, 'FP': 45, 'FN': 3, 'TP': 0}\n",
      "For the Red Wine NB para Qualidade-> 4 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.65      0.78       310\n",
      "           1       0.06      0.70      0.11        10\n",
      "\n",
      "    accuracy                           0.65       320\n",
      "   macro avg       0.52      0.68      0.45       320\n",
      "weighted avg       0.96      0.65      0.76       320\n",
      "\n",
      "Number of FP 108\n",
      "Number of TP 7\n",
      "Number of TP 7\n",
      "980.0\n",
      "Confusion Matrix para classe 4:\n",
      "[[202 108]\n",
      " [  3   7]]\n",
      "{'TN': 202, 'FP': 108, 'FN': 3, 'TP': 7}\n",
      "For the Red Wine NB para Qualidade-> 5 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.74      0.79       193\n",
      "           1       0.66      0.78      0.72       127\n",
      "\n",
      "    accuracy                           0.76       320\n",
      "   macro avg       0.75      0.76      0.75       320\n",
      "weighted avg       0.77      0.76      0.76       320\n",
      "\n",
      "Number of FP 50\n",
      "Number of TP 99\n",
      "Number of TP 99\n",
      "980.0\n",
      "Confusion Matrix para classe 5:\n",
      "[[143  50]\n",
      " [ 28  99]]\n",
      "{'TN': 143, 'FP': 50, 'FN': 28, 'TP': 99}\n",
      "For the Red Wine NB para Qualidade-> 6 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.54      0.63       179\n",
      "           1       0.57      0.79      0.66       141\n",
      "\n",
      "    accuracy                           0.65       320\n",
      "   macro avg       0.67      0.66      0.65       320\n",
      "weighted avg       0.68      0.65      0.64       320\n",
      "\n",
      "Number of FP 83\n",
      "Number of TP 111\n",
      "Number of TP 111\n",
      "980.0\n",
      "Confusion Matrix para classe 6:\n",
      "[[ 96  83]\n",
      " [ 30 111]]\n",
      "{'TN': 96, 'FP': 83, 'FN': 30, 'TP': 111}\n",
      "For the Red Wine NB para Qualidade-> 7 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.72      0.82       282\n",
      "           1       0.29      0.84      0.43        38\n",
      "\n",
      "    accuracy                           0.73       320\n",
      "   macro avg       0.63      0.78      0.63       320\n",
      "weighted avg       0.89      0.73      0.78       320\n",
      "\n",
      "Number of FP 80\n",
      "Number of TP 32\n",
      "Number of TP 32\n",
      "980.0\n",
      "Confusion Matrix para classe 7:\n",
      "[[202  80]\n",
      " [  6  32]]\n",
      "{'TN': 202, 'FP': 80, 'FN': 6, 'TP': 32}\n",
      "For the Red Wine NB para Qualidade-> 8 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.78      0.88       319\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.78       320\n",
      "   macro avg       0.50      0.39      0.44       320\n",
      "weighted avg       0.99      0.78      0.87       320\n",
      "\n",
      "Number of FP 69\n",
      "Number of TP 0\n",
      "Number of TP 0\n",
      "980.0\n",
      "Confusion Matrix para classe 8:\n",
      "[[250  69]\n",
      " [  1   0]]\n",
      "{'TN': 250, 'FP': 69, 'FN': 1, 'TP': 0}\n",
      "Para a analise total temos as medias:\n",
      "Para macro avg ->  {'precision macro avg': 0.26386305493110457, 'recall macro avg': 0.5181444774610341, 'f1-score macro avg': 0.319790756363112}\n",
      "Para weighted avg ->  {'precision weighted avg': 0.5516363955526438, 'recall weighted avg': 0.778125, 'f1-score weighted avg': 0.6308776092364266}\n",
      "Numero de amostrar que deram true em algum momento:  684\n",
      "Pela formula:  0\n",
      "Numero de Amostras que sao classificadas:  684\n",
      "684\n",
      "Amostras repetidas pelo menos uma vez 243\n",
      "Numero real de amostras no dataset teste:  320\n",
      "VALORES GERAIS -> FP:  435 FN:  71 TN  1165 TP 249\n",
      "Overall-> Precision: 0.36403508771929827 | Recall: 0.778125|F1-Score: 0.49601593625498\n",
      "Overall Accuracy:  0.7364583333333333\n"
     ]
    }
   ],
   "source": [
    "## red wine\n",
    "\n",
    "class_rep = {}\n",
    "number_samples_final = 0\n",
    "predicted_as_1_in_any_model = np.zeros_like(data_red_wine_one_vs_all['6']['red_wine_y_test'], dtype=int)\n",
    "fpp = 0\n",
    "fnn = 0\n",
    "tnn = 0\n",
    "tpp = 0\n",
    "\n",
    "for i in np.sort(data_red_wine['quality'].unique()):\n",
    "    red_wine_nb_model = GaussianNB()\n",
    "    red_wine_nb_model.fit(data_red_wine_one_vs_all[f'{i}']['red_wine_X_train'],data_red_wine_one_vs_all[f'{i}']['red_wine_y_train'])\n",
    "    #use the nb model to try to predict the test dataset\n",
    "    red_wine_y_pred = red_wine_nb_model.predict(data_red_wine_one_vs_all[f'{i}']['red_wine_X_test'])\n",
    "    ## for better understanding of precition and recall\n",
    "    ## https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "    print('For the Red Wine NB para Qualidade->',i,'\\n',classification_report(data_red_wine_one_vs_all[f'{i}']['red_wine_y_test'],red_wine_y_pred))\n",
    "    class_rep[f'{i}'] = classification_report(data_red_wine_one_vs_all[f'{i}']['red_wine_y_test'],red_wine_y_pred,output_dict=True)\n",
    "    number_samples_final += np.sum(red_wine_y_pred == 1)\n",
    "    #print(red_wine_y_pred)\n",
    "    predicted_as_1_in_any_model = predicted_as_1_in_any_model + red_wine_y_pred\n",
    "    print(\"Number of FP\",np.sum((~data_red_wine_one_vs_all[f'{i}']['red_wine_y_test'] & red_wine_y_pred)==1))\n",
    "    print(\"Number of TP\",np.sum((data_red_wine_one_vs_all[f'{i}']['red_wine_y_test']+red_wine_y_pred)>1))\n",
    "    print(\"Number of TP\",np.sum((data_red_wine_one_vs_all[f'{i}']['red_wine_y_test']&red_wine_y_pred)==1))\n",
    "    #print(predicted_as_1_in_any_model)\n",
    "    print(number_samples)\n",
    "    cm = confusion_matrix(data_red_wine_one_vs_all[f'{i}']['red_wine_y_test'], red_wine_y_pred)\n",
    "    print(f\"Confusion Matrix para classe {i}:\\n{cm}\")\n",
    "    tn, fp, fn, tp = confusion_matrix(data_red_wine_one_vs_all[f'{i}']['red_wine_y_test'], red_wine_y_pred).ravel()\n",
    "    print({'TN': tn, 'FP': fp, 'FN': fn, 'TP': tp})\n",
    "    fpp = fpp + fp\n",
    "    fnn = fn + fnn\n",
    "    tnn = tn + tnn\n",
    "    tpp = tpp + tp    \n",
    "\n",
    "\n",
    "## tabela juntando os valores previstos como verdadeiros\n",
    "macro_avgs = {\"precision macro avg\":0,\"recall macro avg\":0,\"f1-score macro avg\":0}\n",
    "weighted_avg = {\"precision weighted avg\":0,\"recall weighted avg\":0,\"f1-score weighted avg\":0}\n",
    "number_samples= 0\n",
    "\n",
    "for i in np.sort(data_red_wine['quality'].unique()):\n",
    "    macro_avgs[\"precision macro avg\"] = macro_avgs[\"precision macro avg\"] + class_rep[f'{i}'][\"1\"][\"precision\"]\n",
    "    weighted_avg[\"precision weighted avg\"] = weighted_avg[\"precision weighted avg\"] + class_rep[f'{i}'][\"1\"][\"precision\"]*class_rep[f'{i}'][\"1\"][\"support\"]\n",
    "    macro_avgs[\"recall macro avg\"] = macro_avgs[\"recall macro avg\"] + class_rep[f'{i}'][\"1\"][\"recall\"]\n",
    "    weighted_avg[\"recall weighted avg\"] = weighted_avg[\"recall weighted avg\"] + class_rep[f'{i}'][\"1\"][\"recall\"]*class_rep[f'{i}'][\"1\"][\"support\"]    \n",
    "    macro_avgs[\"f1-score macro avg\"] = macro_avgs[\"f1-score macro avg\"] + class_rep[f'{i}'][\"1\"][\"f1-score\"]\n",
    "    weighted_avg[\"f1-score weighted avg\"] = weighted_avg[\"f1-score weighted avg\"] + class_rep[f'{i}'][\"1\"][\"f1-score\"]*class_rep[f'{i}'][\"1\"][\"support\"]\n",
    "    number_samples = number_samples + class_rep[f'{i}'][\"1\"][\"support\"]\n",
    "\n",
    "for i in macro_avgs:\n",
    "    macro_avgs[i] = macro_avgs[i]/len(np.sort(data_red_wine['quality'].unique()))\n",
    "\n",
    "for i in weighted_avg:\n",
    "    weighted_avg[i] = weighted_avg[i]/number_samples\n",
    "\n",
    "print(\"Para a analise total temos as medias:\")\n",
    "print(\"Para macro avg -> \",macro_avgs)\n",
    "print(\"Para weighted avg -> \",weighted_avg)\n",
    "print(\"Numero de amostrar que deram true em algum momento: \",number_samples_final)\n",
    "print(\"Pela formula: \",calculo_numero_amostras)\n",
    "print(\"Numero de Amostras que sao classificadas: \",tpp+fpp)\n",
    "valor = 0\n",
    "for i in predicted_as_1_in_any_model:\n",
    "    if i > 0:\n",
    "        valor = valor + i\n",
    "print(valor)\n",
    "print(\"Amostras repetidas pelo menos uma vez\",np.sum(predicted_as_1_in_any_model>1))\n",
    "print(\"Numero real de amostras no dataset teste: \",len(data_red_wine_one_vs_all['6']['red_wine_y_test']))\n",
    "print('VALORES GERAIS -> FP: ',fpp,'FN: ',fnn,'TN ',tnn,\"TP\",tpp)\n",
    "print(f\"Overall-> Precision: {tpp/(tpp+fpp)} | Recall: {tpp/(tpp+fnn)}|F1-Score: {2/((1/(tpp/(tpp+fpp))) + (1/(tpp/(tpp+fnn))))}\")\n",
    "print(\"Overall Accuracy: \",(tpp+tnn)/(tpp+tnn+fpp+fnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the White Wine NB para Qualidade-> 3 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.91      0.95       976\n",
      "           1       0.01      0.25      0.02         4\n",
      "\n",
      "    accuracy                           0.91       980\n",
      "   macro avg       0.50      0.58      0.49       980\n",
      "weighted avg       0.99      0.91      0.95       980\n",
      "\n",
      "Number of FP 83\n",
      "Number of TP 1\n",
      "Number of TP 1\n",
      "84\n",
      "Confusion Matrix para classe 3:\n",
      "[[893  83]\n",
      " [  3   1]]\n",
      "{'TN': 893, 'FP': 83, 'FN': 3, 'TP': 1}\n",
      "For the White Wine NB para Qualidade-> 4 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.86      0.92       947\n",
      "           1       0.13      0.61      0.21        33\n",
      "\n",
      "    accuracy                           0.85       980\n",
      "   macro avg       0.56      0.73      0.56       980\n",
      "weighted avg       0.96      0.85      0.89       980\n",
      "\n",
      "Number of FP 137\n",
      "Number of TP 20\n",
      "Number of TP 20\n",
      "241\n",
      "Confusion Matrix para classe 4:\n",
      "[[810 137]\n",
      " [ 13  20]]\n",
      "{'TN': 810, 'FP': 137, 'FN': 13, 'TP': 20}\n",
      "For the White Wine NB para Qualidade-> 5 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.84      0.68      0.75       689\n",
      "           1       0.48      0.69      0.57       291\n",
      "\n",
      "    accuracy                           0.69       980\n",
      "   macro avg       0.66      0.69      0.66       980\n",
      "weighted avg       0.73      0.69      0.70       980\n",
      "\n",
      "Number of FP 219\n",
      "Number of TP 202\n",
      "Number of TP 202\n",
      "662\n",
      "Confusion Matrix para classe 5:\n",
      "[[470 219]\n",
      " [ 89 202]]\n",
      "{'TN': 470, 'FP': 219, 'FN': 89, 'TP': 202}\n",
      "For the White Wine NB para Qualidade-> 6 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.32      0.43       540\n",
      "           1       0.48      0.77      0.59       440\n",
      "\n",
      "    accuracy                           0.52       980\n",
      "   macro avg       0.55      0.54      0.51       980\n",
      "weighted avg       0.56      0.52      0.50       980\n",
      "\n",
      "Number of FP 366\n",
      "Number of TP 337\n",
      "Number of TP 337\n",
      "1365\n",
      "Confusion Matrix para classe 6:\n",
      "[[174 366]\n",
      " [103 337]]\n",
      "{'TN': 174, 'FP': 366, 'FN': 103, 'TP': 337}\n",
      "For the White Wine NB para Qualidade-> 7 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.55      0.69       804\n",
      "           1       0.28      0.80      0.42       176\n",
      "\n",
      "    accuracy                           0.60       980\n",
      "   macro avg       0.60      0.68      0.56       980\n",
      "weighted avg       0.81      0.60      0.64       980\n",
      "\n",
      "Number of FP 359\n",
      "Number of TP 141\n",
      "Number of TP 141\n",
      "1865\n",
      "Confusion Matrix para classe 7:\n",
      "[[445 359]\n",
      " [ 35 141]]\n",
      "{'TN': 445, 'FP': 359, 'FN': 35, 'TP': 141}\n",
      "For the White Wine NB para Qualidade-> 8 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.58      0.73       945\n",
      "           1       0.06      0.71      0.11        35\n",
      "\n",
      "    accuracy                           0.58       980\n",
      "   macro avg       0.52      0.65      0.42       980\n",
      "weighted avg       0.95      0.58      0.71       980\n",
      "\n",
      "Number of FP 399\n",
      "Number of TP 25\n",
      "Number of TP 25\n",
      "2289\n",
      "Confusion Matrix para classe 8:\n",
      "[[546 399]\n",
      " [ 10  25]]\n",
      "{'TN': 546, 'FP': 399, 'FN': 10, 'TP': 25}\n",
      "For the White Wine NB para Qualidade-> 9 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.92      0.96       979\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.92       980\n",
      "   macro avg       0.50      0.46      0.48       980\n",
      "weighted avg       1.00      0.92      0.96       980\n",
      "\n",
      "Number of FP 79\n",
      "Number of TP 0\n",
      "Number of TP 0\n",
      "2368\n",
      "Confusion Matrix para classe 9:\n",
      "[[900  79]\n",
      " [  1   0]]\n",
      "{'TN': 900, 'FP': 79, 'FN': 1, 'TP': 0}\n",
      "Para a analise total temos as medias:\n",
      "Para macro avg ->  {'precision macro avg': 0.20563423546966306, 'recall macro avg': 0.5473642643561643, 'f1-score macro avg': 0.2737768335006807}\n",
      "Para weighted avg ->  {'precision weighted avg': 0.4147922774042701, 'recall weighted avg': 0.7408163265306122, 'f1-score weighted avg': 0.5192312013804579}\n",
      "Numero de amostrar que deram true em algum momento:  2368\n",
      "Pela formula:  0\n",
      "Numero de Amostras que sao classificadas mais de uma vez:  2368\n",
      "Amostras repetidas pelo menos uma vez 864\n",
      "Numero real de amostras no dataset teste:  980\n",
      "VALORES GERAIS -> FP:  1642 FN:  254 TN  4238 TP 726\n",
      "Overall-> Precision: 0.30658783783783783 | Recall: 0.7408163265306122|F1-Score: 0.4336917562724014\n",
      "Overall Accuracy:  0.7236151603498542\n"
     ]
    }
   ],
   "source": [
    "## white wine\n",
    "class_rep = {}\n",
    "number_samples_final = 0\n",
    "predicted_as_1_in_any_model = np.zeros_like(data_white_wine_one_vs_all['6']['white_wine_y_test'], dtype=int)\n",
    "fpp = 0\n",
    "fnn = 0\n",
    "tnn = 0\n",
    "tpp = 0\n",
    "\n",
    "for i in np.sort(data_white_wine['quality'].unique()):\n",
    "    white_wine_nb_model = GaussianNB()\n",
    "    white_wine_nb_model.fit(data_white_wine_one_vs_all[f'{i}']['white_wine_X_train'],data_white_wine_one_vs_all[f'{i}']['white_wine_y_train'])\n",
    "    #use the nb model to try to pwhiteict the test dataset\n",
    "    white_wine_y_pred = white_wine_nb_model.predict(data_white_wine_one_vs_all[f'{i}']['white_wine_X_test'])\n",
    "    ## for better understanding of precition and recall\n",
    "    ## https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "    print('For the White Wine NB para Qualidade->',i,'\\n',classification_report(data_white_wine_one_vs_all[f'{i}']['white_wine_y_test'],white_wine_y_pred))\n",
    "\n",
    "    class_rep[f'{i}'] = classification_report(data_white_wine_one_vs_all[f'{i}']['white_wine_y_test'],white_wine_y_pred,output_dict=True)\n",
    "    number_samples_final += np.sum(white_wine_y_pred == 1)\n",
    "    #print(white_wine_y_pred)\n",
    "    predicted_as_1_in_any_model = predicted_as_1_in_any_model + white_wine_y_pred\n",
    "    print(\"Number of FP\",np.sum((~data_white_wine_one_vs_all[f'{i}']['white_wine_y_test'] & white_wine_y_pred)==1))\n",
    "    print(\"Number of TP\",np.sum((data_white_wine_one_vs_all[f'{i}']['white_wine_y_test']+white_wine_y_pred)>1))\n",
    "    print(\"Number of TP\",np.sum((data_white_wine_one_vs_all[f'{i}']['white_wine_y_test']&white_wine_y_pred)==1))\n",
    "    #print(predicted_as_1_in_any_model)\n",
    "    print(number_samples_final)\n",
    "    cm = confusion_matrix(data_white_wine_one_vs_all[f'{i}']['white_wine_y_test'], white_wine_y_pred)\n",
    "    print(f\"Confusion Matrix para classe {i}:\\n{cm}\")\n",
    "    tn, fp, fn, tp = confusion_matrix(data_white_wine_one_vs_all[f'{i}']['white_wine_y_test'], white_wine_y_pred).ravel()\n",
    "    print({'TN': tn, 'FP': fp, 'FN': fn, 'TP': tp})\n",
    "    fpp = fpp + fp\n",
    "    fnn = fn + fnn\n",
    "    tnn = tn + tnn\n",
    "    tpp = tpp + tp\n",
    "\n",
    "\n",
    "## tabela juntando os valores previstos como verdadeiros\n",
    "macro_avgs = {\"precision macro avg\":0,\"recall macro avg\":0,\"f1-score macro avg\":0}\n",
    "weighted_avg = {\"precision weighted avg\":0,\"recall weighted avg\":0,\"f1-score weighted avg\":0}\n",
    "number_samples= 0\n",
    "for i in np.sort(data_white_wine['quality'].unique()):\n",
    "    macro_avgs[\"precision macro avg\"] = macro_avgs[\"precision macro avg\"] + class_rep[f'{i}'][\"1\"][\"precision\"]\n",
    "    weighted_avg[\"precision weighted avg\"] = weighted_avg[\"precision weighted avg\"] + class_rep[f'{i}'][\"1\"][\"precision\"]*class_rep[f'{i}'][\"1\"][\"support\"]\n",
    "    macro_avgs[\"recall macro avg\"] = macro_avgs[\"recall macro avg\"] + class_rep[f'{i}'][\"1\"][\"recall\"]\n",
    "    weighted_avg[\"recall weighted avg\"] = weighted_avg[\"recall weighted avg\"] + class_rep[f'{i}'][\"1\"][\"recall\"]*class_rep[f'{i}'][\"1\"][\"support\"]    \n",
    "    macro_avgs[\"f1-score macro avg\"] = macro_avgs[\"f1-score macro avg\"] + class_rep[f'{i}'][\"1\"][\"f1-score\"]\n",
    "    weighted_avg[\"f1-score weighted avg\"] = weighted_avg[\"f1-score weighted avg\"] + class_rep[f'{i}'][\"1\"][\"f1-score\"]*class_rep[f'{i}'][\"1\"][\"support\"]\n",
    "    number_samples = number_samples + class_rep[f'{i}'][\"1\"][\"support\"]\n",
    "\n",
    "for i in macro_avgs:\n",
    "    macro_avgs[i] = macro_avgs[i]/len(np.sort(data_white_wine['quality'].unique()))\n",
    "\n",
    "for i in weighted_avg:\n",
    "    weighted_avg[i] = weighted_avg[i]/number_samples\n",
    "\n",
    "print(\"Para a analise total temos as medias:\")\n",
    "print(\"Para macro avg -> \",macro_avgs)\n",
    "print(\"Para weighted avg -> \",weighted_avg)\n",
    "print(\"Numero de amostrar que deram true em algum momento: \",number_samples_final)\n",
    "print(\"Pela formula: \",calculo_numero_amostras)\n",
    "print(\"Numero de Amostras que sao classificadas mais de uma vez: \",tpp+fpp)\n",
    "print(\"Amostras repetidas pelo menos uma vez\",np.sum(predicted_as_1_in_any_model>1))\n",
    "print(\"Numero real de amostras no dataset teste: \",len(data_white_wine_one_vs_all['6']['white_wine_y_test']))\n",
    "print('VALORES GERAIS -> FP: ',fpp,'FN: ',fnn,'TN ',tnn,\"TP\",tpp)\n",
    "print(f\"Overall-> Precision: {tpp/(tpp+fpp)} | Recall: {tpp/(tpp+fnn)}|F1-Score: {2/((1/(tpp/(tpp+fpp))) + (1/(tpp/(tpp+fnn))))}\")\n",
    "print(\"Overall Accuracy: \",(tpp+tnn)/(tpp+tnn+fpp+fnn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the Red Wine svm para Qualidade-> 3 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       317\n",
      "           1       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.98       320\n",
      "   macro avg       0.50      0.49      0.49       320\n",
      "weighted avg       0.98      0.98      0.98       320\n",
      "\n",
      "Number of FP 4\n",
      "Number of TP 0\n",
      "Number of TP 0\n",
      "4\n",
      "Confusion Matrix para classe 3:\n",
      "[[313   4]\n",
      " [  3   0]]\n",
      "{'TN': 313, 'FP': 4, 'FN': 3, 'TP': 0}\n",
      "For the Red Wine svm para Qualidade-> 4 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.88      0.93       310\n",
      "           1       0.12      0.50      0.20        10\n",
      "\n",
      "    accuracy                           0.87       320\n",
      "   macro avg       0.55      0.69      0.56       320\n",
      "weighted avg       0.96      0.87      0.91       320\n",
      "\n",
      "Number of FP 36\n",
      "Number of TP 5\n",
      "Number of TP 5\n",
      "45\n",
      "Confusion Matrix para classe 4:\n",
      "[[274  36]\n",
      " [  5   5]]\n",
      "{'TN': 274, 'FP': 36, 'FN': 5, 'TP': 5}\n",
      "For the Red Wine svm para Qualidade-> 5 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.76      0.81       193\n",
      "           1       0.69      0.83      0.75       127\n",
      "\n",
      "    accuracy                           0.78       320\n",
      "   macro avg       0.78      0.79      0.78       320\n",
      "weighted avg       0.80      0.78      0.79       320\n",
      "\n",
      "Number of FP 47\n",
      "Number of TP 105\n",
      "Number of TP 105\n",
      "197\n",
      "Confusion Matrix para classe 5:\n",
      "[[146  47]\n",
      " [ 22 105]]\n",
      "{'TN': 146, 'FP': 47, 'FN': 22, 'TP': 105}\n",
      "For the Red Wine svm para Qualidade-> 6 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.66      0.71       179\n",
      "           1       0.64      0.74      0.69       141\n",
      "\n",
      "    accuracy                           0.70       320\n",
      "   macro avg       0.70      0.70      0.70       320\n",
      "weighted avg       0.71      0.70      0.70       320\n",
      "\n",
      "Number of FP 60\n",
      "Number of TP 105\n",
      "Number of TP 105\n",
      "362\n",
      "Confusion Matrix para classe 6:\n",
      "[[119  60]\n",
      " [ 36 105]]\n",
      "{'TN': 119, 'FP': 60, 'FN': 36, 'TP': 105}\n",
      "For the Red Wine svm para Qualidade-> 7 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.83      0.89       282\n",
      "           1       0.36      0.74      0.49        38\n",
      "\n",
      "    accuracy                           0.82       320\n",
      "   macro avg       0.66      0.78      0.69       320\n",
      "weighted avg       0.89      0.82      0.84       320\n",
      "\n",
      "Number of FP 49\n",
      "Number of TP 28\n",
      "Number of TP 28\n",
      "439\n",
      "Confusion Matrix para classe 7:\n",
      "[[233  49]\n",
      " [ 10  28]]\n",
      "{'TN': 233, 'FP': 49, 'FN': 10, 'TP': 28}\n",
      "For the Red Wine svm para Qualidade-> 8 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.93      0.96       319\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.93       320\n",
      "   macro avg       0.50      0.47      0.48       320\n",
      "weighted avg       0.99      0.93      0.96       320\n",
      "\n",
      "Number of FP 22\n",
      "Number of TP 0\n",
      "Number of TP 0\n",
      "461\n",
      "Confusion Matrix para classe 8:\n",
      "[[297  22]\n",
      " [  1   0]]\n",
      "{'TN': 297, 'FP': 22, 'FN': 1, 'TP': 0}\n",
      "Para a analise total temos as medias:\n",
      "Para macro avg ->  {'precision macro avg': 0.3021234488660676, 'recall macro avg': 0.46804910164504915, 'f1-score macro avg': 0.35366627249310195}\n",
      "Para weighted avg ->  {'precision weighted avg': 0.6015475934327226, 'recall weighted avg': 0.759375, 'f1-score weighted avg': 0.6650663620988367}\n",
      "Numero de amostrar que deram true em algum momento:  461\n",
      "Pela formula:  0\n",
      "Numero de Amostras que sao classificadas mais de uma vez:  461\n",
      "Amostras repetidas pelo menos uma vez 132\n",
      "Numero real de amostras no dataset teste:  320\n",
      "VALORES GERAIS -> FP:  218 FN:  77 TN  1382 TP 243\n",
      "Overall-> Precision: 0.527114967462039 | Recall: 0.759375|F1-Score: 0.6222791293213827\n",
      "Overall Accuracy:  0.8463541666666666\n"
     ]
    }
   ],
   "source": [
    "## red wine\n",
    "\n",
    "class_rep = {}\n",
    "number_samples_final = 0\n",
    "predicted_as_1_in_any_model = np.zeros_like(data_red_wine_one_vs_all['6']['red_wine_y_test'], dtype=int)\n",
    "fpp = 0\n",
    "fnn = 0\n",
    "tnn = 0\n",
    "tpp = 0\n",
    "\n",
    "\n",
    "for i in np.sort(data_red_wine['quality'].unique()):\n",
    "    red_wine_svm_model = SVC()\n",
    "    red_wine_svm_model.fit(data_red_wine_one_vs_all[f'{i}']['red_wine_X_train'],data_red_wine_one_vs_all[f'{i}']['red_wine_y_train'])\n",
    "    #use the svm model to try to predict the test dataset\n",
    "    red_wine_y_pred = red_wine_svm_model.predict(data_red_wine_one_vs_all[f'{i}']['red_wine_X_test'])\n",
    "    ## for better understanding of precition and recall\n",
    "    ## https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "    print('For the Red Wine svm para Qualidade->',i,'\\n',classification_report(data_red_wine_one_vs_all[f'{i}']['red_wine_y_test'],red_wine_y_pred,zero_division=True))\n",
    "    class_rep[f'{i}'] = classification_report(data_red_wine_one_vs_all[f'{i}']['red_wine_y_test'],red_wine_y_pred,output_dict=True)\n",
    "    \n",
    "    number_samples_final += np.sum(red_wine_y_pred == 1)\n",
    "    #print(red_wine_y_pred)\n",
    "    predicted_as_1_in_any_model = predicted_as_1_in_any_model + red_wine_y_pred\n",
    "    print(\"Number of FP\",np.sum((~data_red_wine_one_vs_all[f'{i}']['red_wine_y_test'] & red_wine_y_pred)==1))\n",
    "    print(\"Number of TP\",np.sum((data_red_wine_one_vs_all[f'{i}']['red_wine_y_test']+red_wine_y_pred)>1))\n",
    "    print(\"Number of TP\",np.sum((data_red_wine_one_vs_all[f'{i}']['red_wine_y_test']&red_wine_y_pred)==1))\n",
    "    #print(predicted_as_1_in_any_model)\n",
    "    print(number_samples_final)\n",
    "    cm = confusion_matrix(data_red_wine_one_vs_all[f'{i}']['red_wine_y_test'], red_wine_y_pred)\n",
    "    print(f\"Confusion Matrix para classe {i}:\\n{cm}\")\n",
    "    tn, fp, fn, tp = confusion_matrix(data_red_wine_one_vs_all[f'{i}']['red_wine_y_test'], red_wine_y_pred).ravel()\n",
    "    print({'TN': tn, 'FP': fp, 'FN': fn, 'TP': tp})\n",
    "    fpp = fpp + fp\n",
    "    fnn = fn + fnn\n",
    "    tnn = tn + tnn\n",
    "    tpp = tpp + tp\n",
    "\n",
    "## tabela juntando os valores previstos como verdadeiros\n",
    "macro_avgs = {\"precision macro avg\":0,\"recall macro avg\":0,\"f1-score macro avg\":0}\n",
    "weighted_avg = {\"precision weighted avg\":0,\"recall weighted avg\":0,\"f1-score weighted avg\":0}\n",
    "number_samples= 0\n",
    "\n",
    "for i in np.sort(data_red_wine['quality'].unique()):\n",
    "    macro_avgs[\"precision macro avg\"] = macro_avgs[\"precision macro avg\"] + class_rep[f'{i}'][\"1\"][\"precision\"]\n",
    "    weighted_avg[\"precision weighted avg\"] = weighted_avg[\"precision weighted avg\"] + class_rep[f'{i}'][\"1\"][\"precision\"]*class_rep[f'{i}'][\"1\"][\"support\"]\n",
    "    macro_avgs[\"recall macro avg\"] = macro_avgs[\"recall macro avg\"] + class_rep[f'{i}'][\"1\"][\"recall\"]\n",
    "    weighted_avg[\"recall weighted avg\"] = weighted_avg[\"recall weighted avg\"] + class_rep[f'{i}'][\"1\"][\"recall\"]*class_rep[f'{i}'][\"1\"][\"support\"]    \n",
    "    macro_avgs[\"f1-score macro avg\"] = macro_avgs[\"f1-score macro avg\"] + class_rep[f'{i}'][\"1\"][\"f1-score\"]\n",
    "    weighted_avg[\"f1-score weighted avg\"] = weighted_avg[\"f1-score weighted avg\"] + class_rep[f'{i}'][\"1\"][\"f1-score\"]*class_rep[f'{i}'][\"1\"][\"support\"]\n",
    "    number_samples = number_samples + class_rep[f'{i}'][\"1\"][\"support\"]\n",
    "\n",
    "for i in macro_avgs:\n",
    "    macro_avgs[i] = macro_avgs[i]/len(np.sort(data_red_wine['quality'].unique()))\n",
    "\n",
    "for i in weighted_avg:\n",
    "    weighted_avg[i] = weighted_avg[i]/number_samples\n",
    "\n",
    "print(\"Para a analise total temos as medias:\")\n",
    "print(\"Para macro avg -> \",macro_avgs)\n",
    "print(\"Para weighted avg -> \",weighted_avg)\n",
    "print(\"Numero de amostrar que deram true em algum momento: \",number_samples_final)\n",
    "print(\"Pela formula: \",calculo_numero_amostras)\n",
    "print(\"Numero de Amostras que sao classificadas mais de uma vez: \",tpp+fpp)\n",
    "print(\"Amostras repetidas pelo menos uma vez\",np.sum(predicted_as_1_in_any_model>1))\n",
    "print(\"Numero real de amostras no dataset teste: \",len(data_red_wine_one_vs_all['6']['red_wine_y_test']))\n",
    "print('VALORES GERAIS -> FP: ',fpp,'FN: ',fnn,'TN ',tnn,\"TP\",tpp)\n",
    "print(f\"Overall-> Precision: {tpp/(tpp+fpp)} | Recall: {tpp/(tpp+fnn)}|F1-Score: {2/((1/(tpp/(tpp+fpp))) + (1/(tpp/(tpp+fnn))))}\")\n",
    "print(\"Overall Accuracy: \",(tpp+tnn)/(tpp+tnn+fpp+fnn))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the White Wine svm para Qualidade-> 3 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       976\n",
      "           1       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.98       980\n",
      "   macro avg       0.50      0.49      0.49       980\n",
      "weighted avg       0.99      0.98      0.99       980\n",
      "\n",
      "Number of FP 17\n",
      "Number of TP 0\n",
      "Number of TP 0\n",
      "17\n",
      "Confusion Matrix para classe 3:\n",
      "[[959  17]\n",
      " [  4   0]]\n",
      "{'TN': 959, 'FP': 17, 'FN': 4, 'TP': 0}\n",
      "For the White Wine svm para Qualidade-> 4 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.88      0.93       947\n",
      "           1       0.13      0.52      0.21        33\n",
      "\n",
      "    accuracy                           0.87       980\n",
      "   macro avg       0.55      0.70      0.57       980\n",
      "weighted avg       0.95      0.87      0.90       980\n",
      "\n",
      "Number of FP 115\n",
      "Number of TP 17\n",
      "Number of TP 17\n",
      "149\n",
      "Confusion Matrix para classe 4:\n",
      "[[832 115]\n",
      " [ 16  17]]\n",
      "{'TN': 832, 'FP': 115, 'FN': 16, 'TP': 17}\n",
      "For the White Wine svm para Qualidade-> 5 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.74      0.80       689\n",
      "           1       0.54      0.73      0.62       291\n",
      "\n",
      "    accuracy                           0.74       980\n",
      "   macro avg       0.70      0.73      0.71       980\n",
      "weighted avg       0.77      0.74      0.75       980\n",
      "\n",
      "Number of FP 178\n",
      "Number of TP 211\n",
      "Number of TP 211\n",
      "538\n",
      "Confusion Matrix para classe 5:\n",
      "[[511 178]\n",
      " [ 80 211]]\n",
      "{'TN': 511, 'FP': 178, 'FN': 80, 'TP': 211}\n",
      "For the White Wine svm para Qualidade-> 6 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.56      0.62       540\n",
      "           1       0.56      0.70      0.62       440\n",
      "\n",
      "    accuracy                           0.62       980\n",
      "   macro avg       0.63      0.63      0.62       980\n",
      "weighted avg       0.63      0.62      0.62       980\n",
      "\n",
      "Number of FP 240\n",
      "Number of TP 306\n",
      "Number of TP 306\n",
      "1084\n",
      "Confusion Matrix para classe 6:\n",
      "[[300 240]\n",
      " [134 306]]\n",
      "{'TN': 300, 'FP': 240, 'FN': 134, 'TP': 306}\n",
      "For the White Wine svm para Qualidade-> 7 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.71      0.81       804\n",
      "           1       0.37      0.80      0.51       176\n",
      "\n",
      "    accuracy                           0.72       980\n",
      "   macro avg       0.66      0.75      0.66       980\n",
      "weighted avg       0.84      0.72      0.75       980\n",
      "\n",
      "Number of FP 235\n",
      "Number of TP 140\n",
      "Number of TP 140\n",
      "1459\n",
      "Confusion Matrix para classe 7:\n",
      "[[569 235]\n",
      " [ 36 140]]\n",
      "{'TN': 569, 'FP': 235, 'FN': 36, 'TP': 140}\n",
      "For the White Wine svm para Qualidade-> 8 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.83      0.90       945\n",
      "           1       0.14      0.71      0.23        35\n",
      "\n",
      "    accuracy                           0.83       980\n",
      "   macro avg       0.56      0.77      0.57       980\n",
      "weighted avg       0.96      0.83      0.88       980\n",
      "\n",
      "Number of FP 158\n",
      "Number of TP 25\n",
      "Number of TP 25\n",
      "1642\n",
      "Confusion Matrix para classe 8:\n",
      "[[787 158]\n",
      " [ 10  25]]\n",
      "{'TN': 787, 'FP': 158, 'FN': 10, 'TP': 25}\n",
      "For the White Wine svm para Qualidade-> 9 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       979\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           1.00       980\n",
      "   macro avg       0.50      0.50      0.50       980\n",
      "weighted avg       1.00      1.00      1.00       980\n",
      "\n",
      "Number of FP 3\n",
      "Number of TP 0\n",
      "Number of TP 0\n",
      "1645\n",
      "Confusion Matrix para classe 9:\n",
      "[[976   3]\n",
      " [  1   0]]\n",
      "{'TN': 976, 'FP': 3, 'FN': 1, 'TP': 0}\n",
      "Para a analise total temos as medias:\n",
      "Para macro avg ->  {'precision macro avg': 0.24879846383726506, 'recall macro avg': 0.49220460442846303, 'f1-score macro avg': 0.3121233234056115}\n",
      "Para weighted avg ->  {'precision weighted avg': 0.48895375682221326, 'recall weighted avg': 0.713265306122449, 'f1-score weighted avg': 0.5693464635225542}\n",
      "Numero de amostrar que deram true em algum momento:  1645\n",
      "Pela formula:  0\n",
      "Numero de Amostras que sao classificadas mais de uma vez:  1645\n",
      "Numero real de amostras no dataset teste:  980\n",
      "Amostras repetidas pelo menos uma vez 566\n",
      "VALORES GERAIS -> FP:  946 FN:  281 TN  4934 TP 699\n",
      "Overall-> Precision: 0.4249240121580547 | Recall: 0.713265306122449|F1-Score: 0.5325714285714286\n",
      "Overall Accuracy:  0.821137026239067\n"
     ]
    }
   ],
   "source": [
    "## white wine\n",
    "\n",
    "class_rep = {}\n",
    "number_samples_final = 0\n",
    "predicted_as_1_in_any_model = np.zeros_like(data_white_wine_one_vs_all['6']['white_wine_y_test'], dtype=int)\n",
    "fpp = 0\n",
    "fnn = 0\n",
    "tnn = 0\n",
    "tpp = 0\n",
    "\n",
    "for i in np.sort(data_white_wine['quality'].unique()):\n",
    "    white_wine_svm_model = SVC(class_weight='balanced')\n",
    "    white_wine_svm_model.fit(data_white_wine_one_vs_all[f'{i}']['white_wine_X_train'],data_white_wine_one_vs_all[f'{i}']['white_wine_y_train'])\n",
    "    #use the svm model to try to pwhiteict the test dataset\n",
    "    white_wine_y_pred = white_wine_svm_model.predict(data_white_wine_one_vs_all[f'{i}']['white_wine_X_test'])\n",
    "    ## for better understanding of precition and recall\n",
    "    ## https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "    print('For the White Wine svm para Qualidade->',i,'\\n',classification_report(data_white_wine_one_vs_all[f'{i}']['white_wine_y_test'],white_wine_y_pred,zero_division=True))\n",
    "    class_rep[f'{i}'] = classification_report(data_white_wine_one_vs_all[f'{i}']['white_wine_y_test'],white_wine_y_pred,output_dict=True)\n",
    "\n",
    "    number_samples_final += np.sum(white_wine_y_pred == 1)\n",
    "    #print(white_wine_y_pred)\n",
    "    predicted_as_1_in_any_model = predicted_as_1_in_any_model + white_wine_y_pred\n",
    "    print(\"Number of FP\",np.sum((~data_white_wine_one_vs_all[f'{i}']['white_wine_y_test'] & white_wine_y_pred)==1))\n",
    "    print(\"Number of TP\",np.sum((data_white_wine_one_vs_all[f'{i}']['white_wine_y_test']+white_wine_y_pred)>1))\n",
    "    print(\"Number of TP\",np.sum((data_white_wine_one_vs_all[f'{i}']['white_wine_y_test']&white_wine_y_pred)==1))\n",
    "    #print(predicted_as_1_in_any_model)\n",
    "    print(number_samples_final)\n",
    "    cm = confusion_matrix(data_white_wine_one_vs_all[f'{i}']['white_wine_y_test'], white_wine_y_pred)\n",
    "    print(f\"Confusion Matrix para classe {i}:\\n{cm}\")\n",
    "    tn, fp, fn, tp = confusion_matrix(data_white_wine_one_vs_all[f'{i}']['white_wine_y_test'], white_wine_y_pred).ravel()\n",
    "    print({'TN': tn, 'FP': fp, 'FN': fn, 'TP': tp})\n",
    "    fpp = fpp + fp\n",
    "    fnn = fn + fnn\n",
    "    tnn = tn + tnn\n",
    "    tpp = tpp + tp\n",
    "    \n",
    "## tabela juntando os valores previstos como verdadeiros\n",
    "macro_avgs = {\"precision macro avg\":0,\"recall macro avg\":0,\"f1-score macro avg\":0}\n",
    "weighted_avg = {\"precision weighted avg\":0,\"recall weighted avg\":0,\"f1-score weighted avg\":0}\n",
    "number_samples= 0\n",
    "for i in np.sort(data_white_wine['quality'].unique()):\n",
    "    macro_avgs[\"precision macro avg\"] = macro_avgs[\"precision macro avg\"] + class_rep[f'{i}'][\"1\"][\"precision\"]\n",
    "    weighted_avg[\"precision weighted avg\"] = weighted_avg[\"precision weighted avg\"] + class_rep[f'{i}'][\"1\"][\"precision\"]*class_rep[f'{i}'][\"1\"][\"support\"]\n",
    "    macro_avgs[\"recall macro avg\"] = macro_avgs[\"recall macro avg\"] + class_rep[f'{i}'][\"1\"][\"recall\"]\n",
    "    weighted_avg[\"recall weighted avg\"] = weighted_avg[\"recall weighted avg\"] + class_rep[f'{i}'][\"1\"][\"recall\"]*class_rep[f'{i}'][\"1\"][\"support\"]    \n",
    "    macro_avgs[\"f1-score macro avg\"] = macro_avgs[\"f1-score macro avg\"] + class_rep[f'{i}'][\"1\"][\"f1-score\"]\n",
    "    weighted_avg[\"f1-score weighted avg\"] = weighted_avg[\"f1-score weighted avg\"] + class_rep[f'{i}'][\"1\"][\"f1-score\"]*class_rep[f'{i}'][\"1\"][\"support\"]\n",
    "    number_samples = number_samples + class_rep[f'{i}'][\"1\"][\"support\"]\n",
    "\n",
    "for i in macro_avgs:\n",
    "    macro_avgs[i] = macro_avgs[i]/len(np.sort(data_white_wine['quality'].unique()))\n",
    "\n",
    "for i in weighted_avg:\n",
    "    weighted_avg[i] = weighted_avg[i]/number_samples\n",
    "\n",
    "print(\"Para a analise total temos as medias:\")\n",
    "print(\"Para macro avg -> \",macro_avgs)\n",
    "print(\"Para weighted avg -> \",weighted_avg)\n",
    "print(\"Numero de amostrar que deram true em algum momento: \",number_samples_final)\n",
    "print(\"Pela formula: \",calculo_numero_amostras)\n",
    "print(\"Numero de Amostras que sao classificadas mais de uma vez: \",tpp+fpp)\n",
    "print(\"Numero real de amostras no dataset teste: \",len(data_white_wine_one_vs_all['6']['white_wine_y_test']))\n",
    "print(\"Amostras repetidas pelo menos uma vez\",np.sum(predicted_as_1_in_any_model>1))\n",
    "print('VALORES GERAIS -> FP: ',fpp,'FN: ',fnn,'TN ',tnn,\"TP\",tpp)\n",
    "print(f\"Overall-> Precision: {tpp/(tpp+fpp)} | Recall: {tpp/(tpp+fnn)}|F1-Score: {2/((1/(tpp/(tpp+fpp))) + (1/(tpp/(tpp+fnn))))}\")\n",
    "print(\"Overall Accuracy: \",(tpp+tnn)/(tpp+tnn+fpp+fnn))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the Red Wine lr para Qualidade-> 3 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.96      0.97       317\n",
      "           1       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.95       320\n",
      "   macro avg       0.50      0.48      0.49       320\n",
      "weighted avg       0.98      0.95      0.97       320\n",
      "\n",
      "Number of FP 13\n",
      "Number of TP 0\n",
      "Number of TP 0\n",
      "13\n",
      "Confusion Matrix para classe 3:\n",
      "[[304  13]\n",
      " [  3   0]]\n",
      "{'TN': 304, 'FP': 13, 'FN': 3, 'TP': 0}\n",
      "For the Red Wine lr para Qualidade-> 4 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.82      0.90       310\n",
      "           1       0.11      0.70      0.19        10\n",
      "\n",
      "    accuracy                           0.82       320\n",
      "   macro avg       0.55      0.76      0.54       320\n",
      "weighted avg       0.96      0.82      0.87       320\n",
      "\n",
      "Number of FP 56\n",
      "Number of TP 7\n",
      "Number of TP 7\n",
      "76\n",
      "Confusion Matrix para classe 4:\n",
      "[[254  56]\n",
      " [  3   7]]\n",
      "{'TN': 254, 'FP': 56, 'FN': 3, 'TP': 7}\n",
      "For the Red Wine lr para Qualidade-> 5 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.75      0.80       193\n",
      "           1       0.68      0.81      0.74       127\n",
      "\n",
      "    accuracy                           0.78       320\n",
      "   macro avg       0.77      0.78      0.77       320\n",
      "weighted avg       0.79      0.78      0.78       320\n",
      "\n",
      "Number of FP 48\n",
      "Number of TP 103\n",
      "Number of TP 103\n",
      "227\n",
      "Confusion Matrix para classe 5:\n",
      "[[145  48]\n",
      " [ 24 103]]\n",
      "{'TN': 145, 'FP': 48, 'FN': 24, 'TP': 103}\n",
      "For the Red Wine lr para Qualidade-> 6 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.59      0.65       179\n",
      "           1       0.58      0.71      0.64       141\n",
      "\n",
      "    accuracy                           0.64       320\n",
      "   macro avg       0.65      0.65      0.64       320\n",
      "weighted avg       0.66      0.64      0.64       320\n",
      "\n",
      "Number of FP 73\n",
      "Number of TP 100\n",
      "Number of TP 100\n",
      "400\n",
      "Confusion Matrix para classe 6:\n",
      "[[106  73]\n",
      " [ 41 100]]\n",
      "{'TN': 106, 'FP': 73, 'FN': 41, 'TP': 100}\n",
      "For the Red Wine lr para Qualidade-> 7 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.76      0.85       282\n",
      "           1       0.31      0.79      0.44        38\n",
      "\n",
      "    accuracy                           0.77       320\n",
      "   macro avg       0.64      0.78      0.65       320\n",
      "weighted avg       0.89      0.77      0.80       320\n",
      "\n",
      "Number of FP 67\n",
      "Number of TP 30\n",
      "Number of TP 30\n",
      "497\n",
      "Confusion Matrix para classe 7:\n",
      "[[215  67]\n",
      " [  8  30]]\n",
      "{'TN': 215, 'FP': 67, 'FN': 8, 'TP': 30}\n",
      "For the Red Wine lr para Qualidade-> 8 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.90      0.95       319\n",
      "           1       0.03      1.00      0.06         1\n",
      "\n",
      "    accuracy                           0.90       320\n",
      "   macro avg       0.52      0.95      0.50       320\n",
      "weighted avg       1.00      0.90      0.94       320\n",
      "\n",
      "Number of FP 32\n",
      "Number of TP 1\n",
      "Number of TP 1\n",
      "530\n",
      "Confusion Matrix para classe 8:\n",
      "[[287  32]\n",
      " [  0   1]]\n",
      "{'TN': 287, 'FP': 32, 'FN': 0, 'TP': 1}\n",
      "Para a analise total temos as medias:\n",
      "Para macro avg ->  {'precision macro avg': 0.2851410632180906, 'recall macro avg': 0.6682861940689665, 'f1-score macro avg': 0.34549977752964284}\n",
      "Para weighted avg ->  {'precision weighted avg': 0.5657063147101871, 'recall weighted avg': 0.753125, 'f1-score weighted avg': 0.6336948484499867}\n",
      "Numero de amostrar que deram true em algum momento:  530\n",
      "Pela formula:  0\n",
      "Numero de Amostras que sao classificadas mais de uma vez:  530\n",
      "Amostras repetidas pelo menos uma vez 171\n",
      "Numero real de amostras no dataset teste:  320\n",
      "VALORES GERAIS -> FP:  289 FN:  79 TN  1311 TP 241\n",
      "Overall-> Precision: 0.4547169811320755 | Recall: 0.753125|F1-Score: 0.5670588235294117\n",
      "Overall Accuracy:  0.8083333333333333\n"
     ]
    }
   ],
   "source": [
    "## red wine\n",
    "class_rep = {}\n",
    "number_samples_final = 0\n",
    "predicted_as_1_in_any_model = np.zeros_like(data_red_wine_one_vs_all['6']['red_wine_y_test'], dtype=int)\n",
    "fpp = 0\n",
    "fnn = 0\n",
    "tnn = 0\n",
    "tpp = 0\n",
    "\n",
    "for i in np.sort(data_red_wine['quality'].unique()):\n",
    "    red_wine_lr_model = LogisticRegression()\n",
    "    red_wine_lr_model.fit(data_red_wine_one_vs_all[f'{i}']['red_wine_X_train'],data_red_wine_one_vs_all[f'{i}']['red_wine_y_train'])\n",
    "    #use the lr model to try to predict the test dataset\n",
    "    red_wine_y_pred = red_wine_lr_model.predict(data_red_wine_one_vs_all[f'{i}']['red_wine_X_test'])\n",
    "    ## for better understanding of precition and recall\n",
    "    ## https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "    print('For the Red Wine lr para Qualidade->',i,'\\n',classification_report(data_red_wine_one_vs_all[f'{i}']['red_wine_y_test'],red_wine_y_pred))\n",
    "    class_rep[f'{i}'] = classification_report(data_red_wine_one_vs_all[f'{i}']['red_wine_y_test'],red_wine_y_pred,output_dict=True)\n",
    "\n",
    "    number_samples_final += np.sum(red_wine_y_pred == 1)\n",
    "    #print(red_wine_y_pred)\n",
    "    predicted_as_1_in_any_model = predicted_as_1_in_any_model + red_wine_y_pred\n",
    "    print(\"Number of FP\",np.sum((~data_red_wine_one_vs_all[f'{i}']['red_wine_y_test'] & red_wine_y_pred)==1))\n",
    "    print(\"Number of TP\",np.sum((data_red_wine_one_vs_all[f'{i}']['red_wine_y_test']+red_wine_y_pred)>1))\n",
    "    print(\"Number of TP\",np.sum((data_red_wine_one_vs_all[f'{i}']['red_wine_y_test']&red_wine_y_pred)==1))\n",
    "    #print(predicted_as_1_in_any_model)\n",
    "    print(number_samples_final)\n",
    "    cm = confusion_matrix(data_red_wine_one_vs_all[f'{i}']['red_wine_y_test'], red_wine_y_pred)\n",
    "    print(f\"Confusion Matrix para classe {i}:\\n{cm}\")\n",
    "    tn, fp, fn, tp = confusion_matrix(data_red_wine_one_vs_all[f'{i}']['red_wine_y_test'], red_wine_y_pred).ravel()\n",
    "    print({'TN': tn, 'FP': fp, 'FN': fn, 'TP': tp})\n",
    "    fpp = fpp + fp\n",
    "    fnn = fn + fnn\n",
    "    tnn = tn + tnn\n",
    "    tpp = tpp + tp\n",
    "\n",
    "## tabela juntando os valores previstos como verdadeiros\n",
    "macro_avgs = {\"precision macro avg\":0,\"recall macro avg\":0,\"f1-score macro avg\":0}\n",
    "weighted_avg = {\"precision weighted avg\":0,\"recall weighted avg\":0,\"f1-score weighted avg\":0}\n",
    "number_samples= 0\n",
    "\n",
    "for i in np.sort(data_red_wine['quality'].unique()):\n",
    "    macro_avgs[\"precision macro avg\"] = macro_avgs[\"precision macro avg\"] + class_rep[f'{i}'][\"1\"][\"precision\"]\n",
    "    weighted_avg[\"precision weighted avg\"] = weighted_avg[\"precision weighted avg\"] + class_rep[f'{i}'][\"1\"][\"precision\"]*class_rep[f'{i}'][\"1\"][\"support\"]\n",
    "    macro_avgs[\"recall macro avg\"] = macro_avgs[\"recall macro avg\"] + class_rep[f'{i}'][\"1\"][\"recall\"]\n",
    "    weighted_avg[\"recall weighted avg\"] = weighted_avg[\"recall weighted avg\"] + class_rep[f'{i}'][\"1\"][\"recall\"]*class_rep[f'{i}'][\"1\"][\"support\"]    \n",
    "    macro_avgs[\"f1-score macro avg\"] = macro_avgs[\"f1-score macro avg\"] + class_rep[f'{i}'][\"1\"][\"f1-score\"]\n",
    "    weighted_avg[\"f1-score weighted avg\"] = weighted_avg[\"f1-score weighted avg\"] + class_rep[f'{i}'][\"1\"][\"f1-score\"]*class_rep[f'{i}'][\"1\"][\"support\"]\n",
    "    number_samples = number_samples + class_rep[f'{i}'][\"1\"][\"support\"]\n",
    "\n",
    "for i in macro_avgs:\n",
    "    macro_avgs[i] = macro_avgs[i]/len(np.sort(data_red_wine['quality'].unique()))\n",
    "\n",
    "for i in weighted_avg:\n",
    "    weighted_avg[i] = weighted_avg[i]/number_samples\n",
    "\n",
    "print(\"Para a analise total temos as medias:\")\n",
    "print(\"Para macro avg -> \",macro_avgs)\n",
    "print(\"Para weighted avg -> \",weighted_avg)\n",
    "print(\"Numero de amostrar que deram true em algum momento: \",number_samples_final)\n",
    "print(\"Pela formula: \",calculo_numero_amostras)\n",
    "print(\"Numero de Amostras que sao classificadas mais de uma vez: \",tpp+fpp)\n",
    "print(\"Amostras repetidas pelo menos uma vez\",np.sum(predicted_as_1_in_any_model>1))\n",
    "print(\"Numero real de amostras no dataset teste: \",len(data_red_wine_one_vs_all['6']['red_wine_y_test']))\n",
    "print('VALORES GERAIS -> FP: ',fpp,'FN: ',fnn,'TN ',tnn,\"TP\",tpp)\n",
    "print(f\"Overall-> Precision: {tpp/(tpp+fpp)} | Recall: {tpp/(tpp+fnn)}|F1-Score: {2/((1/(tpp/(tpp+fpp))) + (1/(tpp/(tpp+fnn))))}\")\n",
    "print(\"Overall Accuracy: \",(tpp+tnn)/(tpp+tnn+fpp+fnn))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the White Wine lr para Qualidade-> 3 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.73      0.84       976\n",
      "           1       0.01      0.50      0.01         4\n",
      "\n",
      "    accuracy                           0.73       980\n",
      "   macro avg       0.50      0.61      0.43       980\n",
      "weighted avg       0.99      0.73      0.84       980\n",
      "\n",
      "Number of FP 266\n",
      "Number of TP 2\n",
      "Number of TP 2\n",
      "268\n",
      "Confusion Matrix para classe 3:\n",
      "[[710 266]\n",
      " [  2   2]]\n",
      "{'TN': 710, 'FP': 266, 'FN': 2, 'TP': 2}\n",
      "For the White Wine lr para Qualidade-> 4 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.79      0.88       947\n",
      "           1       0.09      0.61      0.16        33\n",
      "\n",
      "    accuracy                           0.78       980\n",
      "   macro avg       0.54      0.70      0.52       980\n",
      "weighted avg       0.95      0.78      0.85       980\n",
      "\n",
      "Number of FP 198\n",
      "Number of TP 20\n",
      "Number of TP 20\n",
      "486\n",
      "Confusion Matrix para classe 4:\n",
      "[[749 198]\n",
      " [ 13  20]]\n",
      "{'TN': 749, 'FP': 198, 'FN': 13, 'TP': 20}\n",
      "For the White Wine lr para Qualidade-> 5 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.68      0.76       689\n",
      "           1       0.49      0.73      0.59       291\n",
      "\n",
      "    accuracy                           0.70       980\n",
      "   macro avg       0.68      0.71      0.68       980\n",
      "weighted avg       0.75      0.70      0.71       980\n",
      "\n",
      "Number of FP 218\n",
      "Number of TP 213\n",
      "Number of TP 213\n",
      "917\n",
      "Confusion Matrix para classe 5:\n",
      "[[471 218]\n",
      " [ 78 213]]\n",
      "{'TN': 471, 'FP': 218, 'FN': 78, 'TP': 213}\n",
      "For the White Wine lr para Qualidade-> 6 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.49      0.55       540\n",
      "           1       0.51      0.65      0.57       440\n",
      "\n",
      "    accuracy                           0.56       980\n",
      "   macro avg       0.57      0.57      0.56       980\n",
      "weighted avg       0.57      0.56      0.56       980\n",
      "\n",
      "Number of FP 278\n",
      "Number of TP 285\n",
      "Number of TP 285\n",
      "1480\n",
      "Confusion Matrix para classe 6:\n",
      "[[262 278]\n",
      " [155 285]]\n",
      "{'TN': 262, 'FP': 278, 'FN': 155, 'TP': 285}\n",
      "For the White Wine lr para Qualidade-> 7 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.67      0.78       804\n",
      "           1       0.33      0.74      0.46       176\n",
      "\n",
      "    accuracy                           0.68       980\n",
      "   macro avg       0.63      0.71      0.62       980\n",
      "weighted avg       0.82      0.68      0.72       980\n",
      "\n",
      "Number of FP 263\n",
      "Number of TP 130\n",
      "Number of TP 130\n",
      "1873\n",
      "Confusion Matrix para classe 7:\n",
      "[[541 263]\n",
      " [ 46 130]]\n",
      "{'TN': 541, 'FP': 263, 'FN': 46, 'TP': 130}\n",
      "For the White Wine lr para Qualidade-> 8 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.71      0.82       945\n",
      "           1       0.07      0.63      0.13        35\n",
      "\n",
      "    accuracy                           0.71       980\n",
      "   macro avg       0.53      0.67      0.48       980\n",
      "weighted avg       0.95      0.71      0.80       980\n",
      "\n",
      "Number of FP 276\n",
      "Number of TP 22\n",
      "Number of TP 22\n",
      "2171\n",
      "Confusion Matrix para classe 8:\n",
      "[[669 276]\n",
      " [ 13  22]]\n",
      "{'TN': 669, 'FP': 276, 'FN': 13, 'TP': 22}\n",
      "For the White Wine lr para Qualidade-> 9 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       979\n",
      "           1       0.04      1.00      0.08         1\n",
      "\n",
      "    accuracy                           0.98       980\n",
      "   macro avg       0.52      0.99      0.54       980\n",
      "weighted avg       1.00      0.98      0.99       980\n",
      "\n",
      "Number of FP 22\n",
      "Number of TP 1\n",
      "Number of TP 1\n",
      "2194\n",
      "Confusion Matrix para classe 9:\n",
      "[[957  22]\n",
      " [  0   1]]\n",
      "{'TN': 957, 'FP': 22, 'FN': 0, 'TP': 1}\n",
      "Para a analise total temos as medias:\n",
      "Para macro avg ->  {'precision macro avg': 0.22110208662322786, 'recall macro avg': 0.6932792048403241, 'f1-score macro avg': 0.2863998166601713}\n",
      "Para weighted avg ->  {'precision weighted avg': 0.43923569938867, 'recall weighted avg': 0.686734693877551, 'f1-score weighted avg': 0.5226484179999057}\n",
      "Numero de amostrar que deram true em algum momento:  2194\n",
      "Pela formula:  0\n",
      "Numero de Amostras que sao classificadas mais de uma vez:  2194\n",
      "Amostras repetidas pelo menos uma vez 780\n",
      "Numero real de amostras no dataset teste:  980\n",
      "VALORES GERAIS -> FP:  1521 FN:  307 TN  4359 TP 673\n",
      "Overall-> Precision: 0.3067456700091158 | Recall: 0.686734693877551|F1-Score: 0.4240705734089477\n",
      "Overall Accuracy:  0.7335276967930029\n"
     ]
    }
   ],
   "source": [
    "## white wine\n",
    "\n",
    "class_rep = {}\n",
    "number_samples_final = 0\n",
    "predicted_as_1_in_any_model = np.zeros_like(data_white_wine_one_vs_all['6']['white_wine_y_test'], dtype=int)\n",
    "fpp = 0\n",
    "fnn = 0\n",
    "tnn = 0\n",
    "tpp = 0\n",
    "\n",
    "for i in np.sort(data_white_wine['quality'].unique()):\n",
    "    white_wine_lr_model = LogisticRegression(max_iter=5000)\n",
    "    white_wine_lr_model.fit(data_white_wine_one_vs_all[f'{i}']['white_wine_X_train'],data_white_wine_one_vs_all[f'{i}']['white_wine_y_train'])\n",
    "    #use the lr model to try to pwhiteict the test dataset\n",
    "    white_wine_y_pred = white_wine_lr_model.predict(data_white_wine_one_vs_all[f'{i}']['white_wine_X_test'])\n",
    "    ## for better understanding of precition and recall\n",
    "    ## https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "    print('For the White Wine lr para Qualidade->',i,'\\n',classification_report(data_white_wine_one_vs_all[f'{i}']['white_wine_y_test'],white_wine_y_pred,zero_division=True))\n",
    "    class_rep[f'{i}'] = classification_report(data_white_wine_one_vs_all[f'{i}']['white_wine_y_test'],white_wine_y_pred,output_dict=True)\n",
    "\n",
    "    number_samples_final += np.sum(white_wine_y_pred == 1)\n",
    "    #print(white_wine_y_pred)\n",
    "    predicted_as_1_in_any_model = predicted_as_1_in_any_model + white_wine_y_pred\n",
    "    print(\"Number of FP\",np.sum((~data_white_wine_one_vs_all[f'{i}']['white_wine_y_test'] & white_wine_y_pred)==1))\n",
    "    print(\"Number of TP\",np.sum((data_white_wine_one_vs_all[f'{i}']['white_wine_y_test']+white_wine_y_pred)>1))\n",
    "    print(\"Number of TP\",np.sum((data_white_wine_one_vs_all[f'{i}']['white_wine_y_test']&white_wine_y_pred)==1))\n",
    "    #print(predicted_as_1_in_any_model)\n",
    "    print(number_samples_final)\n",
    "    cm = confusion_matrix(data_white_wine_one_vs_all[f'{i}']['white_wine_y_test'], white_wine_y_pred)\n",
    "    print(f\"Confusion Matrix para classe {i}:\\n{cm}\")\n",
    "    tn, fp, fn, tp = confusion_matrix(data_white_wine_one_vs_all[f'{i}']['white_wine_y_test'], white_wine_y_pred).ravel()\n",
    "    print({'TN': tn, 'FP': fp, 'FN': fn, 'TP': tp})\n",
    "    fpp = fpp + fp\n",
    "    fnn = fn + fnn\n",
    "    tnn = tn + tnn\n",
    "    tpp = tpp + tp\n",
    "\n",
    "## tabela juntando os valores previstos como verdadeiros\n",
    "macro_avgs = {\"precision macro avg\":0,\"recall macro avg\":0,\"f1-score macro avg\":0}\n",
    "weighted_avg = {\"precision weighted avg\":0,\"recall weighted avg\":0,\"f1-score weighted avg\":0}\n",
    "number_samples= 0\n",
    "for i in np.sort(data_white_wine['quality'].unique()):\n",
    "    macro_avgs[\"precision macro avg\"] = macro_avgs[\"precision macro avg\"] + class_rep[f'{i}'][\"1\"][\"precision\"]\n",
    "    weighted_avg[\"precision weighted avg\"] = weighted_avg[\"precision weighted avg\"] + class_rep[f'{i}'][\"1\"][\"precision\"]*class_rep[f'{i}'][\"1\"][\"support\"]\n",
    "    macro_avgs[\"recall macro avg\"] = macro_avgs[\"recall macro avg\"] + class_rep[f'{i}'][\"1\"][\"recall\"]\n",
    "    weighted_avg[\"recall weighted avg\"] = weighted_avg[\"recall weighted avg\"] + class_rep[f'{i}'][\"1\"][\"recall\"]*class_rep[f'{i}'][\"1\"][\"support\"]    \n",
    "    macro_avgs[\"f1-score macro avg\"] = macro_avgs[\"f1-score macro avg\"] + class_rep[f'{i}'][\"1\"][\"f1-score\"]\n",
    "    weighted_avg[\"f1-score weighted avg\"] = weighted_avg[\"f1-score weighted avg\"] + class_rep[f'{i}'][\"1\"][\"f1-score\"]*class_rep[f'{i}'][\"1\"][\"support\"]\n",
    "    number_samples = number_samples + class_rep[f'{i}'][\"1\"][\"support\"]\n",
    "\n",
    "for i in macro_avgs:\n",
    "    macro_avgs[i] = macro_avgs[i]/len(np.sort(data_white_wine['quality'].unique()))\n",
    "\n",
    "for i in weighted_avg:\n",
    "    weighted_avg[i] = weighted_avg[i]/number_samples\n",
    "\n",
    "print(\"Para a analise total temos as medias:\")\n",
    "print(\"Para macro avg -> \",macro_avgs)\n",
    "print(\"Para weighted avg -> \",weighted_avg)\n",
    "print(\"Numero de amostrar que deram true em algum momento: \",number_samples_final)\n",
    "print(\"Pela formula: \",calculo_numero_amostras)\n",
    "print(\"Numero de Amostras que sao classificadas mais de uma vez: \",tpp+fpp)\n",
    "print(\"Amostras repetidas pelo menos uma vez\",np.sum(predicted_as_1_in_any_model>1))\n",
    "print(\"Numero real de amostras no dataset teste: \",len(data_white_wine_one_vs_all['6']['white_wine_y_test']))\n",
    "print('VALORES GERAIS -> FP: ',fpp,'FN: ',fnn,'TN ',tnn,\"TP\",tpp)\n",
    "print(f\"Overall-> Precision: {tpp/(tpp+fpp)} | Recall: {tpp/(tpp+fnn)}|F1-Score: {2/((1/(tpp/(tpp+fpp))) + (1/(tpp/(tpp+fnn))))}\")\n",
    "print(\"Overall Accuracy: \",(tpp+tnn)/(tpp+tnn+fpp+fnn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the Red Wine dtc para Qualidade-> 3 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99       317\n",
      "           1       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.98       320\n",
      "   macro avg       0.50      0.50      0.50       320\n",
      "weighted avg       0.98      0.98      0.98       320\n",
      "\n",
      "Number of FP 2\n",
      "Number of TP 0\n",
      "Number of TP 0\n",
      "2\n",
      "Confusion Matrix para classe 3:\n",
      "[[315   2]\n",
      " [  3   0]]\n",
      "{'TN': 315, 'FP': 2, 'FN': 3, 'TP': 0}\n",
      "For the Red Wine dtc para Qualidade-> 4 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.97      0.97       310\n",
      "           1       0.09      0.10      0.10        10\n",
      "\n",
      "    accuracy                           0.94       320\n",
      "   macro avg       0.53      0.53      0.53       320\n",
      "weighted avg       0.94      0.94      0.94       320\n",
      "\n",
      "Number of FP 10\n",
      "Number of TP 1\n",
      "Number of TP 1\n",
      "13\n",
      "Confusion Matrix para classe 4:\n",
      "[[300  10]\n",
      " [  9   1]]\n",
      "{'TN': 300, 'FP': 10, 'FN': 9, 'TP': 1}\n",
      "For the Red Wine dtc para Qualidade-> 5 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.72      0.75       193\n",
      "           1       0.62      0.72      0.67       127\n",
      "\n",
      "    accuracy                           0.72       320\n",
      "   macro avg       0.71      0.72      0.71       320\n",
      "weighted avg       0.73      0.72      0.72       320\n",
      "\n",
      "Number of FP 55\n",
      "Number of TP 91\n",
      "Number of TP 91\n",
      "159\n",
      "Confusion Matrix para classe 5:\n",
      "[[138  55]\n",
      " [ 36  91]]\n",
      "{'TN': 138, 'FP': 55, 'FN': 36, 'TP': 91}\n",
      "For the Red Wine dtc para Qualidade-> 6 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.74      0.70       179\n",
      "           1       0.61      0.52      0.56       141\n",
      "\n",
      "    accuracy                           0.64       320\n",
      "   macro avg       0.64      0.63      0.63       320\n",
      "weighted avg       0.64      0.64      0.64       320\n",
      "\n",
      "Number of FP 47\n",
      "Number of TP 74\n",
      "Number of TP 74\n",
      "280\n",
      "Confusion Matrix para classe 6:\n",
      "[[132  47]\n",
      " [ 67  74]]\n",
      "{'TN': 132, 'FP': 47, 'FN': 67, 'TP': 74}\n",
      "For the Red Wine dtc para Qualidade-> 7 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.92      0.91       282\n",
      "           1       0.34      0.32      0.33        38\n",
      "\n",
      "    accuracy                           0.85       320\n",
      "   macro avg       0.63      0.62      0.62       320\n",
      "weighted avg       0.84      0.85      0.84       320\n",
      "\n",
      "Number of FP 23\n",
      "Number of TP 12\n",
      "Number of TP 12\n",
      "315\n",
      "Confusion Matrix para classe 7:\n",
      "[[259  23]\n",
      " [ 26  12]]\n",
      "{'TN': 259, 'FP': 23, 'FN': 26, 'TP': 12}\n",
      "For the Red Wine dtc para Qualidade-> 8 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       319\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.98       320\n",
      "   macro avg       0.50      0.49      0.50       320\n",
      "weighted avg       0.99      0.98      0.99       320\n",
      "\n",
      "Number of FP 5\n",
      "Number of TP 0\n",
      "Number of TP 0\n",
      "320\n",
      "Confusion Matrix para classe 8:\n",
      "[[314   5]\n",
      " [  1   0]]\n",
      "{'TN': 314, 'FP': 5, 'FN': 1, 'TP': 0}\n",
      "Para a analise total temos as medias:\n",
      "Para macro avg ->  {'precision macro avg': 0.2781040254888325, 'recall macro avg': 0.2761912669650896, 'f1-score macro avg': 0.2759262302292732}\n",
      "Para weighted avg ->  {'precision weighted avg': 0.5603956298216105, 'recall weighted avg': 0.55625, 'f1-score weighted avg': 0.55550329145566}\n",
      "Numero de amostrar que deram true em algum momento:  320\n",
      "Pela formula:  0\n",
      "Numero de Amostras que sao classificadas mais de uma vez:  320\n",
      "Amostras repetidas pelo menos uma vez 57\n",
      "Numero real de amostras no dataset teste:  320\n",
      "VALORES GERAIS -> FP:  142 FN:  142 TN  1458 TP 178\n",
      "Overall-> Precision: 0.55625 | Recall: 0.55625|F1-Score: 0.55625\n",
      "Overall Accuracy:  0.8520833333333333\n"
     ]
    }
   ],
   "source": [
    "## red wine\n",
    "\n",
    "class_rep = {}\n",
    "number_samples_final = 0\n",
    "predicted_as_1_in_any_model = np.zeros_like(data_red_wine_one_vs_all['6']['red_wine_y_test'], dtype=int)\n",
    "fpp = 0\n",
    "fnn = 0\n",
    "tnn = 0\n",
    "tpp = 0\n",
    "\n",
    "for i in np.sort(data_red_wine['quality'].unique()):\n",
    "    red_wine_dtc_model = DecisionTreeClassifier()\n",
    "    red_wine_dtc_model.fit(data_red_wine_one_vs_all[f'{i}']['red_wine_X_train'],data_red_wine_one_vs_all[f'{i}']['red_wine_y_train'])\n",
    "    #use the dtc model to try to predict the test dataset\n",
    "    red_wine_y_pred = red_wine_dtc_model.predict(data_red_wine_one_vs_all[f'{i}']['red_wine_X_test'])\n",
    "    ## for better understanding of precition and recall\n",
    "    ## https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "    print('For the Red Wine dtc para Qualidade->',i,'\\n',classification_report(data_red_wine_one_vs_all[f'{i}']['red_wine_y_test'],red_wine_y_pred))\n",
    "    class_rep[f'{i}'] = classification_report(data_red_wine_one_vs_all[f'{i}']['red_wine_y_test'],red_wine_y_pred,output_dict=True)\n",
    "\n",
    "    number_samples_final += np.sum(red_wine_y_pred == 1)\n",
    "    #print(red_wine_y_pred)\n",
    "    predicted_as_1_in_any_model = predicted_as_1_in_any_model + red_wine_y_pred\n",
    "    print(\"Number of FP\",np.sum((~data_red_wine_one_vs_all[f'{i}']['red_wine_y_test'] & red_wine_y_pred)==1))\n",
    "    print(\"Number of TP\",np.sum((data_red_wine_one_vs_all[f'{i}']['red_wine_y_test']+red_wine_y_pred)>1))\n",
    "    print(\"Number of TP\",np.sum((data_red_wine_one_vs_all[f'{i}']['red_wine_y_test']&red_wine_y_pred)==1))\n",
    "    #print(predicted_as_1_in_any_model)\n",
    "    print(number_samples_final)\n",
    "    cm = confusion_matrix(data_red_wine_one_vs_all[f'{i}']['red_wine_y_test'], red_wine_y_pred)\n",
    "    print(f\"Confusion Matrix para classe {i}:\\n{cm}\")\n",
    "    tn, fp, fn, tp = confusion_matrix(data_red_wine_one_vs_all[f'{i}']['red_wine_y_test'], red_wine_y_pred).ravel()\n",
    "    print({'TN': tn, 'FP': fp, 'FN': fn, 'TP': tp})\n",
    "    fpp = fpp + fp\n",
    "    fnn = fn + fnn\n",
    "    tnn = tn + tnn\n",
    "    tpp = tpp + tp\n",
    "\n",
    "## tabela juntando os valores previstos como verdadeiros\n",
    "macro_avgs = {\"precision macro avg\":0,\"recall macro avg\":0,\"f1-score macro avg\":0}\n",
    "weighted_avg = {\"precision weighted avg\":0,\"recall weighted avg\":0,\"f1-score weighted avg\":0}\n",
    "number_samples= 0\n",
    "\n",
    "for i in np.sort(data_red_wine['quality'].unique()):\n",
    "    macro_avgs[\"precision macro avg\"] = macro_avgs[\"precision macro avg\"] + class_rep[f'{i}'][\"1\"][\"precision\"]\n",
    "    weighted_avg[\"precision weighted avg\"] = weighted_avg[\"precision weighted avg\"] + class_rep[f'{i}'][\"1\"][\"precision\"]*class_rep[f'{i}'][\"1\"][\"support\"]\n",
    "    macro_avgs[\"recall macro avg\"] = macro_avgs[\"recall macro avg\"] + class_rep[f'{i}'][\"1\"][\"recall\"]\n",
    "    weighted_avg[\"recall weighted avg\"] = weighted_avg[\"recall weighted avg\"] + class_rep[f'{i}'][\"1\"][\"recall\"]*class_rep[f'{i}'][\"1\"][\"support\"]    \n",
    "    macro_avgs[\"f1-score macro avg\"] = macro_avgs[\"f1-score macro avg\"] + class_rep[f'{i}'][\"1\"][\"f1-score\"]\n",
    "    weighted_avg[\"f1-score weighted avg\"] = weighted_avg[\"f1-score weighted avg\"] + class_rep[f'{i}'][\"1\"][\"f1-score\"]*class_rep[f'{i}'][\"1\"][\"support\"]\n",
    "    number_samples = number_samples + class_rep[f'{i}'][\"1\"][\"support\"]\n",
    "\n",
    "for i in macro_avgs:\n",
    "    macro_avgs[i] = macro_avgs[i]/len(np.sort(data_red_wine['quality'].unique()))\n",
    "\n",
    "for i in weighted_avg:\n",
    "    weighted_avg[i] = weighted_avg[i]/number_samples\n",
    "\n",
    "print(\"Para a analise total temos as medias:\")\n",
    "print(\"Para macro avg -> \",macro_avgs)\n",
    "print(\"Para weighted avg -> \",weighted_avg)\n",
    "print(\"Numero de amostrar que deram true em algum momento: \",number_samples_final)\n",
    "print(\"Pela formula: \",calculo_numero_amostras)\n",
    "print(\"Numero de Amostras que sao classificadas mais de uma vez: \",tpp+fpp)\n",
    "print(\"Amostras repetidas pelo menos uma vez\",np.sum(predicted_as_1_in_any_model>1))\n",
    "print(\"Numero real de amostras no dataset teste: \",len(data_red_wine_one_vs_all['6']['red_wine_y_test']))\n",
    "print('VALORES GERAIS -> FP: ',fpp,'FN: ',fnn,'TN ',tnn,\"TP\",tpp)\n",
    "print(f\"Overall-> Precision: {tpp/(tpp+fpp)} | Recall: {tpp/(tpp+fnn)}|F1-Score: {2/((1/(tpp/(tpp+fpp))) + (1/(tpp/(tpp+fnn))))}\")\n",
    "print(\"Overall Accuracy: \",(tpp+tnn)/(tpp+tnn+fpp+fnn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the White Wine dtc para Qualidade-> 3 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       976\n",
      "           1       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.99       980\n",
      "   macro avg       0.50      0.50      0.50       980\n",
      "weighted avg       0.99      0.99      0.99       980\n",
      "\n",
      "Number of FP 2\n",
      "Number of TP 0\n",
      "Number of TP 0\n",
      "2\n",
      "Confusion Matrix para classe 3:\n",
      "[[974   2]\n",
      " [  4   0]]\n",
      "{'TN': 974, 'FP': 2, 'FN': 4, 'TP': 0}\n",
      "For the White Wine dtc para Qualidade-> 4 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.97       947\n",
      "           1       0.28      0.30      0.29        33\n",
      "\n",
      "    accuracy                           0.95       980\n",
      "   macro avg       0.63      0.64      0.63       980\n",
      "weighted avg       0.95      0.95      0.95       980\n",
      "\n",
      "Number of FP 26\n",
      "Number of TP 10\n",
      "Number of TP 10\n",
      "38\n",
      "Confusion Matrix para classe 4:\n",
      "[[921  26]\n",
      " [ 23  10]]\n",
      "{'TN': 921, 'FP': 26, 'FN': 23, 'TP': 10}\n",
      "For the White Wine dtc para Qualidade-> 5 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.83      0.82       689\n",
      "           1       0.58      0.57      0.58       291\n",
      "\n",
      "    accuracy                           0.75       980\n",
      "   macro avg       0.70      0.70      0.70       980\n",
      "weighted avg       0.75      0.75      0.75       980\n",
      "\n",
      "Number of FP 119\n",
      "Number of TP 166\n",
      "Number of TP 166\n",
      "323\n",
      "Confusion Matrix para classe 5:\n",
      "[[570 119]\n",
      " [125 166]]\n",
      "{'TN': 570, 'FP': 119, 'FN': 125, 'TP': 166}\n",
      "For the White Wine dtc para Qualidade-> 6 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.67      0.67       540\n",
      "           1       0.60      0.62      0.61       440\n",
      "\n",
      "    accuracy                           0.64       980\n",
      "   macro avg       0.64      0.64      0.64       980\n",
      "weighted avg       0.65      0.64      0.65       980\n",
      "\n",
      "Number of FP 180\n",
      "Number of TP 272\n",
      "Number of TP 272\n",
      "775\n",
      "Confusion Matrix para classe 6:\n",
      "[[360 180]\n",
      " [168 272]]\n",
      "{'TN': 360, 'FP': 180, 'FN': 168, 'TP': 272}\n",
      "For the White Wine dtc para Qualidade-> 7 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.91      0.90       804\n",
      "           1       0.54      0.47      0.50       176\n",
      "\n",
      "    accuracy                           0.83       980\n",
      "   macro avg       0.72      0.69      0.70       980\n",
      "weighted avg       0.83      0.83      0.83       980\n",
      "\n",
      "Number of FP 70\n",
      "Number of TP 83\n",
      "Number of TP 83\n",
      "928\n",
      "Confusion Matrix para classe 7:\n",
      "[[734  70]\n",
      " [ 93  83]]\n",
      "{'TN': 734, 'FP': 70, 'FN': 93, 'TP': 83}\n",
      "For the White Wine dtc para Qualidade-> 8 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.98      0.98       945\n",
      "           1       0.38      0.31      0.34        35\n",
      "\n",
      "    accuracy                           0.96       980\n",
      "   macro avg       0.68      0.65      0.66       980\n",
      "weighted avg       0.95      0.96      0.96       980\n",
      "\n",
      "Number of FP 18\n",
      "Number of TP 11\n",
      "Number of TP 11\n",
      "957\n",
      "Confusion Matrix para classe 8:\n",
      "[[927  18]\n",
      " [ 24  11]]\n",
      "{'TN': 927, 'FP': 18, 'FN': 24, 'TP': 11}\n",
      "For the White Wine dtc para Qualidade-> 9 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       979\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           1.00       980\n",
      "   macro avg       0.50      0.50      0.50       980\n",
      "weighted avg       1.00      1.00      1.00       980\n",
      "\n",
      "Number of FP 1\n",
      "Number of TP 0\n",
      "Number of TP 0\n",
      "958\n",
      "Confusion Matrix para classe 9:\n",
      "[[978   1]\n",
      " [  1   0]]\n",
      "{'TN': 978, 'FP': 1, 'FN': 1, 'TP': 0}\n",
      "Para a analise total temos as medias:\n",
      "Para macro avg ->  {'precision macro avg': 0.34054254779876925, 'recall macro avg': 0.32536221142627625, 'f1-score macro avg': 0.3320598146744846}\n",
      "Para weighted avg ->  {'precision weighted avg': 0.563462398798664, 'recall weighted avg': 0.5530612244897959, 'f1-score weighted avg': 0.5576212988202053}\n",
      "Numero de amostrar que deram true em algum momento:  958\n",
      "Pela formula:  0\n",
      "Numero de Amostras que sao classificadas mais de uma vez:  958\n",
      "Amostras repetidas pelo menos uma vez 187\n",
      "Amostras NÃO CLassificadas 217\n",
      "Numero real de amostras no dataset teste:  980\n",
      "VALORES GERAIS -> FP:  416 FN:  438 TN  5464 TP 542\n",
      "Overall-> Precision: 0.5657620041753654 | Recall: 0.5530612244897959|F1-Score: 0.5593395252837977\n",
      "Overall Accuracy:  0.8755102040816326\n"
     ]
    }
   ],
   "source": [
    "## white wine\n",
    "\n",
    "class_rep = {}\n",
    "number_samples_final = 0\n",
    "predicted_as_1_in_any_model = np.zeros_like(data_white_wine_one_vs_all['6']['white_wine_y_test'], dtype=int)\n",
    "fpp = 0\n",
    "fnn = 0\n",
    "tnn = 0\n",
    "tpp = 0\n",
    "\n",
    "for i in np.sort(data_white_wine['quality'].unique()):\n",
    "    white_wine_dtc_model = DecisionTreeClassifier()\n",
    "    white_wine_dtc_model.fit(data_white_wine_one_vs_all[f'{i}']['white_wine_X_train'],data_white_wine_one_vs_all[f'{i}']['white_wine_y_train'])\n",
    "    #use the dtc model to try to pwhiteict the test dataset\n",
    "    white_wine_y_pred = white_wine_dtc_model.predict(data_white_wine_one_vs_all[f'{i}']['white_wine_X_test'])\n",
    "    ## for better understanding of precition and recall\n",
    "    ## https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "    print('For the White Wine dtc para Qualidade->',i,'\\n',classification_report(data_white_wine_one_vs_all[f'{i}']['white_wine_y_test'],white_wine_y_pred,zero_division=True))\n",
    "    class_rep[f'{i}'] = classification_report(data_white_wine_one_vs_all[f'{i}']['white_wine_y_test'],white_wine_y_pred,output_dict=True)\n",
    "    \n",
    "    number_samples_final += np.sum(white_wine_y_pred == 1)\n",
    "    #print(white_wine_y_pred)\n",
    "    predicted_as_1_in_any_model = predicted_as_1_in_any_model + white_wine_y_pred\n",
    "    print(\"Number of FP\",np.sum((~data_white_wine_one_vs_all[f'{i}']['white_wine_y_test'] & white_wine_y_pred)==1))\n",
    "    print(\"Number of TP\",np.sum((data_white_wine_one_vs_all[f'{i}']['white_wine_y_test']+white_wine_y_pred)>1))\n",
    "    print(\"Number of TP\",np.sum((data_white_wine_one_vs_all[f'{i}']['white_wine_y_test']&white_wine_y_pred)==1))\n",
    "    #print(predicted_as_1_in_any_model)\n",
    "    print(number_samples_final)\n",
    "    cm = confusion_matrix(data_white_wine_one_vs_all[f'{i}']['white_wine_y_test'], white_wine_y_pred)\n",
    "    print(f\"Confusion Matrix para classe {i}:\\n{cm}\")\n",
    "    tn, fp, fn, tp = confusion_matrix(data_white_wine_one_vs_all[f'{i}']['white_wine_y_test'], white_wine_y_pred).ravel()\n",
    "    print({'TN': tn, 'FP': fp, 'FN': fn, 'TP': tp})\n",
    "    fpp = fpp + fp\n",
    "    fnn = fn + fnn\n",
    "    tnn = tn + tnn\n",
    "    tpp = tpp + tp\n",
    "\n",
    "## tabela juntando os valores previstos como verdadeiros\n",
    "macro_avgs = {\"precision macro avg\":0,\"recall macro avg\":0,\"f1-score macro avg\":0}\n",
    "weighted_avg = {\"precision weighted avg\":0,\"recall weighted avg\":0,\"f1-score weighted avg\":0}\n",
    "number_samples= 0\n",
    "for i in np.sort(data_white_wine['quality'].unique()):\n",
    "    macro_avgs[\"precision macro avg\"] = macro_avgs[\"precision macro avg\"] + class_rep[f'{i}'][\"1\"][\"precision\"]\n",
    "    weighted_avg[\"precision weighted avg\"] = weighted_avg[\"precision weighted avg\"] + class_rep[f'{i}'][\"1\"][\"precision\"]*class_rep[f'{i}'][\"1\"][\"support\"]\n",
    "    macro_avgs[\"recall macro avg\"] = macro_avgs[\"recall macro avg\"] + class_rep[f'{i}'][\"1\"][\"recall\"]\n",
    "    weighted_avg[\"recall weighted avg\"] = weighted_avg[\"recall weighted avg\"] + class_rep[f'{i}'][\"1\"][\"recall\"]*class_rep[f'{i}'][\"1\"][\"support\"]    \n",
    "    macro_avgs[\"f1-score macro avg\"] = macro_avgs[\"f1-score macro avg\"] + class_rep[f'{i}'][\"1\"][\"f1-score\"]\n",
    "    weighted_avg[\"f1-score weighted avg\"] = weighted_avg[\"f1-score weighted avg\"] + class_rep[f'{i}'][\"1\"][\"f1-score\"]*class_rep[f'{i}'][\"1\"][\"support\"]\n",
    "    number_samples = number_samples + class_rep[f'{i}'][\"1\"][\"support\"]\n",
    "\n",
    "for i in macro_avgs:\n",
    "    macro_avgs[i] = macro_avgs[i]/len(np.sort(data_white_wine['quality'].unique()))\n",
    "\n",
    "for i in weighted_avg:\n",
    "    weighted_avg[i] = weighted_avg[i]/number_samples\n",
    "\n",
    "print(\"Para a analise total temos as medias:\")\n",
    "print(\"Para macro avg -> \",macro_avgs)\n",
    "print(\"Para weighted avg -> \",weighted_avg)\n",
    "print(\"Numero de amostrar que deram true em algum momento: \",number_samples_final)\n",
    "print(\"Pela formula: \",calculo_numero_amostras)\n",
    "print(\"Numero de Amostras que sao classificadas mais de uma vez: \",tpp+fpp)\n",
    "print(\"Amostras repetidas pelo menos uma vez\",np.sum(predicted_as_1_in_any_model>1))\n",
    "print(\"Amostras NÃO CLassificadas\",np.sum(predicted_as_1_in_any_model==0))\n",
    "\n",
    "print(\"Numero real de amostras no dataset teste: \",len(data_white_wine_one_vs_all['6']['white_wine_y_test']))\n",
    "print('VALORES GERAIS -> FP: ',fpp,'FN: ',fnn,'TN ',tnn,\"TP\",tpp)\n",
    "print(f\"Overall-> Precision: {tpp/(tpp+fpp)} | Recall: {tpp/(tpp+fnn)}|F1-Score: {2/((1/(tpp/(tpp+fpp))) + (1/(tpp/(tpp+fnn))))}\")\n",
    "print(\"Overall Accuracy: \",(tpp+tnn)/(tpp+tnn+fpp+fnn))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vinic\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\vinic\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\vinic\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\vinic\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\vinic\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\vinic\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the Red Wine rfc para Qualidade-> 3 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      1.00       317\n",
      "           1       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.99       320\n",
      "   macro avg       0.50      0.50      0.50       320\n",
      "weighted avg       0.98      0.99      0.99       320\n",
      "\n",
      "Number of FP 0\n",
      "Number of TP 0\n",
      "Number of TP 0\n",
      "0\n",
      "Confusion Matrix para classe 3:\n",
      "[[317   0]\n",
      " [  3   0]]\n",
      "{'TN': 317, 'FP': 0, 'FN': 3, 'TP': 0}\n",
      "For the Red Wine rfc para Qualidade-> 4 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       310\n",
      "           1       0.50      0.10      0.17        10\n",
      "\n",
      "    accuracy                           0.97       320\n",
      "   macro avg       0.74      0.55      0.58       320\n",
      "weighted avg       0.96      0.97      0.96       320\n",
      "\n",
      "Number of FP 1\n",
      "Number of TP 1\n",
      "Number of TP 1\n",
      "2\n",
      "Confusion Matrix para classe 4:\n",
      "[[309   1]\n",
      " [  9   1]]\n",
      "{'TN': 309, 'FP': 1, 'FN': 9, 'TP': 1}\n",
      "For the Red Wine rfc para Qualidade-> 5 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.80      0.83       193\n",
      "           1       0.72      0.80      0.76       127\n",
      "\n",
      "    accuracy                           0.80       320\n",
      "   macro avg       0.79      0.80      0.79       320\n",
      "weighted avg       0.80      0.80      0.80       320\n",
      "\n",
      "Number of FP 39\n",
      "Number of TP 101\n",
      "Number of TP 101\n",
      "142\n",
      "Confusion Matrix para classe 5:\n",
      "[[154  39]\n",
      " [ 26 101]]\n",
      "{'TN': 154, 'FP': 39, 'FN': 26, 'TP': 101}\n",
      "For the Red Wine rfc para Qualidade-> 6 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.83      0.75       179\n",
      "           1       0.70      0.51      0.59       141\n",
      "\n",
      "    accuracy                           0.69       320\n",
      "   macro avg       0.69      0.67      0.67       320\n",
      "weighted avg       0.69      0.69      0.68       320\n",
      "\n",
      "Number of FP 31\n",
      "Number of TP 72\n",
      "Number of TP 72\n",
      "245\n",
      "Confusion Matrix para classe 6:\n",
      "[[148  31]\n",
      " [ 69  72]]\n",
      "{'TN': 148, 'FP': 31, 'FN': 69, 'TP': 72}\n",
      "For the Red Wine rfc para Qualidade-> 7 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.97      0.94       282\n",
      "           1       0.62      0.34      0.44        38\n",
      "\n",
      "    accuracy                           0.90       320\n",
      "   macro avg       0.77      0.66      0.69       320\n",
      "weighted avg       0.88      0.90      0.88       320\n",
      "\n",
      "Number of FP 8\n",
      "Number of TP 13\n",
      "Number of TP 13\n",
      "266\n",
      "Confusion Matrix para classe 7:\n",
      "[[274   8]\n",
      " [ 25  13]]\n",
      "{'TN': 274, 'FP': 8, 'FN': 25, 'TP': 13}\n",
      "For the Red Wine rfc para Qualidade-> 8 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       319\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           1.00       320\n",
      "   macro avg       0.50      0.50      0.50       320\n",
      "weighted avg       0.99      1.00      1.00       320\n",
      "\n",
      "Number of FP 0\n",
      "Number of TP 0\n",
      "Number of TP 0\n",
      "266\n",
      "Confusion Matrix para classe 8:\n",
      "[[319   0]\n",
      " [  1   0]]\n",
      "{'TN': 319, 'FP': 0, 'FN': 1, 'TP': 0}\n",
      "Para a analise total temos as medias:\n",
      "Para macro avg ->  {'precision macro avg': 0.4232508861149638, 'recall macro avg': 0.29133652526356935, 'f1-score macro avg': 0.3256771457184493}\n",
      "Para weighted avg ->  {'precision weighted avg': 0.6834635777854832, 'recall weighted avg': 0.584375, 'f1-score weighted avg': 0.6178373160511711}\n",
      "Numero de amostrar que deram true em algum momento:  266\n",
      "Pela formula:  0\n",
      "Numero de Amostras que sao classificadas mais de uma vez:  266\n",
      "Amostras repetidas pelo menos uma vez 9\n",
      "Amostras NÃO CLassificadas 63\n",
      "Numero real de amostras no dataset teste:  320\n",
      "VALORES GERAIS -> FP:  79 FN:  133 TN  1521 TP 187\n",
      "Overall-> Precision: 0.7030075187969925 | Recall: 0.584375|F1-Score: 0.6382252559726962\n",
      "Overall Accuracy:  0.8895833333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vinic\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\vinic\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\vinic\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\vinic\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\vinic\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\vinic\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "## red wine\n",
    "\n",
    "class_rep = {}\n",
    "number_samples_final = 0\n",
    "predicted_as_1_in_any_model = np.zeros_like(data_red_wine_one_vs_all['6']['red_wine_y_test'], dtype=int)\n",
    "fpp = 0\n",
    "fnn = 0\n",
    "tnn = 0\n",
    "tpp = 0\n",
    "\n",
    "for i in np.sort(data_red_wine['quality'].unique()):\n",
    "    red_wine_rfc_model = RandomForestClassifier()\n",
    "    red_wine_rfc_model.fit(data_red_wine_one_vs_all[f'{i}']['red_wine_X_train'],data_red_wine_one_vs_all[f'{i}']['red_wine_y_train'])\n",
    "    #use the rfc model to try to predict the test dataset\n",
    "    red_wine_y_pred = red_wine_rfc_model.predict(data_red_wine_one_vs_all[f'{i}']['red_wine_X_test'])\n",
    "    ## for better understanding of precition and recall\n",
    "    ## https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "    print('For the Red Wine rfc para Qualidade->',i,'\\n',classification_report(data_red_wine_one_vs_all[f'{i}']['red_wine_y_test'],red_wine_y_pred))\n",
    "\n",
    "    class_rep[f'{i}'] = classification_report(data_red_wine_one_vs_all[f'{i}']['red_wine_y_test'],red_wine_y_pred,output_dict=True)\n",
    "    \n",
    "    number_samples_final += np.sum(red_wine_y_pred == 1)\n",
    "    #print(red_wine_y_pred)\n",
    "    predicted_as_1_in_any_model = predicted_as_1_in_any_model + red_wine_y_pred\n",
    "    print(\"Number of FP\",np.sum((~data_red_wine_one_vs_all[f'{i}']['red_wine_y_test'] & red_wine_y_pred)==1))\n",
    "    print(\"Number of TP\",np.sum((data_red_wine_one_vs_all[f'{i}']['red_wine_y_test']+red_wine_y_pred)>1))\n",
    "    print(\"Number of TP\",np.sum((data_red_wine_one_vs_all[f'{i}']['red_wine_y_test']&red_wine_y_pred)==1))\n",
    "    #print(predicted_as_1_in_any_model)\n",
    "    print(number_samples_final)\n",
    "    cm = confusion_matrix(data_red_wine_one_vs_all[f'{i}']['red_wine_y_test'], red_wine_y_pred)\n",
    "    print(f\"Confusion Matrix para classe {i}:\\n{cm}\")\n",
    "    tn, fp, fn, tp = confusion_matrix(data_red_wine_one_vs_all[f'{i}']['red_wine_y_test'], red_wine_y_pred).ravel()\n",
    "    print({'TN': tn, 'FP': fp, 'FN': fn, 'TP': tp})\n",
    "    fpp = fpp + fp\n",
    "    fnn = fn + fnn\n",
    "    tnn = tn + tnn\n",
    "    tpp = tpp + tp\n",
    "\n",
    "## tabela juntando os valores previstos como verdadeiros\n",
    "macro_avgs = {\"precision macro avg\":0,\"recall macro avg\":0,\"f1-score macro avg\":0}\n",
    "weighted_avg = {\"precision weighted avg\":0,\"recall weighted avg\":0,\"f1-score weighted avg\":0}\n",
    "number_samples= 0\n",
    "\n",
    "for i in np.sort(data_red_wine['quality'].unique()):\n",
    "    macro_avgs[\"precision macro avg\"] = macro_avgs[\"precision macro avg\"] + class_rep[f'{i}'][\"1\"][\"precision\"]\n",
    "    weighted_avg[\"precision weighted avg\"] = weighted_avg[\"precision weighted avg\"] + class_rep[f'{i}'][\"1\"][\"precision\"]*class_rep[f'{i}'][\"1\"][\"support\"]\n",
    "    macro_avgs[\"recall macro avg\"] = macro_avgs[\"recall macro avg\"] + class_rep[f'{i}'][\"1\"][\"recall\"]\n",
    "    weighted_avg[\"recall weighted avg\"] = weighted_avg[\"recall weighted avg\"] + class_rep[f'{i}'][\"1\"][\"recall\"]*class_rep[f'{i}'][\"1\"][\"support\"]    \n",
    "    macro_avgs[\"f1-score macro avg\"] = macro_avgs[\"f1-score macro avg\"] + class_rep[f'{i}'][\"1\"][\"f1-score\"]\n",
    "    weighted_avg[\"f1-score weighted avg\"] = weighted_avg[\"f1-score weighted avg\"] + class_rep[f'{i}'][\"1\"][\"f1-score\"]*class_rep[f'{i}'][\"1\"][\"support\"]\n",
    "    number_samples = number_samples + class_rep[f'{i}'][\"1\"][\"support\"]\n",
    "\n",
    "for i in macro_avgs:\n",
    "    macro_avgs[i] = macro_avgs[i]/len(np.sort(data_red_wine['quality'].unique()))\n",
    "\n",
    "for i in weighted_avg:\n",
    "    weighted_avg[i] = weighted_avg[i]/number_samples\n",
    "\n",
    "print(\"Para a analise total temos as medias:\")\n",
    "print(\"Para macro avg -> \",macro_avgs)\n",
    "print(\"Para weighted avg -> \",weighted_avg)\n",
    "print(\"Numero de amostrar que deram true em algum momento: \",number_samples_final)\n",
    "print(\"Pela formula: \",calculo_numero_amostras)\n",
    "print(\"Numero de Amostras que sao classificadas mais de uma vez: \",tpp+fpp)\n",
    "print(\"Amostras repetidas pelo menos uma vez\",np.sum(predicted_as_1_in_any_model>1))\n",
    "print(\"Amostras NÃO CLassificadas\",np.sum(predicted_as_1_in_any_model==0))\n",
    "print(\"Numero real de amostras no dataset teste: \",len(data_red_wine_one_vs_all['6']['red_wine_y_test']))\n",
    "print('VALORES GERAIS -> FP: ',fpp,'FN: ',fnn,'TN ',tnn,\"TP\",tpp)\n",
    "print(f\"Overall-> Precision: {tpp/(tpp+fpp)} | Recall: {tpp/(tpp+fnn)}|F1-Score: {2/((1/(tpp/(tpp+fpp))) + (1/(tpp/(tpp+fnn))))}\")\n",
    "print(\"Overall Accuracy: \",(tpp+tnn)/(tpp+tnn+fpp+fnn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the White Wine rfc para Qualidade-> 3 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       976\n",
      "           1       1.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           1.00       980\n",
      "   macro avg       1.00      0.50      0.50       980\n",
      "weighted avg       1.00      1.00      0.99       980\n",
      "\n",
      "Number of FP 0\n",
      "Number of TP 0\n",
      "Number of TP 0\n",
      "0\n",
      "Confusion Matrix para classe 3:\n",
      "[[976   0]\n",
      " [  4   0]]\n",
      "{'TN': 976, 'FP': 0, 'FN': 4, 'TP': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vinic\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\vinic\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\vinic\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the White Wine rfc para Qualidade-> 4 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.98       947\n",
      "           1       0.75      0.18      0.29        33\n",
      "\n",
      "    accuracy                           0.97       980\n",
      "   macro avg       0.86      0.59      0.64       980\n",
      "weighted avg       0.96      0.97      0.96       980\n",
      "\n",
      "Number of FP 2\n",
      "Number of TP 6\n",
      "Number of TP 6\n",
      "8\n",
      "Confusion Matrix para classe 4:\n",
      "[[945   2]\n",
      " [ 27   6]]\n",
      "{'TN': 945, 'FP': 2, 'FN': 27, 'TP': 6}\n",
      "For the White Wine rfc para Qualidade-> 5 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.88      0.86       689\n",
      "           1       0.69      0.64      0.66       291\n",
      "\n",
      "    accuracy                           0.81       980\n",
      "   macro avg       0.77      0.76      0.76       980\n",
      "weighted avg       0.80      0.81      0.81       980\n",
      "\n",
      "Number of FP 85\n",
      "Number of TP 187\n",
      "Number of TP 187\n",
      "280\n",
      "Confusion Matrix para classe 5:\n",
      "[[604  85]\n",
      " [104 187]]\n",
      "{'TN': 604, 'FP': 85, 'FN': 104, 'TP': 187}\n",
      "For the White Wine rfc para Qualidade-> 6 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.70      0.71       540\n",
      "           1       0.65      0.68      0.66       440\n",
      "\n",
      "    accuracy                           0.69       980\n",
      "   macro avg       0.69      0.69      0.69       980\n",
      "weighted avg       0.69      0.69      0.69       980\n",
      "\n",
      "Number of FP 162\n",
      "Number of TP 297\n",
      "Number of TP 297\n",
      "739\n",
      "Confusion Matrix para classe 6:\n",
      "[[378 162]\n",
      " [143 297]]\n",
      "{'TN': 378, 'FP': 162, 'FN': 143, 'TP': 297}\n",
      "For the White Wine rfc para Qualidade-> 7 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.96      0.93       804\n",
      "           1       0.73      0.47      0.57       176\n",
      "\n",
      "    accuracy                           0.87       980\n",
      "   macro avg       0.81      0.71      0.75       980\n",
      "weighted avg       0.86      0.87      0.86       980\n",
      "\n",
      "Number of FP 31\n",
      "Number of TP 82\n",
      "Number of TP 82\n",
      "852\n",
      "Confusion Matrix para classe 7:\n",
      "[[773  31]\n",
      " [ 94  82]]\n",
      "{'TN': 773, 'FP': 31, 'FN': 94, 'TP': 82}\n",
      "For the White Wine rfc para Qualidade-> 8 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      1.00      0.99       945\n",
      "           1       0.83      0.29      0.43        35\n",
      "\n",
      "    accuracy                           0.97       980\n",
      "   macro avg       0.90      0.64      0.71       980\n",
      "weighted avg       0.97      0.97      0.97       980\n",
      "\n",
      "Number of FP 2\n",
      "Number of TP 10\n",
      "Number of TP 10\n",
      "864\n",
      "Confusion Matrix para classe 8:\n",
      "[[943   2]\n",
      " [ 25  10]]\n",
      "{'TN': 943, 'FP': 2, 'FN': 25, 'TP': 10}\n",
      "For the White Wine rfc para Qualidade-> 9 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       979\n",
      "           1       1.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           1.00       980\n",
      "   macro avg       1.00      0.50      0.50       980\n",
      "weighted avg       1.00      1.00      1.00       980\n",
      "\n",
      "Number of FP 0\n",
      "Number of TP 0\n",
      "Number of TP 0\n",
      "864\n",
      "Confusion Matrix para classe 9:\n",
      "[[979   0]\n",
      " [  1   0]]\n",
      "{'TN': 979, 'FP': 0, 'FN': 1, 'TP': 0}\n",
      "Para a analise total temos as medias:\n",
      "Para macro avg ->  {'precision macro avg': 0.5205079819538435, 'recall macro avg': 0.32157903461290793, 'f1-score macro avg': 0.3729602058058938}\n",
      "Para weighted avg ->  {'precision weighted avg': 0.6800019012029591, 'recall weighted avg': 0.5938775510204082, 'f1-score weighted avg': 0.6208790270715053}\n",
      "Numero de amostrar que deram true em algum momento:  864\n",
      "Pela formula:  0\n",
      "Numero de Amostras que sao classificadas mais de uma vez:  864\n",
      "Amostras repetidas pelo menos uma vez 48\n",
      "Amostras NÃO CLassificadas 164\n",
      "Numero real de amostras no dataset teste:  980\n",
      "VALORES GERAIS -> FP:  282 FN:  398 TN  5598 TP 582\n",
      "Overall-> Precision: 0.6736111111111112 | Recall: 0.5938775510204082|F1-Score: 0.631236442516269\n",
      "Overall Accuracy:  0.9008746355685131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vinic\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\vinic\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\vinic\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "## white wine\n",
    "\n",
    "class_rep = {}\n",
    "number_samples_final = 0\n",
    "predicted_as_1_in_any_model = np.zeros_like(data_white_wine_one_vs_all['6']['white_wine_y_test'], dtype=int)\n",
    "fpp = 0\n",
    "fnn = 0\n",
    "tnn = 0\n",
    "tpp = 0\n",
    "\n",
    "for i in np.sort(data_white_wine['quality'].unique()):\n",
    "    white_wine_rfc_model = RandomForestClassifier()\n",
    "    white_wine_rfc_model.fit(data_white_wine_one_vs_all[f'{i}']['white_wine_X_train'],data_white_wine_one_vs_all[f'{i}']['white_wine_y_train'])\n",
    "    #use the rfc model to try to pwhiteict the test dataset\n",
    "    white_wine_y_pred = white_wine_rfc_model.predict(data_white_wine_one_vs_all[f'{i}']['white_wine_X_test'])\n",
    "    ## for better understanding of precition and recall\n",
    "    ## https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "    print('For the White Wine rfc para Qualidade->',i,'\\n',classification_report(data_white_wine_one_vs_all[f'{i}']['white_wine_y_test'],white_wine_y_pred,zero_division=True))\n",
    "    class_rep[f'{i}'] = classification_report(data_white_wine_one_vs_all[f'{i}']['white_wine_y_test'],white_wine_y_pred,output_dict=True)\n",
    "    \n",
    "    number_samples_final += np.sum(white_wine_y_pred == 1)\n",
    "    #print(white_wine_y_pred)\n",
    "    predicted_as_1_in_any_model = predicted_as_1_in_any_model + white_wine_y_pred\n",
    "    print(\"Number of FP\",np.sum((~data_white_wine_one_vs_all[f'{i}']['white_wine_y_test'] & white_wine_y_pred)==1))\n",
    "    print(\"Number of TP\",np.sum((data_white_wine_one_vs_all[f'{i}']['white_wine_y_test']+white_wine_y_pred)>1))\n",
    "    print(\"Number of TP\",np.sum((data_white_wine_one_vs_all[f'{i}']['white_wine_y_test']&white_wine_y_pred)==1))\n",
    "    #print(predicted_as_1_in_any_model)\n",
    "    print(number_samples_final)\n",
    "    cm = confusion_matrix(data_white_wine_one_vs_all[f'{i}']['white_wine_y_test'], white_wine_y_pred)\n",
    "    print(f\"Confusion Matrix para classe {i}:\\n{cm}\")\n",
    "    tn, fp, fn, tp = confusion_matrix(data_white_wine_one_vs_all[f'{i}']['white_wine_y_test'], white_wine_y_pred).ravel()\n",
    "    print({'TN': tn, 'FP': fp, 'FN': fn, 'TP': tp})\n",
    "    fpp = fpp + fp\n",
    "    fnn = fn + fnn\n",
    "    tnn = tn + tnn\n",
    "    tpp = tpp + tp\n",
    "\n",
    "## tabela juntando os valores previstos como verdadeiros\n",
    "macro_avgs = {\"precision macro avg\":0,\"recall macro avg\":0,\"f1-score macro avg\":0}\n",
    "weighted_avg = {\"precision weighted avg\":0,\"recall weighted avg\":0,\"f1-score weighted avg\":0}\n",
    "number_samples= 0\n",
    "for i in np.sort(data_white_wine['quality'].unique()):\n",
    "    macro_avgs[\"precision macro avg\"] = macro_avgs[\"precision macro avg\"] + class_rep[f'{i}'][\"1\"][\"precision\"]\n",
    "    weighted_avg[\"precision weighted avg\"] = weighted_avg[\"precision weighted avg\"] + class_rep[f'{i}'][\"1\"][\"precision\"]*class_rep[f'{i}'][\"1\"][\"support\"]\n",
    "    macro_avgs[\"recall macro avg\"] = macro_avgs[\"recall macro avg\"] + class_rep[f'{i}'][\"1\"][\"recall\"]\n",
    "    weighted_avg[\"recall weighted avg\"] = weighted_avg[\"recall weighted avg\"] + class_rep[f'{i}'][\"1\"][\"recall\"]*class_rep[f'{i}'][\"1\"][\"support\"]    \n",
    "    macro_avgs[\"f1-score macro avg\"] = macro_avgs[\"f1-score macro avg\"] + class_rep[f'{i}'][\"1\"][\"f1-score\"]\n",
    "    weighted_avg[\"f1-score weighted avg\"] = weighted_avg[\"f1-score weighted avg\"] + class_rep[f'{i}'][\"1\"][\"f1-score\"]*class_rep[f'{i}'][\"1\"][\"support\"]\n",
    "    number_samples = number_samples + class_rep[f'{i}'][\"1\"][\"support\"]\n",
    "\n",
    "for i in macro_avgs:\n",
    "    macro_avgs[i] = macro_avgs[i]/len(np.sort(data_white_wine['quality'].unique()))\n",
    "\n",
    "for i in weighted_avg:\n",
    "    weighted_avg[i] = weighted_avg[i]/number_samples\n",
    "\n",
    "print(\"Para a analise total temos as medias:\")\n",
    "print(\"Para macro avg -> \",macro_avgs)\n",
    "print(\"Para weighted avg -> \",weighted_avg)\n",
    "print(\"Numero de amostrar que deram true em algum momento: \",number_samples_final)\n",
    "print(\"Pela formula: \",calculo_numero_amostras)\n",
    "print(\"Numero de Amostras que sao classificadas mais de uma vez: \",tpp+fpp)\n",
    "print(\"Amostras repetidas pelo menos uma vez\",np.sum(predicted_as_1_in_any_model>1))\n",
    "print(\"Amostras NÃO CLassificadas\",np.sum(predicted_as_1_in_any_model==0))\n",
    "\n",
    "print(\"Numero real de amostras no dataset teste: \",len(data_white_wine_one_vs_all['6']['white_wine_y_test']))\n",
    "print('VALORES GERAIS -> FP: ',fpp,'FN: ',fnn,'TN ',tnn,\"TP\",tpp)\n",
    "\n",
    "print(f\"Overall-> Precision: {tpp/(tpp+fpp)} | Recall: {tpp/(tpp+fnn)}|F1-Score: {2/((1/(tpp/(tpp+fpp))) + (1/(tpp/(tpp+fnn))))}\")\n",
    "print(\"Overall Accuracy: \",(tpp+tnn)/(tpp+tnn+fpp+fnn))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red Whine NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_red(X_train, y_train, num_nodes, dropout_prob, lr, batch_size, epochs,X_valid,y_valid):\n",
    "    col = X_train.shape[1]\n",
    "    num_classes = len(np.unique(y_train))\n",
    "    nn_model = tf.keras.Sequential(\n",
    "        [Input(shape=(col,)),\n",
    "         tf.keras.layers.Dense(num_nodes,activation='relu'),\n",
    "         tf.keras.layers.Dropout(dropout_prob),\n",
    "         tf.keras.layers.Dense(num_nodes,activation='relu'),\n",
    "         tf.keras.layers.Dropout(dropout_prob),\n",
    "         tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    nn_model.compile(optimizer=tf.keras.optimizers.Adam(lr),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    history = nn_model.fit(\n",
    "        X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_valid,y_valid),verbose=0\n",
    "        )\n",
    "    return nn_model, history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import numpy as np\n",
    "#import tensorflow as tf\n",
    "#from itertools import product\n",
    "#from sklearn.metrics import classification_report, f1_score\n",
    "#\n",
    "#def find_best_param(X_train, y_train,X_valid,y_valid):\n",
    "#    # Parâmetros para busca\n",
    "#    num_nodes_list = [16, 32, 34]\n",
    "#    dropout_probs = [0, 0.2]\n",
    "#    learning_rates = [0.01, 0.005, 0.001]\n",
    "#    batch_sizes = [32, 64, 128]\n",
    "#    epochs = 100\n",
    "#\n",
    "#    # Melhor modelo baseado em validação\n",
    "#    best_model = None\n",
    "#    best_model_params = None\n",
    "#    best_f1 = 0\n",
    "#    best_accuracy = 0\n",
    "#    best_val_loss = float('inf')\n",
    "#\n",
    "#    # Lista para armazenar resultados\n",
    "#    results = []\n",
    "#\n",
    "#    # Loop pelos hiperparâmetros\n",
    "#    for num_nodes, dropout_prob, lr, batch_size in product(num_nodes_list, dropout_probs, learning_rates, batch_sizes):\n",
    "#        print(f\"Testando: nodes={num_nodes}, dropout={dropout_prob}, lr={lr}, batch={batch_size}\")\n",
    "#\n",
    "#        # Treina o modelo com os parâmetros atuais\n",
    "#        model, history = train_model_red(\n",
    "#            X_train, y_train, num_nodes, dropout_prob, lr, batch_size, epochs,X_valid,y_valid\n",
    "#        )\n",
    "#\n",
    "#        # Obtém a última perda, acurácia e F1-score no conjunto de validação\n",
    "#        val_loss = history.history['val_loss'][-1]\n",
    "#        val_accuracy = history.history['val_accuracy'][-1]\n",
    "#\n",
    "#\n",
    "#        y_val_pred_probs = model.predict(X_valid)\n",
    "#        y_val_pred = (y_val_pred_probs > 0.5).astype(int)  # Conversão para rótulo binário\n",
    "#        val_f1 = f1_score(y_valid, y_val_pred, average='weighted')\n",
    "#        \n",
    "#        # Armazena os resultados\n",
    "#        results.append({\n",
    "#            \"num_nodes\": num_nodes, \"dropout_prob\": dropout_prob, \"lr\": lr, \"batch_size\": batch_size,\n",
    "#            \"val_loss\": val_loss, \"val_accuracy\": val_accuracy, \"val_f1\": val_f1\n",
    "#        })\n",
    "#\n",
    "#        # Atualiza o melhor modelo com base nas métricas\n",
    "#        if (val_f1 > best_f1) or (val_f1 == best_f1 and val_accuracy > best_accuracy) or (val_f1 == best_f1 and val_loss < best_val_loss):\n",
    "#            best_f1 = val_f1\n",
    "#            best_accuracy = val_accuracy\n",
    "#            best_val_loss = val_loss\n",
    "#            best_model = model\n",
    "#            best_model_params = {\n",
    "#                \"num_nodes\": num_nodes, \"dropout_prob\": dropout_prob, \"lr\": lr, \"batch_size\": batch_size\n",
    "#            }\n",
    "#    return best_model_params\n",
    "\n",
    "### AJUDA DO GPT\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from itertools import product\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def find_best_param(X_train, y_train, X_valid, y_valid):\n",
    "    # Seed para reprodutibilidade\n",
    "    tf.random.set_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Parâmetros para busca\n",
    "    num_nodes_list = [16, 32, 34]\n",
    "    dropout_probs = [0, 0.2]\n",
    "    learning_rates = [0.01, 0.005, 0.001]\n",
    "    batch_sizes = [32, 64, 128]\n",
    "    epochs = 100\n",
    "\n",
    "    # Inicialização do melhor modelo\n",
    "    best_model_params = None\n",
    "    best_f1 = 0\n",
    "    best_accuracy = 0\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    # Loop pelos hiperparâmetros\n",
    "    for num_nodes, dropout_prob, lr, batch_size in product(num_nodes_list, dropout_probs, learning_rates, batch_sizes):\n",
    "        print(f\"\\nTestando: nodes={num_nodes}, dropout={dropout_prob}, lr={lr}, batch={batch_size}\")\n",
    "\n",
    "        # Treina o modelo com os parâmetros atuais\n",
    "        model, history = train_model_red(\n",
    "            X_train, y_train, num_nodes, dropout_prob, lr, batch_size, epochs, X_valid, y_valid\n",
    "        )\n",
    "\n",
    "        # Avaliação no conjunto de validação\n",
    "        val_loss = history.history['val_loss'][-1]\n",
    "        val_accuracy = history.history['val_accuracy'][-1]\n",
    "\n",
    "        y_val_pred_probs = model.predict(X_valid)\n",
    "        y_val_pred = (y_val_pred_probs > 0.5).astype(int)\n",
    "        val_f1 = f1_score(y_valid, y_val_pred, average='weighted')\n",
    "\n",
    "        # Mostra métricas da iteração\n",
    "        print(f\"F1: {val_f1:.4f}, Acc: {val_accuracy:.4f}, Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Atualiza o melhor conjunto de parâmetros\n",
    "        if (val_f1 > best_f1) or (val_f1 == best_f1 and val_accuracy > best_accuracy) or (val_f1 == best_f1 and val_loss < best_val_loss):\n",
    "            best_f1 = val_f1\n",
    "            best_accuracy = val_accuracy\n",
    "            best_val_loss = val_loss\n",
    "            best_model_params = {\n",
    "                \"num_nodes\": num_nodes,\n",
    "                \"dropout_prob\": dropout_prob,\n",
    "                \"lr\": lr,\n",
    "                \"batch_size\": batch_size\n",
    "            }\n",
    "\n",
    "    return best_model_params\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9897, Acc: 0.9906, Loss: 0.2508\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9897, Acc: 0.9906, Loss: 0.3641\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9897, Acc: 0.9906, Loss: 0.2114\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9875, Acc: 0.9875, Loss: 0.1968\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9897, Acc: 0.9906, Loss: 0.3412\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9875, Acc: 0.9875, Loss: 0.2438\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9897, Acc: 0.9906, Loss: 0.2105\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9897, Acc: 0.9906, Loss: 0.1785\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9855, Acc: 0.9844, Loss: 0.1432\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9897, Acc: 0.9906, Loss: 1.1208\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9844, Acc: 0.9875, Loss: 0.8846\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9897, Acc: 0.9906, Loss: 0.7422\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9875, Acc: 0.9875, Loss: 0.3561\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9844, Acc: 0.9875, Loss: 0.5261\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9844, Acc: 0.9875, Loss: 0.3993\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9897, Acc: 0.9906, Loss: 0.2993\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9875, Acc: 0.9875, Loss: 0.3418\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step  \n",
      "F1: 0.9875, Acc: 0.9875, Loss: 0.1551\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9897, Acc: 0.9906, Loss: 0.5015\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9897, Acc: 0.9906, Loss: 0.3016\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9897, Acc: 0.9906, Loss: 0.2728\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9897, Acc: 0.9906, Loss: 0.3082\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9897, Acc: 0.9906, Loss: 0.2048\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9844, Acc: 0.9875, Loss: 0.3180\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9897, Acc: 0.9906, Loss: 0.2666\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "F1: 0.9897, Acc: 0.9906, Loss: 0.2285\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9875, Acc: 0.9875, Loss: 0.1268\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9812, Acc: 0.9812, Loss: 0.8506\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9844, Acc: 0.9875, Loss: 0.8755\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9897, Acc: 0.9906, Loss: 0.7236\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9897, Acc: 0.9906, Loss: 0.5901\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9897, Acc: 0.9906, Loss: 0.4122\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9844, Acc: 0.9875, Loss: 0.2932\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9844, Acc: 0.9875, Loss: 0.2858\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9897, Acc: 0.9906, Loss: 0.2165\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9897, Acc: 0.9906, Loss: 0.1644\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9828, Acc: 0.9844, Loss: 0.3477\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9875, Acc: 0.9875, Loss: 0.3284\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9897, Acc: 0.9906, Loss: 0.2449\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9897, Acc: 0.9906, Loss: 0.3782\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9897, Acc: 0.9906, Loss: 0.2949\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9897, Acc: 0.9906, Loss: 0.2525\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9897, Acc: 0.9906, Loss: 0.2065\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9897, Acc: 0.9906, Loss: 0.1453\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9875, Acc: 0.9875, Loss: 0.1486\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9844, Acc: 0.9875, Loss: 0.7672\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step  \n",
      "F1: 0.9844, Acc: 0.9875, Loss: 0.7394\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9897, Acc: 0.9906, Loss: 0.4913\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9844, Acc: 0.9875, Loss: 0.7052\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9844, Acc: 0.9875, Loss: 0.7015\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9844, Acc: 0.9875, Loss: 0.3505\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9897, Acc: 0.9906, Loss: 0.3193\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9897, Acc: 0.9906, Loss: 0.2408\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9812, Acc: 0.9812, Loss: 0.1632\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9409, Acc: 0.9500, Loss: 2.3977\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9425, Acc: 0.9531, Loss: 1.6335\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9361, Acc: 0.9406, Loss: 2.2715\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9409, Acc: 0.9500, Loss: 3.1354\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9377, Acc: 0.9438, Loss: 2.1242\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9393, Acc: 0.9469, Loss: 1.3856\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9329, Acc: 0.9344, Loss: 0.9746\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9296, Acc: 0.9281, Loss: 0.8623\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9219, Acc: 0.9094, Loss: 0.4828\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9377, Acc: 0.9438, Loss: 5.1416\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9309, Acc: 0.9219, Loss: 5.4173\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9296, Acc: 0.9281, Loss: 4.4468\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9357, Acc: 0.9344, Loss: 4.2552\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9375, Acc: 0.9375, Loss: 2.9757\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9214, Acc: 0.9125, Loss: 2.1856\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9214, Acc: 0.9125, Loss: 1.1708\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9087, Acc: 0.8813, Loss: 0.8467\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.8977, Acc: 0.8625, Loss: 0.6683\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9425, Acc: 0.9531, Loss: 2.3394\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9425, Acc: 0.9531, Loss: 1.9028\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9377, Acc: 0.9438, Loss: 1.3573\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9393, Acc: 0.9469, Loss: 1.8419\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9428, Acc: 0.9469, Loss: 1.2412\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9377, Acc: 0.9438, Loss: 1.1488\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9377, Acc: 0.9438, Loss: 0.9806\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9393, Acc: 0.9469, Loss: 0.9134\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9280, Acc: 0.9250, Loss: 0.6381\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9428, Acc: 0.9469, Loss: 3.2311\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9345, Acc: 0.9375, Loss: 4.8313\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9425, Acc: 0.9531, Loss: 4.3465\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9345, Acc: 0.9375, Loss: 5.8638\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9409, Acc: 0.9500, Loss: 3.8090\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9313, Acc: 0.9312, Loss: 3.5850\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9393, Acc: 0.9406, Loss: 1.5134\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9253, Acc: 0.9156, Loss: 1.1100\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9231, Acc: 0.9156, Loss: 0.8259\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9361, Acc: 0.9406, Loss: 2.5026\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9425, Acc: 0.9531, Loss: 1.9996\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9440, Acc: 0.9563, Loss: 1.7499\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9393, Acc: 0.9469, Loss: 2.2567\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9393, Acc: 0.9469, Loss: 1.7843\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9425, Acc: 0.9531, Loss: 1.1102\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9361, Acc: 0.9406, Loss: 1.2383\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9393, Acc: 0.9469, Loss: 0.8669\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9375, Acc: 0.9375, Loss: 0.5407\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "F1: 0.9288, Acc: 0.9219, Loss: 4.8529\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9329, Acc: 0.9344, Loss: 6.0471\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9329, Acc: 0.9344, Loss: 4.3614\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9361, Acc: 0.9406, Loss: 4.5106\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9425, Acc: 0.9531, Loss: 3.1747\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9393, Acc: 0.9406, Loss: 2.6604\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9313, Acc: 0.9312, Loss: 1.5689\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9219, Acc: 0.9094, Loss: 1.2609\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9183, Acc: 0.9000, Loss: 0.7663\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.6954, Acc: 0.6938, Loss: 1.3358\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.7300, Acc: 0.7281, Loss: 0.8270\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7299, Acc: 0.7281, Loss: 0.8202\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7258, Acc: 0.7250, Loss: 0.8244\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.7393, Acc: 0.7375, Loss: 0.6384\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7484, Acc: 0.7469, Loss: 0.6297\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7054, Acc: 0.7031, Loss: 0.5657\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7395, Acc: 0.7375, Loss: 0.5756\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7333, Acc: 0.7312, Loss: 0.5551\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7264, Acc: 0.7250, Loss: 0.5436\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7570, Acc: 0.7563, Loss: 0.5257\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.6944, Acc: 0.6938, Loss: 0.5877\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7455, Acc: 0.7437, Loss: 0.5398\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.7364, Acc: 0.7344, Loss: 0.5552\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7299, Acc: 0.7281, Loss: 0.5541\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7457, Acc: 0.7437, Loss: 0.5527\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.7426, Acc: 0.7406, Loss: 0.5560\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.7394, Acc: 0.7375, Loss: 0.5631\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.7735, Acc: 0.7719, Loss: 1.6610\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.7488, Acc: 0.7469, Loss: 1.4615\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.7387, Acc: 0.7375, Loss: 1.5438\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7698, Acc: 0.7688, Loss: 1.3390\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7628, Acc: 0.7625, Loss: 1.0975\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7511, Acc: 0.7500, Loss: 0.7548\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7456, Acc: 0.7437, Loss: 0.5818\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7488, Acc: 0.7469, Loss: 0.5880\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.7424, Acc: 0.7406, Loss: 0.5694\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.7516, Acc: 0.7500, Loss: 0.6460\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.7798, Acc: 0.7781, Loss: 0.5503\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.7642, Acc: 0.7625, Loss: 0.6029\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.7736, Acc: 0.7719, Loss: 0.5566\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.7426, Acc: 0.7406, Loss: 0.5521\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.7488, Acc: 0.7469, Loss: 0.5706\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.7488, Acc: 0.7469, Loss: 0.5246\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.7333, Acc: 0.7312, Loss: 0.5479\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.7395, Acc: 0.7375, Loss: 0.5640\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.7545, Acc: 0.7531, Loss: 1.9059\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.7765, Acc: 0.7750, Loss: 1.7399\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.7828, Acc: 0.7812, Loss: 1.2188\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7451, Acc: 0.7437, Loss: 1.3543\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.7632, Acc: 0.7625, Loss: 1.0409\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.7543, Acc: 0.7531, Loss: 0.7994\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.7765, Acc: 0.7750, Loss: 0.5374\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.7612, Acc: 0.7594, Loss: 0.5505\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.7395, Acc: 0.7375, Loss: 0.5824\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.7578, Acc: 0.7563, Loss: 0.6103\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.7673, Acc: 0.7656, Loss: 0.6926\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.7736, Acc: 0.7719, Loss: 0.5633\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7519, Acc: 0.7500, Loss: 0.6027\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7519, Acc: 0.7500, Loss: 0.5517\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "F1: 0.7519, Acc: 0.7500, Loss: 0.5443\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7488, Acc: 0.7469, Loss: 0.5445\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7580, Acc: 0.7563, Loss: 0.5459\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7302, Acc: 0.7281, Loss: 0.5431\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.6466, Acc: 0.6469, Loss: 1.1886\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.6656, Acc: 0.6625, Loss: 1.1271\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.6557, Acc: 0.6531, Loss: 0.9081\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.5823, Acc: 0.5781, Loss: 0.9617\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.6653, Acc: 0.6625, Loss: 0.8727\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.6479, Acc: 0.6469, Loss: 0.8198\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.6410, Acc: 0.6375, Loss: 0.6819\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.6257, Acc: 0.6219, Loss: 0.6590\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.6350, Acc: 0.6313, Loss: 0.6650\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.6391, Acc: 0.6375, Loss: 0.6226\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.6368, Acc: 0.6344, Loss: 0.6358\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.6163, Acc: 0.6125, Loss: 0.6570\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.6437, Acc: 0.6406, Loss: 0.6752\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.6329, Acc: 0.6313, Loss: 0.6423\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.6068, Acc: 0.6031, Loss: 0.6376\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.5759, Acc: 0.5719, Loss: 0.6441\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.5974, Acc: 0.5938, Loss: 0.6440\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.5798, Acc: 0.5781, Loss: 0.6532\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.6862, Acc: 0.6875, Loss: 2.2638\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.6880, Acc: 0.6906, Loss: 1.5497\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.6743, Acc: 0.6719, Loss: 1.6130\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.6803, Acc: 0.6781, Loss: 1.5221\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.6721, Acc: 0.6687, Loss: 1.3102\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.6234, Acc: 0.6250, Loss: 1.2090\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.6315, Acc: 0.6281, Loss: 0.6793\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.6758, Acc: 0.6750, Loss: 0.6329\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.6433, Acc: 0.6406, Loss: 0.6672\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.6571, Acc: 0.6594, Loss: 0.6953\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.6721, Acc: 0.6719, Loss: 0.6882\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.6382, Acc: 0.6406, Loss: 0.6837\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.6912, Acc: 0.6906, Loss: 0.6763\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.6341, Acc: 0.6344, Loss: 0.6403\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.6699, Acc: 0.6687, Loss: 0.6528\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.6442, Acc: 0.6406, Loss: 0.6390\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.6251, Acc: 0.6219, Loss: 0.6338\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.6195, Acc: 0.6156, Loss: 0.6453\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.6761, Acc: 0.6750, Loss: 1.9743\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.7125, Acc: 0.7125, Loss: 1.5936\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.6731, Acc: 0.6750, Loss: 1.4513\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.6411, Acc: 0.6375, Loss: 1.6456\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.6145, Acc: 0.6125, Loss: 1.4509\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.6358, Acc: 0.6344, Loss: 1.1611\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.6251, Acc: 0.6219, Loss: 0.7143\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.6457, Acc: 0.6438, Loss: 0.6964\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.6345, Acc: 0.6313, Loss: 0.6579\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.6587, Acc: 0.6562, Loss: 0.6971\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.6613, Acc: 0.6750, Loss: 0.7100\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.6658, Acc: 0.6656, Loss: 0.6605\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.6754, Acc: 0.6781, Loss: 0.6895\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.6697, Acc: 0.6719, Loss: 0.6556\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.6735, Acc: 0.6719, Loss: 0.6487\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.6424, Acc: 0.6406, Loss: 0.6235\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.6593, Acc: 0.6562, Loss: 0.6249\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.6504, Acc: 0.6469, Loss: 0.6262\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.8610, Acc: 0.8687, Loss: 2.0635\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.8798, Acc: 0.8844, Loss: 1.0513\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.8577, Acc: 0.8562, Loss: 1.3455\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.8651, Acc: 0.8625, Loss: 1.1516\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.8712, Acc: 0.8719, Loss: 0.9264\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.8713, Acc: 0.8687, Loss: 0.6882\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.8656, Acc: 0.8562, Loss: 0.3941\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.8372, Acc: 0.8219, Loss: 0.4205\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.8337, Acc: 0.8094, Loss: 0.3968\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.8494, Acc: 0.8406, Loss: 0.5216\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.8486, Acc: 0.8313, Loss: 0.5843\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.8527, Acc: 0.8375, Loss: 0.5736\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.8476, Acc: 0.8281, Loss: 0.5617\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.8421, Acc: 0.8219, Loss: 0.5822\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.8269, Acc: 0.8031, Loss: 0.4713\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.8252, Acc: 0.8000, Loss: 0.4016\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.8010, Acc: 0.7688, Loss: 0.4144\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.8074, Acc: 0.7750, Loss: 0.4205\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.8628, Acc: 0.8687, Loss: 1.6298\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.8635, Acc: 0.8656, Loss: 1.9733\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.8397, Acc: 0.8344, Loss: 1.2660\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.8635, Acc: 0.8656, Loss: 1.1506\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.8539, Acc: 0.8531, Loss: 1.1620\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.8651, Acc: 0.8625, Loss: 1.2052\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.8553, Acc: 0.8531, Loss: 0.6805\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.8700, Acc: 0.8687, Loss: 0.5020\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.8525, Acc: 0.8406, Loss: 0.4383\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9010, Acc: 0.9000, Loss: 0.6388\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.8724, Acc: 0.8687, Loss: 0.7535\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.8641, Acc: 0.8531, Loss: 0.7115\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.8770, Acc: 0.8719, Loss: 0.6285\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.8674, Acc: 0.8562, Loss: 0.5792\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.8696, Acc: 0.8625, Loss: 0.4446\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.8543, Acc: 0.8406, Loss: 0.3443\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.8543, Acc: 0.8406, Loss: 0.3709\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.8274, Acc: 0.8000, Loss: 0.3945\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.8649, Acc: 0.8656, Loss: 1.6372\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.8762, Acc: 0.8781, Loss: 1.2431\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.8468, Acc: 0.8500, Loss: 1.4153\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.8798, Acc: 0.8844, Loss: 1.1196\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.8800, Acc: 0.8813, Loss: 1.0170\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.8698, Acc: 0.8719, Loss: 0.9746\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.8800, Acc: 0.8813, Loss: 0.5455\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.8749, Acc: 0.8719, Loss: 0.5187\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.8443, Acc: 0.8313, Loss: 0.3906\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.8775, Acc: 0.8781, Loss: 0.5080\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.8770, Acc: 0.8719, Loss: 0.7356\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.8810, Acc: 0.8781, Loss: 0.4484\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9070, Acc: 0.9031, Loss: 0.4970\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.8855, Acc: 0.8813, Loss: 0.5017\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.8681, Acc: 0.8594, Loss: 0.5105\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.8510, Acc: 0.8375, Loss: 0.3897\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.8390, Acc: 0.8188, Loss: 0.3781\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.8269, Acc: 0.8031, Loss: 0.3870\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9812, Acc: 0.9812, Loss: 0.3531\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9775, Acc: 0.9750, Loss: 0.2543\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9739, Acc: 0.9688, Loss: 0.2857\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9833, Acc: 0.9844, Loss: 0.2835\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9811, Acc: 0.9781, Loss: 0.2592\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9854, Acc: 0.9875, Loss: 0.2444\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9852, Acc: 0.9844, Loss: 0.2034\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9739, Acc: 0.9688, Loss: 0.1566\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9772, Acc: 0.9719, Loss: 0.1130\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9753, Acc: 0.9688, Loss: 0.8587\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9775, Acc: 0.9750, Loss: 0.4975\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9854, Acc: 0.9875, Loss: 0.6535\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9756, Acc: 0.9719, Loss: 0.8426\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9766, Acc: 0.9781, Loss: 0.4374\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9833, Acc: 0.9844, Loss: 0.3770\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9793, Acc: 0.9781, Loss: 0.3022\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9704, Acc: 0.9625, Loss: 0.1877\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9721, Acc: 0.9656, Loss: 0.1740\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9852, Acc: 0.9844, Loss: 0.3484\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9852, Acc: 0.9844, Loss: 0.2644\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9812, Acc: 0.9812, Loss: 0.4185\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9833, Acc: 0.9844, Loss: 0.2913\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9833, Acc: 0.9844, Loss: 0.4263\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9833, Acc: 0.9844, Loss: 0.2920\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9831, Acc: 0.9812, Loss: 0.2272\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9854, Acc: 0.9875, Loss: 0.2163\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9812, Acc: 0.9812, Loss: 0.1365\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9793, Acc: 0.9781, Loss: 0.5576\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9812, Acc: 0.9812, Loss: 1.1193\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9812, Acc: 0.9812, Loss: 0.3987\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9854, Acc: 0.9875, Loss: 0.5987\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9854, Acc: 0.9875, Loss: 0.5892\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9854, Acc: 0.9875, Loss: 0.5130\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9833, Acc: 0.9844, Loss: 0.3737\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9854, Acc: 0.9875, Loss: 0.2818\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9721, Acc: 0.9656, Loss: 0.2273\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9878, Acc: 0.9906, Loss: 0.3411\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9833, Acc: 0.9844, Loss: 0.4493\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9833, Acc: 0.9844, Loss: 0.4626\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9812, Acc: 0.9812, Loss: 0.4466\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9812, Acc: 0.9812, Loss: 0.3270\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9833, Acc: 0.9844, Loss: 0.4255\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9793, Acc: 0.9781, Loss: 0.2454\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9878, Acc: 0.9906, Loss: 0.1869\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9812, Acc: 0.9812, Loss: 0.1058\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9854, Acc: 0.9875, Loss: 0.8876\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9756, Acc: 0.9719, Loss: 0.8717\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9833, Acc: 0.9844, Loss: 0.5723\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9854, Acc: 0.9875, Loss: 0.5177\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9878, Acc: 0.9906, Loss: 0.4983\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9878, Acc: 0.9906, Loss: 0.5263\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9833, Acc: 0.9844, Loss: 0.4173\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.9878, Acc: 0.9906, Loss: 0.2830\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.9739, Acc: 0.9688, Loss: 0.1527\n"
     ]
    }
   ],
   "source": [
    "best_param_red_nn = {}\n",
    "# Exibir os melhores parâmetros encontrados\n",
    "for i in np.sort(data_red_wine['quality'].unique()):\n",
    "    best_param_red_nn[f'{i}'] = find_best_param(data_red_wine_one_vs_all[f'{i}']['red_wine_X_train'], data_red_wine_one_vs_all[f'{i}']['red_wine_y_train'],data_red_wine_one_vs_all[f'{i}']['red_wine_X_valid'],data_red_wine_one_vs_all[f'{i}']['red_wine_y_valid'])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'3': {'num_nodes': 34, 'dropout_prob': 0, 'lr': 0.001, 'batch_size': 64}, '4': {'num_nodes': 34, 'dropout_prob': 0, 'lr': 0.01, 'batch_size': 128}, '5': {'num_nodes': 34, 'dropout_prob': 0, 'lr': 0.01, 'batch_size': 128}, '6': {'num_nodes': 34, 'dropout_prob': 0, 'lr': 0.01, 'batch_size': 64}, '7': {'num_nodes': 34, 'dropout_prob': 0.2, 'lr': 0.005, 'batch_size': 32}, '8': {'num_nodes': 34, 'dropout_prob': 0, 'lr': 0.001, 'batch_size': 64}}\n"
     ]
    }
   ],
   "source": [
    "print(best_param_red_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# como ja achei os melhores parametros, vou so jogar aq para agilizar\n",
    "\n",
    "best_param_red_nn = {'3': {'num_nodes': 34, 'dropout_prob': 0, 'lr': 0.001, 'batch_size': 64}, '4': {'num_nodes': 34, 'dropout_prob': 0, 'lr': 0.01, 'batch_size': 128}, '5': {'num_nodes': 34, 'dropout_prob': 0, 'lr': 0.01, 'batch_size': 128}, '6': {'num_nodes': 34, 'dropout_prob': 0, 'lr': 0.01, 'batch_size': 64}, '7': {'num_nodes': 34, 'dropout_prob': 0.2, 'lr': 0.005, 'batch_size': 32}, '8': {'num_nodes': 34, 'dropout_prob': 0, 'lr': 0.001, 'batch_size': 64}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for the 3 -> {'num_nodes': 34, 'dropout_prob': 0, 'lr': 0.001, 'batch_size': 64}\n",
      "For the Red Wine NN para Qualidade-> 3 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      1.00      0.99       317\n",
      "           1       0.00      0.00      0.00         3\n",
      "\n",
      "    accuracy                           0.99       320\n",
      "   macro avg       0.50      0.50      0.50       320\n",
      "weighted avg       0.98      0.99      0.98       320\n",
      "\n",
      "Number of FP 1\n",
      "Number of TP 0\n",
      "Number of TP 0\n",
      "1\n",
      "Confusion Matrix para classe 3:\n",
      "[[316   1]\n",
      " [  3   0]]\n",
      "{'TN': 316, 'FP': 1, 'FN': 3, 'TP': 0}\n",
      "Best Parameters for the 4 -> {'num_nodes': 34, 'dropout_prob': 0, 'lr': 0.01, 'batch_size': 128}\n",
      "For the Red Wine NN para Qualidade-> 4 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.99      0.98       310\n",
      "           1       0.33      0.20      0.25        10\n",
      "\n",
      "    accuracy                           0.96       320\n",
      "   macro avg       0.65      0.59      0.62       320\n",
      "weighted avg       0.95      0.96      0.96       320\n",
      "\n",
      "Number of FP 4\n",
      "Number of TP 2\n",
      "Number of TP 2\n",
      "7\n",
      "Confusion Matrix para classe 4:\n",
      "[[306   4]\n",
      " [  8   2]]\n",
      "{'TN': 306, 'FP': 4, 'FN': 8, 'TP': 2}\n",
      "Best Parameters for the 5 -> {'num_nodes': 34, 'dropout_prob': 0, 'lr': 0.01, 'batch_size': 128}\n",
      "For the Red Wine NN para Qualidade-> 5 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.75      0.76       193\n",
      "           1       0.63      0.67      0.65       127\n",
      "\n",
      "    accuracy                           0.72       320\n",
      "   macro avg       0.70      0.71      0.71       320\n",
      "weighted avg       0.72      0.72      0.72       320\n",
      "\n",
      "Number of FP 49\n",
      "Number of TP 85\n",
      "Number of TP 85\n",
      "141\n",
      "Confusion Matrix para classe 5:\n",
      "[[144  49]\n",
      " [ 42  85]]\n",
      "{'TN': 144, 'FP': 49, 'FN': 42, 'TP': 85}\n",
      "Best Parameters for the 6 -> {'num_nodes': 34, 'dropout_prob': 0, 'lr': 0.01, 'batch_size': 64}\n",
      "For the Red Wine NN para Qualidade-> 6 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.73      0.70       179\n",
      "           1       0.62      0.56      0.59       141\n",
      "\n",
      "    accuracy                           0.65       320\n",
      "   macro avg       0.65      0.64      0.64       320\n",
      "weighted avg       0.65      0.65      0.65       320\n",
      "\n",
      "Number of FP 49\n",
      "Number of TP 79\n",
      "Number of TP 79\n",
      "269\n",
      "Confusion Matrix para classe 6:\n",
      "[[130  49]\n",
      " [ 62  79]]\n",
      "{'TN': 130, 'FP': 49, 'FN': 62, 'TP': 79}\n",
      "Best Parameters for the 7 -> {'num_nodes': 34, 'dropout_prob': 0.2, 'lr': 0.005, 'batch_size': 32}\n",
      "For the Red Wine NN para Qualidade-> 7 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.92      0.93       282\n",
      "           1       0.47      0.53      0.49        38\n",
      "\n",
      "    accuracy                           0.87       320\n",
      "   macro avg       0.70      0.72      0.71       320\n",
      "weighted avg       0.88      0.87      0.88       320\n",
      "\n",
      "Number of FP 23\n",
      "Number of TP 20\n",
      "Number of TP 20\n",
      "312\n",
      "Confusion Matrix para classe 7:\n",
      "[[259  23]\n",
      " [ 18  20]]\n",
      "{'TN': 259, 'FP': 23, 'FN': 18, 'TP': 20}\n",
      "Best Parameters for the 8 -> {'num_nodes': 34, 'dropout_prob': 0, 'lr': 0.001, 'batch_size': 64}\n",
      "For the Red Wine NN para Qualidade-> 8 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99       319\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.98       320\n",
      "   macro avg       0.50      0.49      0.50       320\n",
      "weighted avg       0.99      0.98      0.99       320\n",
      "\n",
      "Number of FP 5\n",
      "Number of TP 0\n",
      "Number of TP 0\n",
      "317\n",
      "Confusion Matrix para classe 8:\n",
      "[[314   5]\n",
      " [  1   0]]\n",
      "{'TN': 314, 'FP': 5, 'FN': 1, 'TP': 0}\n",
      "Para a analise total temos as medias:\n",
      "Para macro avg ->  {'precision macro avg': 0.341660911768676, 'recall macro avg': 0.325981802666604, 'f1-score macro avg': 0.33042145857632477}\n",
      "Para weighted avg ->  {'precision weighted avg': 0.5893465341578806, 'recall weighted avg': 0.58125, 'f1-score weighted avg': 0.5837611952448324}\n",
      "Numero de amostrar que deram true em algum momento:  317\n",
      "Pela formula:  0\n",
      "Numero de Amostras que sao classificadas mais de uma vez:  317\n",
      "Amostras repetidas pelo menos uma vez 43\n",
      "Numero real de amostras no dataset teste:  320\n",
      "VALORES GERAIS -> FP:  131 FN:  134 TN  1469 TP 186\n",
      "Overall-> Precision: 0.5867507886435331 | Recall: 0.58125|F1-Score: 0.5839874411302982\n",
      "Overall Accuracy:  0.8619791666666666\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class_rep = {}\n",
    "number_samples_final = 0\n",
    "predicted_as_1_in_any_model = np.zeros_like(data_red_wine_one_vs_all['6']['red_wine_y_test'], dtype=int)\n",
    "fpp = 0\n",
    "fnn = 0\n",
    "tnn = 0\n",
    "tpp = 0\n",
    "\n",
    "for i in np.sort(data_red_wine['quality'].unique()):\n",
    "    model,history = train_model_red(data_red_wine_one_vs_all[f'{i}']['red_wine_X_train'], data_red_wine_one_vs_all[f'{i}']['red_wine_y_train'], best_param_red_nn[f'{i}']['num_nodes'], best_param_red_nn[f'{i}']['dropout_prob'], best_param_red_nn[f'{i}']['lr'], best_param_red_nn[f'{i}']['batch_size'],epochs,data_red_wine_one_vs_all[f'{i}']['red_wine_X_valid'],data_red_wine_one_vs_all[f'{i}']['red_wine_y_valid'])\n",
    "    # Convert the TensorFlow tensor to a NumPy array\n",
    "    print(f'Best Parameters for the {i} -> {best_param_red_nn[f\"{i}\"]}')\n",
    "    red_wine_y_pred = model(data_red_wine_one_vs_all[f'{i}']['red_wine_X_test'])\n",
    "    red_wine_y_pred = tf.convert_to_tensor(red_wine_y_pred).numpy()\n",
    "\n",
    "    # Apply the thresholding and reshape\n",
    "    red_wine_y_pred = (red_wine_y_pred > 0.5).astype(int).reshape(-1,)\n",
    "    print('For the Red Wine NN para Qualidade->',i,'\\n',classification_report(data_red_wine_one_vs_all[f'{i}']['red_wine_y_test'],red_wine_y_pred))\n",
    "    class_rep[f'{i}'] = classification_report(data_red_wine_one_vs_all[f'{i}']['red_wine_y_test'],red_wine_y_pred,output_dict=True)\n",
    "    \n",
    "    number_samples_final += np.sum(red_wine_y_pred == 1)\n",
    "    #print(red_wine_y_pred)\n",
    "    predicted_as_1_in_any_model = predicted_as_1_in_any_model + red_wine_y_pred\n",
    "    print(\"Number of FP\",np.sum((~data_red_wine_one_vs_all[f'{i}']['red_wine_y_test'] & red_wine_y_pred)==1))\n",
    "    print(\"Number of TP\",np.sum((data_red_wine_one_vs_all[f'{i}']['red_wine_y_test']+red_wine_y_pred)>1))\n",
    "    print(\"Number of TP\",np.sum((data_red_wine_one_vs_all[f'{i}']['red_wine_y_test']&red_wine_y_pred)==1))\n",
    "    #print(predicted_as_1_in_any_model)\n",
    "    print(number_samples_final)\n",
    "    cm = confusion_matrix(data_red_wine_one_vs_all[f'{i}']['red_wine_y_test'], red_wine_y_pred)\n",
    "    print(f\"Confusion Matrix para classe {i}:\\n{cm}\")\n",
    "    tn, fp, fn, tp = confusion_matrix(data_red_wine_one_vs_all[f'{i}']['red_wine_y_test'], red_wine_y_pred).ravel()\n",
    "    print({'TN': tn, 'FP': fp, 'FN': fn, 'TP': tp})\n",
    "    fpp = fpp + fp\n",
    "    fnn = fn + fnn\n",
    "    tnn = tn + tnn\n",
    "    tpp = tpp + tp\n",
    "\n",
    "## tabela juntando os valores previstos como verdadeiros\n",
    "macro_avgs = {\"precision macro avg\":0,\"recall macro avg\":0,\"f1-score macro avg\":0}\n",
    "weighted_avg = {\"precision weighted avg\":0,\"recall weighted avg\":0,\"f1-score weighted avg\":0}\n",
    "number_samples= 0\n",
    "\n",
    "for i in np.sort(data_red_wine['quality'].unique()):\n",
    "    macro_avgs[\"precision macro avg\"] = macro_avgs[\"precision macro avg\"] + class_rep[f'{i}'][\"1\"][\"precision\"]\n",
    "    weighted_avg[\"precision weighted avg\"] = weighted_avg[\"precision weighted avg\"] + class_rep[f'{i}'][\"1\"][\"precision\"]*class_rep[f'{i}'][\"1\"][\"support\"]\n",
    "    macro_avgs[\"recall macro avg\"] = macro_avgs[\"recall macro avg\"] + class_rep[f'{i}'][\"1\"][\"recall\"]\n",
    "    weighted_avg[\"recall weighted avg\"] = weighted_avg[\"recall weighted avg\"] + class_rep[f'{i}'][\"1\"][\"recall\"]*class_rep[f'{i}'][\"1\"][\"support\"]    \n",
    "    macro_avgs[\"f1-score macro avg\"] = macro_avgs[\"f1-score macro avg\"] + class_rep[f'{i}'][\"1\"][\"f1-score\"]\n",
    "    weighted_avg[\"f1-score weighted avg\"] = weighted_avg[\"f1-score weighted avg\"] + class_rep[f'{i}'][\"1\"][\"f1-score\"]*class_rep[f'{i}'][\"1\"][\"support\"]\n",
    "    number_samples = number_samples + class_rep[f'{i}'][\"1\"][\"support\"]\n",
    "\n",
    "for i in macro_avgs:\n",
    "    macro_avgs[i] = macro_avgs[i]/len(np.sort(data_red_wine['quality'].unique()))\n",
    "\n",
    "for i in weighted_avg:\n",
    "    weighted_avg[i] = weighted_avg[i]/number_samples\n",
    "\n",
    "print(\"Para a analise total temos as medias:\")\n",
    "print(\"Para macro avg -> \",macro_avgs)\n",
    "print(\"Para weighted avg -> \",weighted_avg)\n",
    "print(\"Numero de amostrar que deram true em algum momento: \",number_samples_final)\n",
    "print(\"Pela formula: \",calculo_numero_amostras)\n",
    "print(\"Numero de Amostras que sao classificadas mais de uma vez: \",tpp+fpp)\n",
    "print(\"Amostras repetidas pelo menos uma vez\",np.sum(predicted_as_1_in_any_model>1))\n",
    "print(\"Numero real de amostras no dataset teste: \",len(data_red_wine_one_vs_all['6']['red_wine_y_test']))\n",
    "print('VALORES GERAIS -> FP: ',fpp,'FN: ',fnn,'TN ',tnn,\"TP\",tpp)\n",
    "print(f\"Overall-> Precision: {tpp/(tpp+fpp)} | Recall: {tpp/(tpp+fnn)}|F1-Score: {2/((1/(tpp/(tpp+fpp))) + (1/(tpp/(tpp+fnn))))}\")\n",
    "print(\"Overall Accuracy: \",(tpp+tnn)/(tpp+tnn+fpp+fnn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# White Wine NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_white(X_train, y_train, num_nodes, dropout_prob, lr, batch_size, epochs):\n",
    "    col = X_train.shape[1]\n",
    "    #num_classes = len(np.unique(y_train))\n",
    "    nn_model = tf.keras.Sequential(\n",
    "        [Input(shape=(col,)),\n",
    "         tf.keras.layers.Dense(num_nodes,activation='relu'),\n",
    "         tf.keras.layers.Dropout(dropout_prob),\n",
    "         tf.keras.layers.Dense(num_nodes,activation='relu'),\n",
    "         tf.keras.layers.Dropout(dropout_prob),\n",
    "         tf.keras.layers.Dense(1,activation='sigmoid')\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    nn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),loss='binary_crossentropy',metrics=['accuracy'])\n",
    "    history = nn_model.fit(\n",
    "        X_train, y_train, epochs=epochs, batch_size=batch_size, validation_split=0.2,verbose=0\n",
    "        )\n",
    "    return nn_model, history\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_param(X_train, y_train, X_valid, y_valid):\n",
    "    # Seed para reprodutibilidade\n",
    "    tf.random.set_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Parâmetros para busca\n",
    "    num_nodes_list = [16, 32, 34]\n",
    "    dropout_probs = [0, 0.2]\n",
    "    learning_rates = [0.01, 0.005, 0.001]\n",
    "    batch_sizes = [32, 64, 128]\n",
    "    epochs = 100\n",
    "\n",
    "    # Inicialização do melhor modelo\n",
    "    best_model_params = None\n",
    "    best_f1 = 0\n",
    "    best_accuracy = 0\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    # Loop pelos hiperparâmetros\n",
    "    for num_nodes, dropout_prob, lr, batch_size in product(num_nodes_list, dropout_probs, learning_rates, batch_sizes):\n",
    "        print(f\"\\nTestando: nodes={num_nodes}, dropout={dropout_prob}, lr={lr}, batch={batch_size}\")\n",
    "\n",
    "        # Treina o modelo com os parâmetros atuais\n",
    "        model, history = train_model_white(\n",
    "            X_train, y_train, num_nodes, dropout_prob, lr, batch_size, epochs\n",
    "        )\n",
    "\n",
    "        # Avaliação no conjunto de validação\n",
    "        val_loss = history.history['val_loss'][-1]\n",
    "        val_accuracy = history.history['val_accuracy'][-1]\n",
    "\n",
    "        y_val_pred_probs = model.predict(X_valid)\n",
    "        y_val_pred = (y_val_pred_probs > 0.5).astype(int)\n",
    "        val_f1 = f1_score(y_valid.astype(int), y_val_pred, average='weighted')\n",
    "\n",
    "        # Mostra métricas da iteração\n",
    "        print(f\"F1: {val_f1:.4f}, Acc: {val_accuracy:.4f}, Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Atualiza o melhor conjunto de parâmetros\n",
    "        if (val_f1 > best_f1) or (val_f1 == best_f1 and val_accuracy > best_accuracy) or (val_f1 == best_f1 and val_loss < best_val_loss):\n",
    "            best_f1 = val_f1\n",
    "            best_accuracy = val_accuracy\n",
    "            best_val_loss = val_loss\n",
    "            best_model_params = {\n",
    "                \"num_nodes\": num_nodes,\n",
    "                \"dropout_prob\": dropout_prob,\n",
    "                \"lr\": lr,\n",
    "                \"batch_size\": batch_size\n",
    "            }\n",
    "\n",
    "    return best_model_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9929, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9918, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9929, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9929, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.9923, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9913, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9934, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9918, Acc: 1.0000, Loss: 0.0001\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.9908, Acc: 1.0000, Loss: 0.0012\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.9929, Acc: 1.0000, Loss: 0.0138\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9908, Acc: 1.0000, Loss: 0.0050\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.9929, Acc: 1.0000, Loss: 0.0224\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9908, Acc: 1.0000, Loss: 0.0038\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9913, Acc: 1.0000, Loss: 0.0070\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.9923, Acc: 1.0000, Loss: 0.0041\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9908, Acc: 1.0000, Loss: 0.0072\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9929, Acc: 1.0000, Loss: 0.0077\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9877, Acc: 1.0000, Loss: 0.0108\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9923, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.9934, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9929, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9934, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9934, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9918, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9934, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9929, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9934, Acc: 1.0000, Loss: 0.0002\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9913, Acc: 1.0000, Loss: 0.0010\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9923, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9934, Acc: 1.0000, Loss: 0.0011\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9923, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9929, Acc: 1.0000, Loss: 0.0001\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9929, Acc: 1.0000, Loss: 0.0002\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.9923, Acc: 1.0000, Loss: 0.0002\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9934, Acc: 1.0000, Loss: 0.0011\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9913, Acc: 1.0000, Loss: 0.0024\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9934, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9929, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9918, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9934, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.9929, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9934, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9923, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9934, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.9929, Acc: 1.0000, Loss: 0.0002\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9929, Acc: 1.0000, Loss: 0.0001\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9929, Acc: 1.0000, Loss: 0.0001\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9918, Acc: 1.0000, Loss: 0.0005\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9929, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9929, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9934, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9939, Acc: 1.0000, Loss: 0.0003\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9929, Acc: 1.0000, Loss: 0.0001\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9923, Acc: 1.0000, Loss: 0.0011\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.9544, Acc: 1.0000, Loss: 0.0129\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9537, Acc: 0.9802, Loss: 0.0305\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9400, Acc: 1.0000, Loss: 0.0100\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9464, Acc: 1.0000, Loss: 0.0095\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9450, Acc: 0.9941, Loss: 0.0305\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9414, Acc: 1.0000, Loss: 0.0442\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9412, Acc: 0.9934, Loss: 0.1093\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9413, Acc: 1.0000, Loss: 0.0984\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9375, Acc: 0.9822, Loss: 0.1414\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9179, Acc: 0.9558, Loss: 0.2133\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9151, Acc: 0.9591, Loss: 0.2168\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9181, Acc: 0.9927, Loss: 0.1969\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9201, Acc: 0.9848, Loss: 0.1556\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9176, Acc: 0.9927, Loss: 0.1764\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9386, Acc: 0.9387, Loss: 0.2433\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9166, Acc: 0.9743, Loss: 0.2044\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.9244, Acc: 0.9123, Loss: 0.3134\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9257, Acc: 0.8654, Loss: 0.4077\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9551, Acc: 1.0000, Loss: 0.0051\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9600, Acc: 0.9934, Loss: 0.0119\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9537, Acc: 1.0000, Loss: 0.0038\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9579, Acc: 1.0000, Loss: 0.0005\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9621, Acc: 0.9934, Loss: 0.0117\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.9510, Acc: 0.9208, Loss: 0.2384\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9593, Acc: 1.0000, Loss: 0.0165\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9555, Acc: 1.0000, Loss: 0.0323\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9441, Acc: 1.0000, Loss: 0.0420\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9352, Acc: 0.9789, Loss: 0.1028\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9487, Acc: 1.0000, Loss: 0.0536\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9393, Acc: 1.0000, Loss: 0.0560\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9488, Acc: 0.9934, Loss: 0.0493\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9371, Acc: 1.0000, Loss: 0.0744\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9350, Acc: 1.0000, Loss: 0.0875\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.9317, Acc: 0.9934, Loss: 0.0900\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9390, Acc: 0.9855, Loss: 0.1302\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.9326, Acc: 0.9828, Loss: 0.1481\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9579, Acc: 1.0000, Loss: 0.0051\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9558, Acc: 1.0000, Loss: 0.0049\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9600, Acc: 1.0000, Loss: 0.0022\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9570, Acc: 1.0000, Loss: 0.0046\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9623, Acc: 0.9934, Loss: 0.0087\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9576, Acc: 1.0000, Loss: 0.0040\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9524, Acc: 1.0000, Loss: 0.0064\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9544, Acc: 1.0000, Loss: 0.0192\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9536, Acc: 1.0000, Loss: 0.0266\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9483, Acc: 0.9934, Loss: 0.0701\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9521, Acc: 0.9941, Loss: 0.0535\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9449, Acc: 1.0000, Loss: 0.0482\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9394, Acc: 1.0000, Loss: 0.0585\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9439, Acc: 1.0000, Loss: 0.0502\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9405, Acc: 1.0000, Loss: 0.0578\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9354, Acc: 0.9888, Loss: 0.0873\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9390, Acc: 0.9934, Loss: 0.1154\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9241, Acc: 0.9934, Loss: 0.1381\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7311, Acc: 0.7911, Loss: 0.4676\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7454, Acc: 0.7266, Loss: 0.6105\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7520, Acc: 0.7103, Loss: 0.6281\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7421, Acc: 0.7965, Loss: 0.4850\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7507, Acc: 0.7520, Loss: 0.5378\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7586, Acc: 0.7493, Loss: 0.5718\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7416, Acc: 0.7430, Loss: 0.5922\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7430, Acc: 0.6658, Loss: 0.6616\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.7423, Acc: 0.6703, Loss: 0.6618\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7395, Acc: 0.6776, Loss: 0.6203\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7450, Acc: 0.7675, Loss: 0.6444\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7373, Acc: 0.6748, Loss: 0.6323\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7456, Acc: 0.6540, Loss: 0.6672\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.7451, Acc: 0.6285, Loss: 0.6613\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7478, Acc: 0.7166, Loss: 0.6376\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.7455, Acc: 0.6540, Loss: 0.6652\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7459, Acc: 0.6594, Loss: 0.6625\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.7493, Acc: 0.6894, Loss: 0.6732\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7360, Acc: 0.9210, Loss: 0.2051\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7451, Acc: 0.8547, Loss: 0.3360\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7303, Acc: 0.9083, Loss: 0.2141\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7414, Acc: 0.8910, Loss: 0.3200\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7655, Acc: 0.8683, Loss: 0.3162\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7368, Acc: 0.8365, Loss: 0.4198\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7432, Acc: 0.8056, Loss: 0.4664\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7540, Acc: 0.7693, Loss: 0.5298\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7436, Acc: 0.7339, Loss: 0.5700\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.7428, Acc: 0.6512, Loss: 0.5173\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7614, Acc: 0.6939, Loss: 0.5327\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7633, Acc: 0.6694, Loss: 0.6159\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7544, Acc: 0.7784, Loss: 0.4906\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7451, Acc: 0.6730, Loss: 0.5446\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7428, Acc: 0.7266, Loss: 0.5284\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7504, Acc: 0.7066, Loss: 0.5743\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7661, Acc: 0.7339, Loss: 0.5807\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7608, Acc: 0.6757, Loss: 0.6429\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7592, Acc: 0.8701, Loss: 0.3742\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7582, Acc: 0.8792, Loss: 0.2967\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7725, Acc: 0.8901, Loss: 0.3196\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7394, Acc: 0.9083, Loss: 0.2624\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7385, Acc: 0.8937, Loss: 0.2866\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7682, Acc: 0.8320, Loss: 0.3911\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7590, Acc: 0.8129, Loss: 0.4283\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7530, Acc: 0.8065, Loss: 0.4831\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7328, Acc: 0.7411, Loss: 0.5517\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7595, Acc: 0.7266, Loss: 0.4832\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7473, Acc: 0.6567, Loss: 0.5877\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7623, Acc: 0.6985, Loss: 0.5540\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.7548, Acc: 0.7094, Loss: 0.5431\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.7578, Acc: 0.6857, Loss: 0.5917\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7532, Acc: 0.6785, Loss: 0.5794\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7616, Acc: 0.6776, Loss: 0.6289\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7599, Acc: 0.7348, Loss: 0.5922\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7523, Acc: 0.7221, Loss: 0.6026\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.6330, Acc: 0.6227, Loss: 0.7697\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.6098, Acc: 0.6493, Loss: 0.7739\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.5904, Acc: 0.6215, Loss: 0.7125\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.6053, Acc: 0.5984, Loss: 0.7495\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.5848, Acc: 0.5961, Loss: 0.7304\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.6066, Acc: 0.6574, Loss: 0.6343\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.6238, Acc: 0.5868, Loss: 0.6840\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.6265, Acc: 0.6123, Loss: 0.6787\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.6163, Acc: 0.6100, Loss: 0.6710\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.5481, Acc: 0.5000, Loss: 0.7227\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.6118, Acc: 0.5694, Loss: 0.6656\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.6114, Acc: 0.6157, Loss: 0.6394\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.5962, Acc: 0.5532, Loss: 0.7033\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.5991, Acc: 0.5822, Loss: 0.6570\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.6091, Acc: 0.6157, Loss: 0.6579\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.6149, Acc: 0.5961, Loss: 0.6770\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.6071, Acc: 0.5972, Loss: 0.6674\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.6084, Acc: 0.5984, Loss: 0.6745\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.6113, Acc: 0.7222, Loss: 0.9069\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.6340, Acc: 0.7373, Loss: 0.9595\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.6514, Acc: 0.7269, Loss: 0.8744\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.6153, Acc: 0.7407, Loss: 0.8269\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.6210, Acc: 0.7106, Loss: 0.7521\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.6306, Acc: 0.6968, Loss: 0.6782\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.6228, Acc: 0.6493, Loss: 0.6668\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.6136, Acc: 0.6192, Loss: 0.6812\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.6144, Acc: 0.6609, Loss: 0.6197\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.5884, Acc: 0.5764, Loss: 0.6646\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.6189, Acc: 0.6343, Loss: 0.6135\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.6250, Acc: 0.7060, Loss: 0.6047\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.6004, Acc: 0.6331, Loss: 0.6137\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.6290, Acc: 0.6887, Loss: 0.6076\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.6400, Acc: 0.6481, Loss: 0.6089\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.6197, Acc: 0.5984, Loss: 0.6647\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.6199, Acc: 0.6146, Loss: 0.6555\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.6177, Acc: 0.6354, Loss: 0.6482\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.6254, Acc: 0.7083, Loss: 1.0356\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.6332, Acc: 0.7512, Loss: 0.9855\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.6208, Acc: 0.7025, Loss: 0.8492\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.6193, Acc: 0.7176, Loss: 0.8286\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.6294, Acc: 0.7106, Loss: 0.7421\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.6473, Acc: 0.7049, Loss: 0.7163\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.6218, Acc: 0.6447, Loss: 0.6740\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.6229, Acc: 0.6192, Loss: 0.6775\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.6188, Acc: 0.6331, Loss: 0.6499\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.5888, Acc: 0.5718, Loss: 0.6489\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.5764, Acc: 0.6100, Loss: 0.6100\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.6389, Acc: 0.6562, Loss: 0.6039\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.5948, Acc: 0.5729, Loss: 0.6627\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.6278, Acc: 0.6458, Loss: 0.6246\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.6336, Acc: 0.6678, Loss: 0.5924\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.6215, Acc: 0.6273, Loss: 0.6445\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.5989, Acc: 0.6377, Loss: 0.6420\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.6233, Acc: 0.5926, Loss: 0.6561\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.8076, Acc: 0.8958, Loss: 0.4167\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.8040, Acc: 0.8569, Loss: 0.4150\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.8050, Acc: 0.8351, Loss: 0.4040\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.8088, Acc: 0.8507, Loss: 0.4431\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.8041, Acc: 0.8359, Loss: 0.4620\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7823, Acc: 0.8880, Loss: 0.3899\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.8089, Acc: 0.7970, Loss: 0.5097\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7933, Acc: 0.7504, Loss: 0.5424\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.8010, Acc: 0.7263, Loss: 0.5982\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.8156, Acc: 0.7224, Loss: 0.5880\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7913, Acc: 0.6291, Loss: 0.6344\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.7813, Acc: 0.8421, Loss: 0.5532\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.8101, Acc: 0.6555, Loss: 0.6027\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7972, Acc: 0.7372, Loss: 0.6171\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.7901, Acc: 0.7379, Loss: 0.5675\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.8063, Acc: 0.6641, Loss: 0.6722\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7885, Acc: 0.7201, Loss: 0.6162\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.7995, Acc: 0.7123, Loss: 0.6806\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.8261, Acc: 0.9541, Loss: 0.1062\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.8348, Acc: 0.9705, Loss: 0.1277\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.8305, Acc: 0.9673, Loss: 0.1081\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.8384, Acc: 0.9487, Loss: 0.1398\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.8390, Acc: 0.9549, Loss: 0.1278\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.8266, Acc: 0.9432, Loss: 0.1855\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.8090, Acc: 0.8546, Loss: 0.4130\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7939, Acc: 0.8476, Loss: 0.3972\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.8043, Acc: 0.8227, Loss: 0.4446\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7897, Acc: 0.9316, Loss: 0.4066\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.8219, Acc: 0.7908, Loss: 0.4526\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7975, Acc: 0.9044, Loss: 0.4287\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7982, Acc: 0.8740, Loss: 0.4346\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.8008, Acc: 0.8826, Loss: 0.4000\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.8178, Acc: 0.7574, Loss: 0.4782\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7967, Acc: 0.8025, Loss: 0.5034\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.8034, Acc: 0.7885, Loss: 0.5521\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.8032, Acc: 0.7659, Loss: 0.5588\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.8348, Acc: 0.9308, Loss: 0.2517\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.8331, Acc: 0.9961, Loss: 0.0344\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.8159, Acc: 0.9930, Loss: 0.0492\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.8201, Acc: 0.9588, Loss: 0.1390\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.8390, Acc: 0.9261, Loss: 0.1924\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.8156, Acc: 0.9479, Loss: 0.1684\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.8076, Acc: 0.8320, Loss: 0.4103\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.8170, Acc: 0.8927, Loss: 0.3475\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.8101, Acc: 0.8336, Loss: 0.4319\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.8000, Acc: 0.9176, Loss: 0.4241\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.8081, Acc: 0.8927, Loss: 0.4131\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.8182, Acc: 0.9106, Loss: 0.3856\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.8222, Acc: 0.8017, Loss: 0.4606\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.8407, Acc: 0.7535, Loss: 0.5052\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.8136, Acc: 0.9137, Loss: 0.3906\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.7963, Acc: 0.8639, Loss: 0.4710\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.8032, Acc: 0.8250, Loss: 0.5313\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.7809, Acc: 0.8196, Loss: 0.5280\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9598, Acc: 1.0000, Loss: 0.0189\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9538, Acc: 1.0000, Loss: 0.0253\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9464, Acc: 1.0000, Loss: 0.0161\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.9507, Acc: 1.0000, Loss: 0.0174\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9527, Acc: 1.0000, Loss: 0.0131\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9530, Acc: 1.0000, Loss: 0.0351\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9352, Acc: 0.9907, Loss: 0.0714\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.9256, Acc: 0.9610, Loss: 0.1436\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9285, Acc: 0.9517, Loss: 0.1818\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.8885, Acc: 0.9431, Loss: 0.3489\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9020, Acc: 0.9107, Loss: 0.3201\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.8918, Acc: 0.9140, Loss: 0.3655\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.8922, Acc: 0.9854, Loss: 0.2400\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.8968, Acc: 0.9841, Loss: 0.2434\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.8989, Acc: 0.9570, Loss: 0.2457\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.8860, Acc: 0.9636, Loss: 0.3431\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.8794, Acc: 0.9676, Loss: 0.3206\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.8774, Acc: 0.9418, Loss: 0.3510\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9674, Acc: 1.0000, Loss: 0.0106\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9635, Acc: 1.0000, Loss: 0.0045\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9666, Acc: 1.0000, Loss: 0.0012\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9628, Acc: 1.0000, Loss: 0.0010\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9651, Acc: 1.0000, Loss: 0.0012\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.9628, Acc: 1.0000, Loss: 0.0012\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.9609, Acc: 1.0000, Loss: 0.0024\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9584, Acc: 1.0000, Loss: 0.0219\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9524, Acc: 1.0000, Loss: 0.0283\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9445, Acc: 0.9484, Loss: 0.1562\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9559, Acc: 0.9854, Loss: 0.1038\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "F1: 0.9459, Acc: 0.9921, Loss: 0.0811\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9359, Acc: 0.9921, Loss: 0.0793\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.9506, Acc: 1.0000, Loss: 0.0683\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.9359, Acc: 0.9934, Loss: 0.1153\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.9375, Acc: 0.9696, Loss: 0.1417\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9178, Acc: 0.9907, Loss: 0.1614\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.9172, Acc: 0.9841, Loss: 0.1863\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9678, Acc: 1.0000, Loss: 0.0013\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9591, Acc: 1.0000, Loss: 0.0123\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9620, Acc: 1.0000, Loss: 0.0002\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9674, Acc: 1.0000, Loss: 0.0009\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9647, Acc: 1.0000, Loss: 0.0009\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.9639, Acc: 1.0000, Loss: 0.0012\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9492, Acc: 1.0000, Loss: 0.0029\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9576, Acc: 1.0000, Loss: 0.0088\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9534, Acc: 1.0000, Loss: 0.0454\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9455, Acc: 0.9630, Loss: 0.0861\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9498, Acc: 0.9894, Loss: 0.0951\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9552, Acc: 0.9934, Loss: 0.0744\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9317, Acc: 0.9907, Loss: 0.0784\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9428, Acc: 0.9934, Loss: 0.0953\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9414, Acc: 1.0000, Loss: 0.0807\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9316, Acc: 0.9907, Loss: 0.0960\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9402, Acc: 0.9775, Loss: 0.1542\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.9300, Acc: 0.9623, Loss: 0.1924\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9974, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9969, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9974, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9980, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9980, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9969, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9974, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9974, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9974, Acc: 1.0000, Loss: 0.0001\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9985, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9985, Acc: 1.0000, Loss: 0.0001\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9980, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9985, Acc: 1.0000, Loss: 0.0001\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9985, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9985, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9980, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9980, Acc: 1.0000, Loss: 0.0004\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9985, Acc: 1.0000, Loss: 0.0001\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9985, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9985, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9974, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9974, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9974, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9969, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9974, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9969, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.9974, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9985, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9985, Acc: 1.0000, Loss: 0.0001\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9985, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9985, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9985, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9985, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9985, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9985, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.9985, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9974, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9969, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9969, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9980, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.9974, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9969, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.9974, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.9969, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9974, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9980, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.9980, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9985, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9980, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9985, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9985, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9985, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.9985, Acc: 1.0000, Loss: 0.0000\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "F1: 0.9985, Acc: 1.0000, Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "best_param_white_nn = {}\n",
    "# Exibir os melhores parâmetros encontrados\n",
    "for i in np.sort(data_white_wine['quality'].unique()):\n",
    "    best_param_white_nn[f'{i}'] = find_best_param(data_white_wine_one_vs_all[f'{i}']['white_wine_X_train'], data_white_wine_one_vs_all[f'{i}']['white_wine_y_train'],data_white_wine_one_vs_all[f'{i}']['white_wine_X_test'],data_white_wine_one_vs_all[f'{i}']['white_wine_y_test'])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'3': {'num_nodes': 34, 'dropout_prob': 0.2, 'lr': 0.001, 'batch_size': 32}, '4': {'num_nodes': 34, 'dropout_prob': 0, 'lr': 0.005, 'batch_size': 64}, '5': {'num_nodes': 34, 'dropout_prob': 0, 'lr': 0.01, 'batch_size': 128}, '6': {'num_nodes': 32, 'dropout_prob': 0, 'lr': 0.01, 'batch_size': 128}, '7': {'num_nodes': 34, 'dropout_prob': 0.2, 'lr': 0.005, 'batch_size': 64}, '8': {'num_nodes': 34, 'dropout_prob': 0, 'lr': 0.01, 'batch_size': 32}, '9': {'num_nodes': 32, 'dropout_prob': 0, 'lr': 0.01, 'batch_size': 32}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(best_param_white_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "## como ja achei os melhores parametros vou deixar aq pra agilizar\n",
    "\n",
    "best_param_white_nn = {'3': {'num_nodes': 34, 'dropout_prob': 0.2, 'lr': 0.001, 'batch_size': 32}, '4': {'num_nodes': 34, 'dropout_prob': 0, 'lr': 0.005, 'batch_size': 64}, '5': {'num_nodes': 34, 'dropout_prob': 0, 'lr': 0.01, 'batch_size': 128}, '6': {'num_nodes': 32, 'dropout_prob': 0, 'lr': 0.01, 'batch_size': 128}, '7': {'num_nodes': 34, 'dropout_prob': 0.2, 'lr': 0.005, 'batch_size': 64}, '8': {'num_nodes': 34, 'dropout_prob': 0, 'lr': 0.01, 'batch_size': 32}, '9': {'num_nodes': 32, 'dropout_prob': 0, 'lr': 0.01, 'batch_size': 32}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters for the 3 -> {'num_nodes': 34, 'dropout_prob': 0.2, 'lr': 0.001, 'batch_size': 32}\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For the White Wine rfc para Qualidade-> 3 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       976\n",
      "           1       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.99       980\n",
      "   macro avg       0.50      0.50      0.50       980\n",
      "weighted avg       0.99      0.99      0.99       980\n",
      "\n",
      "Number of FP 2\n",
      "Number of TP 0\n",
      "Number of TP 0\n",
      "2\n",
      "Confusion Matrix para classe 3:\n",
      "[[974   2]\n",
      " [  4   0]]\n",
      "{'TN': 974, 'FP': 2, 'FN': 4, 'TP': 0}\n",
      "Best Parameters for the 4 -> {'num_nodes': 34, 'dropout_prob': 0, 'lr': 0.005, 'batch_size': 64}\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For the White Wine rfc para Qualidade-> 4 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       947\n",
      "           1       0.44      0.42      0.43        33\n",
      "\n",
      "    accuracy                           0.96       980\n",
      "   macro avg       0.71      0.70      0.71       980\n",
      "weighted avg       0.96      0.96      0.96       980\n",
      "\n",
      "Number of FP 18\n",
      "Number of TP 14\n",
      "Number of TP 14\n",
      "34\n",
      "Confusion Matrix para classe 4:\n",
      "[[929  18]\n",
      " [ 19  14]]\n",
      "{'TN': 929, 'FP': 18, 'FN': 19, 'TP': 14}\n",
      "Best Parameters for the 5 -> {'num_nodes': 34, 'dropout_prob': 0, 'lr': 0.01, 'batch_size': 128}\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For the White Wine rfc para Qualidade-> 5 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.82      0.83      0.82       689\n",
      "           1       0.58      0.56      0.57       291\n",
      "\n",
      "    accuracy                           0.75       980\n",
      "   macro avg       0.70      0.69      0.69       980\n",
      "weighted avg       0.75      0.75      0.75       980\n",
      "\n",
      "Number of FP 118\n",
      "Number of TP 162\n",
      "Number of TP 162\n",
      "314\n",
      "Confusion Matrix para classe 5:\n",
      "[[571 118]\n",
      " [129 162]]\n",
      "{'TN': 571, 'FP': 118, 'FN': 129, 'TP': 162}\n",
      "Best Parameters for the 6 -> {'num_nodes': 32, 'dropout_prob': 0, 'lr': 0.01, 'batch_size': 128}\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For the White Wine rfc para Qualidade-> 6 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.66      0.72      0.69       540\n",
      "           1       0.62      0.55      0.58       440\n",
      "\n",
      "    accuracy                           0.65       980\n",
      "   macro avg       0.64      0.64      0.64       980\n",
      "weighted avg       0.64      0.65      0.64       980\n",
      "\n",
      "Number of FP 151\n",
      "Number of TP 244\n",
      "Number of TP 244\n",
      "709\n",
      "Confusion Matrix para classe 6:\n",
      "[[389 151]\n",
      " [196 244]]\n",
      "{'TN': 389, 'FP': 151, 'FN': 196, 'TP': 244}\n",
      "Best Parameters for the 7 -> {'num_nodes': 34, 'dropout_prob': 0.2, 'lr': 0.005, 'batch_size': 64}\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For the White Wine rfc para Qualidade-> 7 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.83      0.87       804\n",
      "           1       0.46      0.65      0.54       176\n",
      "\n",
      "    accuracy                           0.80       980\n",
      "   macro avg       0.69      0.74      0.70       980\n",
      "weighted avg       0.83      0.80      0.81       980\n",
      "\n",
      "Number of FP 137\n",
      "Number of TP 115\n",
      "Number of TP 115\n",
      "961\n",
      "Confusion Matrix para classe 7:\n",
      "[[667 137]\n",
      " [ 61 115]]\n",
      "{'TN': 667, 'FP': 137, 'FN': 61, 'TP': 115}\n",
      "Best Parameters for the 8 -> {'num_nodes': 34, 'dropout_prob': 0, 'lr': 0.01, 'batch_size': 32}\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For the White Wine rfc para Qualidade-> 8 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.98      0.98       945\n",
      "           1       0.54      0.54      0.54        35\n",
      "\n",
      "    accuracy                           0.97       980\n",
      "   macro avg       0.76      0.76      0.76       980\n",
      "weighted avg       0.97      0.97      0.97       980\n",
      "\n",
      "Number of FP 16\n",
      "Number of TP 19\n",
      "Number of TP 19\n",
      "996\n",
      "Confusion Matrix para classe 8:\n",
      "[[929  16]\n",
      " [ 16  19]]\n",
      "{'TN': 929, 'FP': 16, 'FN': 16, 'TP': 19}\n",
      "Best Parameters for the 9 -> {'num_nodes': 32, 'dropout_prob': 0, 'lr': 0.01, 'batch_size': 32}\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n",
      "For the White Wine rfc para Qualidade-> 9 \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       979\n",
      "           1       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           1.00       980\n",
      "   macro avg       0.50      0.50      0.50       980\n",
      "weighted avg       1.00      1.00      1.00       980\n",
      "\n",
      "Number of FP 3\n",
      "Number of TP 0\n",
      "Number of TP 0\n",
      "999\n",
      "Confusion Matrix para classe 9:\n",
      "[[976   3]\n",
      " [  1   0]]\n",
      "{'TN': 976, 'FP': 3, 'FN': 1, 'TP': 0}\n",
      "Para a analise total temos as medias:\n",
      "Para macro avg ->  {'precision macro avg': 0.3761427566807313, 'recall macro avg': 0.3902507347831353, 'f1-score macro avg': 0.3804094654425572}\n",
      "Para weighted avg ->  {'precision weighted avg': 0.5652211371287514, 'recall weighted avg': 0.5653061224489796, 'f1-score weighted avg': 0.561291188874486}\n",
      "Numero de amostrar que deram true em algum momento:  999\n",
      "Pela formula:  0\n",
      "Numero de Amostras que sao classificadas mais de uma vez:  999\n",
      "Amostras repetidas pelo menos uma vez 189\n",
      "Numero real de amostras no dataset teste:  980\n",
      "VALORES GERAIS -> FP:  445 FN:  426 TN  5435 TP 554\n",
      "Overall-> Precision: 0.5545545545545546 | Recall: 0.5653061224489796|F1-Score: 0.5598787266296109\n",
      "Overall Accuracy:  0.8730320699708455\n"
     ]
    }
   ],
   "source": [
    "class_rep = {}\n",
    "number_samples_final = 0\n",
    "predicted_as_1_in_any_model = np.zeros_like(data_white_wine_one_vs_all['6']['white_wine_y_test'], dtype=int)\n",
    "fpp = 0\n",
    "fnn = 0\n",
    "tnn = 0\n",
    "tpp = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in np.sort(data_white_wine['quality'].unique()):\n",
    "    model,history = train_model_white(data_white_wine_one_vs_all[f'{i}']['white_wine_X_train'], data_white_wine_one_vs_all[f'{i}']['white_wine_y_train'], best_param_white_nn[f'{i}']['num_nodes'], best_param_white_nn[f'{i}']['dropout_prob'], best_param_white_nn[f'{i}']['lr'], best_param_white_nn[f'{i}']['batch_size'], epochs)\n",
    "    # Convert the TensorFlow tensor to a NumPy array\n",
    "    print(f'Best Parameters for the {i} -> {best_param_white_nn[f\"{i}\"]}')\n",
    "    white_wine_y_pred = model.predict(data_white_wine_one_vs_all[f'{i}']['white_wine_X_test'])\n",
    "    white_wine_y_pred = tf.convert_to_tensor(white_wine_y_pred).numpy()\n",
    "\n",
    "    # Apply the thresholding and reshape\n",
    "    white_wine_y_pred = (white_wine_y_pred > 0.5).astype(int).reshape(-1,)\n",
    "    print('For the White Wine rfc para Qualidade->',i,'\\n',classification_report(data_white_wine_one_vs_all[f'{i}']['white_wine_y_test'],white_wine_y_pred,zero_division=True))\n",
    "\n",
    "    class_rep[f'{i}'] = classification_report(data_white_wine_one_vs_all[f'{i}']['white_wine_y_test'],white_wine_y_pred,output_dict=True)\n",
    "    \n",
    "    number_samples_final += np.sum(white_wine_y_pred == 1)\n",
    "    #print(white_wine_y_pred)\n",
    "    predicted_as_1_in_any_model = predicted_as_1_in_any_model + white_wine_y_pred\n",
    "    print(\"Number of FP\",np.sum((~data_white_wine_one_vs_all[f'{i}']['white_wine_y_test'] & white_wine_y_pred)==1))\n",
    "    print(\"Number of TP\",np.sum((data_white_wine_one_vs_all[f'{i}']['white_wine_y_test']+white_wine_y_pred)>1))\n",
    "    print(\"Number of TP\",np.sum((data_white_wine_one_vs_all[f'{i}']['white_wine_y_test']&white_wine_y_pred)==1))\n",
    "    #print(predicted_as_1_in_any_model)\n",
    "    print(number_samples_final)\n",
    "    cm = confusion_matrix(data_white_wine_one_vs_all[f'{i}']['white_wine_y_test'], white_wine_y_pred)\n",
    "    print(f\"Confusion Matrix para classe {i}:\\n{cm}\")\n",
    "    tn, fp, fn, tp = confusion_matrix(data_white_wine_one_vs_all[f'{i}']['white_wine_y_test'], white_wine_y_pred).ravel()\n",
    "    print({'TN': tn, 'FP': fp, 'FN': fn, 'TP': tp})\n",
    "    fpp = fpp + fp\n",
    "    fnn = fn + fnn\n",
    "    tnn = tn + tnn\n",
    "    tpp = tpp + tp\n",
    "\n",
    "\n",
    "## tabela juntando os valores previstos como verdadeiros\n",
    "macro_avgs = {\"precision macro avg\":0,\"recall macro avg\":0,\"f1-score macro avg\":0}\n",
    "weighted_avg = {\"precision weighted avg\":0,\"recall weighted avg\":0,\"f1-score weighted avg\":0}\n",
    "number_samples= 0\n",
    "for i in np.sort(data_white_wine['quality'].unique()):\n",
    "    macro_avgs[\"precision macro avg\"] = macro_avgs[\"precision macro avg\"] + class_rep[f'{i}'][\"1\"][\"precision\"]\n",
    "    weighted_avg[\"precision weighted avg\"] = weighted_avg[\"precision weighted avg\"] + class_rep[f'{i}'][\"1\"][\"precision\"]*class_rep[f'{i}'][\"1\"][\"support\"]\n",
    "    macro_avgs[\"recall macro avg\"] = macro_avgs[\"recall macro avg\"] + class_rep[f'{i}'][\"1\"][\"recall\"]\n",
    "    weighted_avg[\"recall weighted avg\"] = weighted_avg[\"recall weighted avg\"] + class_rep[f'{i}'][\"1\"][\"recall\"]*class_rep[f'{i}'][\"1\"][\"support\"]    \n",
    "    macro_avgs[\"f1-score macro avg\"] = macro_avgs[\"f1-score macro avg\"] + class_rep[f'{i}'][\"1\"][\"f1-score\"]\n",
    "    weighted_avg[\"f1-score weighted avg\"] = weighted_avg[\"f1-score weighted avg\"] + class_rep[f'{i}'][\"1\"][\"f1-score\"]*class_rep[f'{i}'][\"1\"][\"support\"]\n",
    "    number_samples = number_samples + class_rep[f'{i}'][\"1\"][\"support\"]\n",
    "\n",
    "for i in macro_avgs:\n",
    "    macro_avgs[i] = macro_avgs[i]/len(np.sort(data_white_wine['quality'].unique()))\n",
    "\n",
    "for i in weighted_avg:\n",
    "    weighted_avg[i] = weighted_avg[i]/number_samples\n",
    "\n",
    "print(\"Para a analise total temos as medias:\")\n",
    "print(\"Para macro avg -> \",macro_avgs)\n",
    "print(\"Para weighted avg -> \",weighted_avg)\n",
    "print(\"Numero de amostrar que deram true em algum momento: \",number_samples_final)\n",
    "print(\"Pela formula: \",calculo_numero_amostras)\n",
    "print(\"Numero de Amostras que sao classificadas mais de uma vez: \",tpp+fpp)\n",
    "print(\"Amostras repetidas pelo menos uma vez\",np.sum(predicted_as_1_in_any_model>1))\n",
    "print(\"Numero real de amostras no dataset teste: \",len(data_white_wine_one_vs_all['6']['white_wine_y_test']))\n",
    "print('VALORES GERAIS -> FP: ',fpp,'FN: ',fnn,'TN ',tnn,\"TP\",tpp)\n",
    "print(f\"Overall-> Precision: {tpp/(tpp+fpp)} | Recall: {tpp/(tpp+fnn)}|F1-Score: {2/((1/(tpp/(tpp+fpp))) + (1/(tpp/(tpp+fnn))))}\")\n",
    "print(\"Overall Accuracy: \",(tpp+tnn)/(tpp+tnn+fpp+fnn))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### feito"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
