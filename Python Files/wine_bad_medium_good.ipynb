{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import RandomOverSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saw this one in a video and tought this was a good idea to improve \n",
    "# the model, eventought we lose some information\n",
    "# but for this kind of data is worthy\n",
    "\n",
    "red_wine = 'C://Users//vinic//Documents//Python//Machine Learning//Projetos UCI ML//Wine Quality//wine+quality//winequality-red.csv'\n",
    "white_wine = 'C://Users//vinic//Documents//Python//Machine Learning//Projetos UCI ML//Wine Quality//wine+quality//winequality-white.csv'\n",
    "data_red_wine = pd.read_csv(red_wine,header=0,sep=';')\n",
    "data_white_wine = pd.read_csv(white_wine,header=0,sep=';')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0,5.5,7.5,10]\n",
    "labels = [0,1,2]\n",
    "\n",
    "data_red_wine['quality'] = pd.cut(data_red_wine['quality'],bins = bins, labels = labels)\n",
    "\n",
    "data_white_wine['quality'] = pd.cut(data_white_wine['quality'],bins = bins, labels = labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fixed acidity</th>\n",
       "      <th>volatile acidity</th>\n",
       "      <th>citric acid</th>\n",
       "      <th>residual sugar</th>\n",
       "      <th>chlorides</th>\n",
       "      <th>free sulfur dioxide</th>\n",
       "      <th>total sulfur dioxide</th>\n",
       "      <th>density</th>\n",
       "      <th>pH</th>\n",
       "      <th>sulphates</th>\n",
       "      <th>alcohol</th>\n",
       "      <th>quality</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7.0</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.36</td>\n",
       "      <td>20.7</td>\n",
       "      <td>0.045</td>\n",
       "      <td>45.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>1.00100</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>8.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.34</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.049</td>\n",
       "      <td>14.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>0.99400</td>\n",
       "      <td>3.30</td>\n",
       "      <td>0.49</td>\n",
       "      <td>9.5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8.1</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.40</td>\n",
       "      <td>6.9</td>\n",
       "      <td>0.050</td>\n",
       "      <td>30.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>0.99510</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>10.1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.99560</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.2</td>\n",
       "      <td>0.23</td>\n",
       "      <td>0.32</td>\n",
       "      <td>8.5</td>\n",
       "      <td>0.058</td>\n",
       "      <td>47.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>0.99560</td>\n",
       "      <td>3.19</td>\n",
       "      <td>0.40</td>\n",
       "      <td>9.9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4893</th>\n",
       "      <td>6.2</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.29</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.039</td>\n",
       "      <td>24.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>0.99114</td>\n",
       "      <td>3.27</td>\n",
       "      <td>0.50</td>\n",
       "      <td>11.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4894</th>\n",
       "      <td>6.6</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.36</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.047</td>\n",
       "      <td>57.0</td>\n",
       "      <td>168.0</td>\n",
       "      <td>0.99490</td>\n",
       "      <td>3.15</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4895</th>\n",
       "      <td>6.5</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.19</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.041</td>\n",
       "      <td>30.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>0.99254</td>\n",
       "      <td>2.99</td>\n",
       "      <td>0.46</td>\n",
       "      <td>9.4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4896</th>\n",
       "      <td>5.5</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.30</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.022</td>\n",
       "      <td>20.0</td>\n",
       "      <td>110.0</td>\n",
       "      <td>0.98869</td>\n",
       "      <td>3.34</td>\n",
       "      <td>0.38</td>\n",
       "      <td>12.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4897</th>\n",
       "      <td>6.0</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.020</td>\n",
       "      <td>22.0</td>\n",
       "      <td>98.0</td>\n",
       "      <td>0.98941</td>\n",
       "      <td>3.26</td>\n",
       "      <td>0.32</td>\n",
       "      <td>11.8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4898 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
       "0               7.0              0.27         0.36            20.7      0.045   \n",
       "1               6.3              0.30         0.34             1.6      0.049   \n",
       "2               8.1              0.28         0.40             6.9      0.050   \n",
       "3               7.2              0.23         0.32             8.5      0.058   \n",
       "4               7.2              0.23         0.32             8.5      0.058   \n",
       "...             ...               ...          ...             ...        ...   \n",
       "4893            6.2              0.21         0.29             1.6      0.039   \n",
       "4894            6.6              0.32         0.36             8.0      0.047   \n",
       "4895            6.5              0.24         0.19             1.2      0.041   \n",
       "4896            5.5              0.29         0.30             1.1      0.022   \n",
       "4897            6.0              0.21         0.38             0.8      0.020   \n",
       "\n",
       "      free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
       "0                    45.0                 170.0  1.00100  3.00       0.45   \n",
       "1                    14.0                 132.0  0.99400  3.30       0.49   \n",
       "2                    30.0                  97.0  0.99510  3.26       0.44   \n",
       "3                    47.0                 186.0  0.99560  3.19       0.40   \n",
       "4                    47.0                 186.0  0.99560  3.19       0.40   \n",
       "...                   ...                   ...      ...   ...        ...   \n",
       "4893                 24.0                  92.0  0.99114  3.27       0.50   \n",
       "4894                 57.0                 168.0  0.99490  3.15       0.46   \n",
       "4895                 30.0                 111.0  0.99254  2.99       0.46   \n",
       "4896                 20.0                 110.0  0.98869  3.34       0.38   \n",
       "4897                 22.0                  98.0  0.98941  3.26       0.32   \n",
       "\n",
       "      alcohol quality  \n",
       "0         8.8       1  \n",
       "1         9.5       1  \n",
       "2        10.1       1  \n",
       "3         9.9       1  \n",
       "4         9.9       1  \n",
       "...       ...     ...  \n",
       "4893     11.2       1  \n",
       "4894      9.6       0  \n",
       "4895      9.4       1  \n",
       "4896     12.8       1  \n",
       "4897     11.8       1  \n",
       "\n",
       "[4898 rows x 12 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_white_wine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Validation and test Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vinic\\anaconda3\\envs\\myenv\\lib\\site-packages\\numpy\\core\\fromnumeric.py:59: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "#data.sample shuffles the data\n",
    "\n",
    "red_wine_train, red_wine_valid, red_wine_test = np.split(data_red_wine.sample(frac=1), [int(0.6*len(data_red_wine)), int(0.8*len(data_red_wine))])\n",
    "white_wine_train, white_wine_valid, white_wine_test = np.split(data_white_wine.sample(frac=1), [int(0.6*len(data_white_wine)), int(0.8*len(data_white_wine))])\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_dataset(dataframe,oversample = False):\n",
    "    X = dataframe[dataframe.columns[:-1]].values\n",
    "    y = dataframe[dataframe.columns[-1]].values\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    #Scale the number, so we dont have  huge discrepancy between columns, it affects the model\n",
    "    X = scaler.fit_transform(X)\n",
    "    \n",
    "    #the difference between the len of the values must not be huge, so we have to scale it\n",
    "    #oversample the one that has the least, taking more of the less class\n",
    "    \n",
    "    if oversample:\n",
    "        ros = RandomOverSampler()\n",
    "        X,y = ros.fit_resample(X,y)\n",
    "        \n",
    "    # concat 2 arrays, y is only one dimension so we have to make it 2 \n",
    "    #in this function using -1 is the same as len(y)\n",
    "    #same as concat in pandas put hstack in numpay\n",
    "    df = np.hstack((X,np.reshape(y,(-1,1))))\n",
    "    \n",
    "    return df,X,y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## red\n",
    "red_wine_train, red_wine_X_train, red_wine_y_train = scale_dataset(red_wine_train,oversample=True)\n",
    "red_wine_valid, red_wine_X_valid, red_wine_y_valid = scale_dataset(red_wine_valid,oversample=False)\n",
    "red_wine_test, red_wine_X_test, red_wine_y_test = scale_dataset(red_wine_test,oversample=False)\n",
    "\n",
    "## white\n",
    "\n",
    "white_wine_train, white_wine_X_train, white_wine_y_train = scale_dataset(white_wine_train,oversample=True)\n",
    "white_wine_valid, white_wine_X_valid, white_wine_y_valid = scale_dataset(white_wine_valid,oversample=False)\n",
    "white_wine_test, white_wine_X_test, white_wine_y_test = scale_dataset(white_wine_test,oversample=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict the Quality in the margins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-nearest neighbors - KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the Red Wine Knn\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.76      0.73       140\n",
      "           1       0.78      0.71      0.74       176\n",
      "           2       0.25      0.25      0.25         4\n",
      "\n",
      "    accuracy                           0.73       320\n",
      "   macro avg       0.57      0.57      0.57       320\n",
      "weighted avg       0.73      0.73      0.73       320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## red wine\n",
    "red_wine_knn_model = KNeighborsClassifier(n_neighbors=1)\n",
    "red_wine_knn_model.fit(red_wine_X_train,red_wine_y_train)\n",
    "#use the knn model to try to predict the test dataset\n",
    "red_wine_y_pred = red_wine_knn_model.predict(red_wine_X_test)\n",
    "## for better understanding of precition and recall\n",
    "## https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "print('For the Red Wine Knn\\n',classification_report(red_wine_y_test,red_wine_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## improved 0.74 - 0.57 = 0.17 , in comparisson to using all the qualities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the White Wine Knn\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.65      0.66       329\n",
      "           1       0.79      0.80      0.80       619\n",
      "           2       0.43      0.41      0.42        32\n",
      "\n",
      "    accuracy                           0.74       980\n",
      "   macro avg       0.63      0.62      0.63       980\n",
      "weighted avg       0.74      0.74      0.74       980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## white wine\n",
    "white_wine_knn_model = KNeighborsClassifier(n_neighbors=1)\n",
    "white_wine_knn_model.fit(white_wine_X_train,white_wine_y_train)\n",
    "#use the knn model to try to predict the test dataset\n",
    "white_wine_y_pred = white_wine_knn_model.predict(white_wine_X_test)\n",
    "## for better understanding of precition and recall\n",
    "## https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "print('For the White Wine Knn\\n',classification_report(white_wine_y_test,white_wine_y_pred,zero_division=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## improved 0.73 - 0.56 = 0.17 , in comparisson to using all the qualities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the Red Wine NB\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.76      0.74       140\n",
      "           1       0.72      0.49      0.58       176\n",
      "           2       0.02      0.25      0.04         4\n",
      "\n",
      "    accuracy                           0.60       320\n",
      "   macro avg       0.49      0.50      0.45       320\n",
      "weighted avg       0.71      0.60      0.64       320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## red wine\n",
    "red_wine_nb_model = GaussianNB()\n",
    "red_wine_nb_model.fit(red_wine_X_train,red_wine_y_train)\n",
    "#use the knn model to try to predict the test dataset\n",
    "red_wine_y_pred = red_wine_nb_model.predict(red_wine_X_test)\n",
    "## for better understanding of precition and recall\n",
    "## https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "print('For the Red Wine NB\\n',classification_report(red_wine_y_test,red_wine_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## improved 0.53 - 0.41 = 0.12 , in comparisson to using all the qualities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the White Wine NB\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.73      0.64       329\n",
      "           1       0.73      0.18      0.29       619\n",
      "           2       0.06      0.69      0.10        32\n",
      "\n",
      "    accuracy                           0.38       980\n",
      "   macro avg       0.45      0.53      0.34       980\n",
      "weighted avg       0.65      0.38      0.40       980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## white wine\n",
    "white_wine_nb_model = GaussianNB()\n",
    "white_wine_nb_model.fit(white_wine_X_train,white_wine_y_train)\n",
    "\n",
    "white_wine_y_pred = white_wine_nb_model.predict(white_wine_X_test)\n",
    "## for better understanding of precition and recall\n",
    "## https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "print('For the White Wine NB\\n',classification_report(white_wine_y_test,white_wine_y_pred,zero_division=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## improved 0.40 - 0.47 = -0.07 , in comparisson to using all the qualities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the Red Wine SVM\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.86      0.77       140\n",
      "           1       0.84      0.64      0.73       176\n",
      "           2       0.08      0.25      0.12         4\n",
      "\n",
      "    accuracy                           0.73       320\n",
      "   macro avg       0.54      0.58      0.54       320\n",
      "weighted avg       0.76      0.73      0.74       320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## red wine\n",
    "red_wine_svm_model = SVC()\n",
    "red_wine_svm_model.fit(red_wine_X_train,red_wine_y_train)\n",
    "#use the knn model to try to predict the test dataset\n",
    "red_wine_y_pred = red_wine_svm_model.predict(red_wine_X_test)\n",
    "## for better understanding of precition and recall\n",
    "## https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "print('For the Red Wine SVM\\n',classification_report(red_wine_y_test,red_wine_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## improved 0.70 - 0.46 = 0.24 , in comparisson to using all the qualities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the White Wine SVM\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.76      0.67       329\n",
      "           1       0.81      0.55      0.66       619\n",
      "           2       0.13      0.59      0.21        32\n",
      "\n",
      "    accuracy                           0.62       980\n",
      "   macro avg       0.52      0.64      0.52       980\n",
      "weighted avg       0.72      0.62      0.65       980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## white wine\n",
    "white_wine_svm_model = SVC()\n",
    "white_wine_svm_model.fit(white_wine_X_train,white_wine_y_train)\n",
    "\n",
    "white_wine_y_pred = white_wine_svm_model.predict(white_wine_X_test)\n",
    "## for better understanding of precition and recall\n",
    "## https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "print('For the White Wine SVM\\n',classification_report(white_wine_y_test,white_wine_y_pred,zero_division=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## improved 0.61 - 0.45 = 0.24 , in comparisson to using all the qualities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the Red Wine LR\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.84      0.77       140\n",
      "           1       0.79      0.52      0.63       176\n",
      "           2       0.03      0.25      0.05         4\n",
      "\n",
      "    accuracy                           0.66       320\n",
      "   macro avg       0.51      0.54      0.48       320\n",
      "weighted avg       0.74      0.66      0.68       320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## red wine\n",
    "red_wine_lr_model = LogisticRegression()\n",
    "red_wine_lr_model.fit(red_wine_X_train,red_wine_y_train)\n",
    "#use the knn model to try to predict the test dataset\n",
    "red_wine_y_pred = red_wine_lr_model.predict(red_wine_X_test)\n",
    "## for better understanding of precition and recall\n",
    "## https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "print('For the Red Wine LR\\n',classification_report(red_wine_y_test,red_wine_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## improved 0.65 - 0.39 = 0.26 , in comparisson to using all the qualities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the White Wine LR\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.75      0.64       329\n",
      "           1       0.76      0.30      0.43       619\n",
      "           2       0.07      0.62      0.12        32\n",
      "\n",
      "    accuracy                           0.46       980\n",
      "   macro avg       0.46      0.56      0.40       980\n",
      "weighted avg       0.67      0.46      0.49       980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## white wine\n",
    "white_wine_lr_model = LogisticRegression()\n",
    "white_wine_lr_model.fit(white_wine_X_train,white_wine_y_train)\n",
    "\n",
    "white_wine_y_pred = white_wine_lr_model.predict(white_wine_X_test)\n",
    "## for better understanding of precition and recall\n",
    "## https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "print('For the White Wine LR\\n',classification_report(white_wine_y_test,white_wine_y_pred,zero_division=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "## improved 0.51 - 0.49 = 0.02 , in comparisson to using all the qualities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the Red Wine DTC\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.79      0.72       140\n",
      "           1       0.78      0.64      0.70       176\n",
      "           2       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.70       320\n",
      "   macro avg       0.48      0.48      0.47       320\n",
      "weighted avg       0.71      0.70      0.70       320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## red wine\n",
    "red_wine_dtc_model = DecisionTreeClassifier()\n",
    "red_wine_dtc_model.fit(red_wine_X_train,red_wine_y_train)\n",
    "#use the knn model to try to predict the test dataset\n",
    "red_wine_y_pred = red_wine_dtc_model.predict(red_wine_X_test)\n",
    "## for better understanding of precition and recall\n",
    "## https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "print('For the Red Wine DTC\\n',classification_report(red_wine_y_test,red_wine_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## improved 0.72 - 0.51 = 0.21 , in comparisson to using all the qualities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the White Wine dtc\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.59      0.61      0.60       329\n",
      "           1       0.76      0.74      0.75       619\n",
      "           2       0.31      0.38      0.34        32\n",
      "\n",
      "    accuracy                           0.68       980\n",
      "   macro avg       0.55      0.57      0.56       980\n",
      "weighted avg       0.69      0.68      0.69       980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## white wine\n",
    "white_wine_dtc_model = DecisionTreeClassifier()\n",
    "white_wine_dtc_model.fit(white_wine_X_train,white_wine_y_train)\n",
    "\n",
    "white_wine_y_pred = white_wine_dtc_model.predict(white_wine_X_test)\n",
    "## for better understanding of precition and recall\n",
    "## https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "print('For the White Wine dtc\\n',classification_report(white_wine_y_test,white_wine_y_pred,zero_division=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "## improved 0.72 - 0.59 = 0.13 , in comparisson to using all the qualities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the Red Wine rfc\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.84      0.78       140\n",
      "           1       0.83      0.76      0.80       176\n",
      "           2       0.00      0.00      0.00         4\n",
      "\n",
      "    accuracy                           0.78       320\n",
      "   macro avg       0.52      0.53      0.53       320\n",
      "weighted avg       0.78      0.78      0.78       320\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vinic\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\vinic\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "c:\\Users\\vinic\\anaconda3\\envs\\myenv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "## red wine\n",
    "red_wine_rfc_model = RandomForestClassifier()\n",
    "red_wine_rfc_model.fit(red_wine_X_train,red_wine_y_train)\n",
    "#use the knn model to try to predict the test dataset\n",
    "red_wine_y_pred = red_wine_rfc_model.predict(red_wine_X_test)\n",
    "## for better understanding of precition and recall\n",
    "## https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "print('For the Red Wine rfc\\n',classification_report(red_wine_y_test,red_wine_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "## improved 0.70 - 0.50 = 0.20 , in comparisson to using all the qualities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the White Wine rfc\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.72      0.73       329\n",
      "           1       0.82      0.86      0.84       619\n",
      "           2       0.79      0.34      0.48        32\n",
      "\n",
      "    accuracy                           0.79       980\n",
      "   macro avg       0.78      0.64      0.68       980\n",
      "weighted avg       0.79      0.79      0.79       980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## white wine\n",
    "white_wine_rfc_model = RandomForestClassifier()\n",
    "white_wine_rfc_model.fit(white_wine_X_train,white_wine_y_train)\n",
    "\n",
    "white_wine_y_pred = white_wine_rfc_model.predict(white_wine_X_test)\n",
    "## for better understanding of precition and recall\n",
    "## https://en.wikipedia.org/wiki/Precision_and_recall\n",
    "\n",
    "print('For the White Wine rfc\\n',classification_report(white_wine_y_test,white_wine_y_pred,zero_division=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## improved 0.80 - 0.66 = 0.14 , in comparisson to using all the qualities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Red Whine NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_red(X_train, y_train, num_nodes, dropout_prob, lr, batch_size, epochs,X_valid,y_valid):\n",
    "    col = X_train.shape[1]\n",
    "    num_classes = len(np.unique(y_train))\n",
    "    nn_model = tf.keras.Sequential(\n",
    "        [Input(shape=(col,)),\n",
    "         tf.keras.layers.Dense(num_nodes,activation='relu'),\n",
    "         tf.keras.layers.Dropout(dropout_prob),\n",
    "         tf.keras.layers.Dense(num_nodes,activation='relu'),\n",
    "         tf.keras.layers.Dropout(dropout_prob),\n",
    "         tf.keras.layers.Dense(num_classes,activation='softmax')\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    nn_model.compile(optimizer=tf.keras.optimizers.Adam(lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "    history = nn_model.fit(\n",
    "        X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_valid,y_valid),verbose=0\n",
    "        )\n",
    "    return nn_model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from itertools import product\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def find_best_param(X_train, y_train, X_valid, y_valid):\n",
    "    # Seed para reprodutibilidade\n",
    "    tf.random.set_seed(42)\n",
    "    np.random.seed(42)\n",
    "\n",
    "    # Parâmetros para busca\n",
    "    num_nodes_list = [16, 32, 34]\n",
    "    dropout_probs = [0, 0.2]\n",
    "    learning_rates = [0.01, 0.005, 0.001]\n",
    "    batch_sizes = [32, 64, 128]\n",
    "    epochs = 100\n",
    "\n",
    "    # Inicialização do melhor modelo\n",
    "    best_model_params = None\n",
    "    best_f1 = 0\n",
    "    best_accuracy = 0\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    # Loop pelos hiperparâmetros\n",
    "    for num_nodes, dropout_prob, lr, batch_size in product(num_nodes_list, dropout_probs, learning_rates, batch_sizes):\n",
    "        print(f\"\\nTestando: nodes={num_nodes}, dropout={dropout_prob}, lr={lr}, batch={batch_size}\")\n",
    "\n",
    "        # Treina o modelo com os parâmetros atuais\n",
    "        model, history = train_model_red(\n",
    "            X_train, y_train, num_nodes, dropout_prob, lr, batch_size, epochs, X_valid, y_valid\n",
    "        )\n",
    "\n",
    "        # Avaliação no conjunto de validação\n",
    "        val_loss = history.history['val_loss'][-1]\n",
    "        val_accuracy = history.history['val_accuracy'][-1]\n",
    "\n",
    "        y_val_pred_probs = model.predict(X_valid)\n",
    "        y_val_pred = np.argmax(y_val_pred_probs, axis=1)  # <- Aqui é a correção para multi-classe\n",
    "        val_f1 = f1_score(y_valid, y_val_pred, average='weighted')\n",
    "\n",
    "        # Mostra métricas da iteração\n",
    "        print(f\"F1: {val_f1:.4f}, Acc: {val_accuracy:.4f}, Loss: {val_loss:.4f}\")\n",
    "\n",
    "        # Atualiza o melhor conjunto de parâmetros\n",
    "        if (val_f1 > best_f1) or (val_f1 == best_f1 and val_accuracy > best_accuracy) or (val_f1 == best_f1 and val_loss < best_val_loss):\n",
    "            best_f1 = val_f1\n",
    "            best_accuracy = val_accuracy\n",
    "            best_val_loss = val_loss\n",
    "            best_model_params = {\n",
    "                \"num_nodes\": num_nodes,\n",
    "                \"dropout_prob\": dropout_prob,\n",
    "                \"lr\": lr,\n",
    "                \"batch_size\": batch_size\n",
    "            }\n",
    "\n",
    "    return best_model_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step  \n",
      "F1: 0.7069, Acc: 0.7125, Loss: 1.8085\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.6818, Acc: 0.6875, Loss: 1.8949\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.6667, Acc: 0.6687, Loss: 1.9277\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7062, Acc: 0.7063, Loss: 1.5276\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7173, Acc: 0.7219, Loss: 1.4442\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7138, Acc: 0.7188, Loss: 1.0568\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.6984, Acc: 0.7000, Loss: 0.8289\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.7219, Acc: 0.7250, Loss: 0.8387\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7049, Acc: 0.7031, Loss: 0.7093\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.6796, Acc: 0.6812, Loss: 1.2106\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7422, Acc: 0.7469, Loss: 1.2346\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.7222, Acc: 0.7281, Loss: 1.3897\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7056, Acc: 0.7125, Loss: 1.0151\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7088, Acc: 0.7125, Loss: 1.0708\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7086, Acc: 0.7156, Loss: 1.0800\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7238, Acc: 0.7312, Loss: 0.9409\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7283, Acc: 0.7344, Loss: 0.8368\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7144, Acc: 0.7125, Loss: 0.6860\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step  \n",
      "F1: 0.6935, Acc: 0.7000, Loss: 2.9918\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "F1: 0.6624, Acc: 0.6656, Loss: 2.7064\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7094, Acc: 0.7188, Loss: 2.1119\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7005, Acc: 0.7031, Loss: 1.9550\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.6957, Acc: 0.7031, Loss: 1.7501\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step \n",
      "F1: 0.7179, Acc: 0.7219, Loss: 1.5371\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7464, Acc: 0.7500, Loss: 0.9434\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7022, Acc: 0.7063, Loss: 0.8335\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7224, Acc: 0.7281, Loss: 0.8338\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step  \n",
      "F1: 0.7332, Acc: 0.7406, Loss: 1.0860\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7248, Acc: 0.7312, Loss: 1.1772\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.6975, Acc: 0.7063, Loss: 1.1427\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7005, Acc: 0.7063, Loss: 1.2782\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7350, Acc: 0.7406, Loss: 1.2607\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step \n",
      "F1: 0.7119, Acc: 0.7188, Loss: 1.2341\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7271, Acc: 0.7312, Loss: 0.8372\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7390, Acc: 0.7469, Loss: 0.8315\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step  \n",
      "F1: 0.7301, Acc: 0.7344, Loss: 0.7358\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.6819, Acc: 0.6875, Loss: 2.9485\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.6962, Acc: 0.7031, Loss: 2.4204\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.6922, Acc: 0.6969, Loss: 2.1024\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.6981, Acc: 0.7031, Loss: 2.3175\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.6884, Acc: 0.7000, Loss: 2.2131\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "F1: 0.7134, Acc: 0.7156, Loss: 1.4300\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7588, Acc: 0.7625, Loss: 0.9035\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7400, Acc: 0.7437, Loss: 0.8925\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "F1: 0.7043, Acc: 0.7094, Loss: 0.8070\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7367, Acc: 0.7437, Loss: 1.1558\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7236, Acc: 0.7312, Loss: 1.0162\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "F1: 0.7160, Acc: 0.7219, Loss: 1.2234\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "F1: 0.7391, Acc: 0.7437, Loss: 0.8763\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7215, Acc: 0.7312, Loss: 1.3318\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "F1: 0.7263, Acc: 0.7312, Loss: 1.1090\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step \n",
      "F1: 0.7339, Acc: 0.7406, Loss: 0.9698\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "F1: 0.7247, Acc: 0.7312, Loss: 0.8924\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step  \n",
      "F1: 0.7410, Acc: 0.7437, Loss: 0.7266\n"
     ]
    }
   ],
   "source": [
    "# red wine\n",
    "best_model_params = find_best_param(red_wine_X_train, red_wine_y_train,red_wine_X_valid,red_wine_y_valid)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_nodes': 34, 'dropout_prob': 0, 'lr': 0.001, 'batch_size': 32}\n"
     ]
    }
   ],
   "source": [
    "print(best_model_params)\n",
    "model,history = train_model_red(red_wine_X_train, red_wine_y_train, best_model_params['num_nodes'], best_model_params['dropout_prob'], best_model_params['lr'], best_model_params['batch_size'], epochs,red_wine_X_valid,red_wine_y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.80      0.74       140\n",
      "           1       0.80      0.69      0.74       176\n",
      "           2       0.25      0.25      0.25         4\n",
      "\n",
      "    accuracy                           0.73       320\n",
      "   macro avg       0.58      0.58      0.58       320\n",
      "weighted avg       0.74      0.73      0.73       320\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "# Realizar previsões no conjunto de teste\n",
    "y_pred_probs = model.predict(red_wine_X_test)\n",
    "\n",
    "# Obter a classe com maior probabilidade\n",
    "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Avaliar o modelo\n",
    "print(classification_report(red_wine_y_test, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "## improved 0.73 - 0.47 = 0.26 , in comparisson to using all the qualities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# White Wine NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_white(X_train, y_train, num_nodes, dropout_prob, lr, batch_size, epochs,X_valid,y_valid):\n",
    "    col = X_train.shape[1]\n",
    "    num_classes = len(np.unique(y_train))\n",
    "    nn_model = tf.keras.Sequential(\n",
    "        [Input(shape=(col,)),\n",
    "         tf.keras.layers.Dense(num_nodes,activation='relu'),\n",
    "         tf.keras.layers.Dropout(dropout_prob),\n",
    "         tf.keras.layers.Dense(num_nodes,activation='relu'),\n",
    "         tf.keras.layers.Dropout(dropout_prob),\n",
    "         tf.keras.layers.Dense(num_classes,activation='softmax')\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    nn_model.compile(optimizer=tf.keras.optimizers.Adam(lr),loss='sparse_categorical_crossentropy',metrics=['accuracy'])\n",
    "    history = nn_model.fit(\n",
    "        X_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(X_valid,y_valid),verbose=0\n",
    "        )\n",
    "    return nn_model, history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.6648, Acc: 0.6571, Loss: 1.2768\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.6846, Acc: 0.6735, Loss: 1.2729\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.6840, Acc: 0.6765, Loss: 1.2308\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "F1: 0.6942, Acc: 0.6867, Loss: 1.1980\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.6766, Acc: 0.6694, Loss: 1.0464\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.7013, Acc: 0.6939, Loss: 0.8284\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "F1: 0.6679, Acc: 0.6541, Loss: 0.8300\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "F1: 0.6871, Acc: 0.6735, Loss: 0.7600\n",
      "\n",
      "Testando: nodes=16, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "F1: 0.6291, Acc: 0.6082, Loss: 0.8178\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.5588, Acc: 0.5408, Loss: 0.7971\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.5908, Acc: 0.5745, Loss: 0.8292\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "F1: 0.5703, Acc: 0.5480, Loss: 0.8445\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.5495, Acc: 0.5327, Loss: 0.9246\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "F1: 0.5433, Acc: 0.5296, Loss: 0.8178\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.5633, Acc: 0.5418, Loss: 0.8479\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "F1: 0.5428, Acc: 0.5235, Loss: 0.8112\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.5410, Acc: 0.5276, Loss: 0.8300\n",
      "\n",
      "Testando: nodes=16, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "F1: 0.5213, Acc: 0.5051, Loss: 0.8500\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "F1: 0.6880, Acc: 0.6816, Loss: 1.9004\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "F1: 0.7130, Acc: 0.7071, Loss: 1.9498\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "F1: 0.7124, Acc: 0.7102, Loss: 1.7298\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.7059, Acc: 0.7031, Loss: 1.6853\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "F1: 0.7131, Acc: 0.7102, Loss: 1.6820\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "F1: 0.7061, Acc: 0.7020, Loss: 1.3408\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "F1: 0.7163, Acc: 0.7122, Loss: 0.9017\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "F1: 0.7025, Acc: 0.6959, Loss: 0.8683\n",
      "\n",
      "Testando: nodes=32, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "F1: 0.6967, Acc: 0.6888, Loss: 0.7645\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "F1: 0.6350, Acc: 0.6184, Loss: 0.7562\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "F1: 0.6593, Acc: 0.6500, Loss: 0.7222\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.6359, Acc: 0.6255, Loss: 0.7419\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.6614, Acc: 0.6520, Loss: 0.7106\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.6325, Acc: 0.6173, Loss: 0.7402\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.6570, Acc: 0.6408, Loss: 0.7473\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.6614, Acc: 0.6480, Loss: 0.7329\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "F1: 0.6282, Acc: 0.6112, Loss: 0.7767\n",
      "\n",
      "Testando: nodes=32, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.5914, Acc: 0.5694, Loss: 0.7801\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "F1: 0.7210, Acc: 0.7184, Loss: 2.4002\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "F1: 0.7144, Acc: 0.7102, Loss: 2.0227\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "F1: 0.7258, Acc: 0.7255, Loss: 1.6336\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.6964, Acc: 0.6878, Loss: 1.7590\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.7079, Acc: 0.7051, Loss: 1.6979\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "F1: 0.7216, Acc: 0.7194, Loss: 1.2328\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "F1: 0.7050, Acc: 0.7010, Loss: 0.9515\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "F1: 0.7269, Acc: 0.7214, Loss: 0.8385\n",
      "\n",
      "Testando: nodes=34, dropout=0, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.7031, Acc: 0.6949, Loss: 0.7836\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "F1: 0.6453, Acc: 0.6296, Loss: 0.7543\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.6235, Acc: 0.6122, Loss: 0.7536\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.01, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "F1: 0.6497, Acc: 0.6337, Loss: 0.7259\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.6690, Acc: 0.6582, Loss: 0.7382\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "F1: 0.6517, Acc: 0.6378, Loss: 0.7319\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.005, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "F1: 0.6758, Acc: 0.6612, Loss: 0.7304\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=32\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n",
      "F1: 0.6611, Acc: 0.6469, Loss: 0.7220\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=64\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "F1: 0.6393, Acc: 0.6194, Loss: 0.7650\n",
      "\n",
      "Testando: nodes=34, dropout=0.2, lr=0.001, batch=128\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "F1: 0.5934, Acc: 0.5735, Loss: 0.7860\n"
     ]
    }
   ],
   "source": [
    "# white wine\n",
    "\n",
    "\n",
    "best_model_params = find_best_param(white_wine_X_train, white_wine_y_train,white_wine_X_valid,white_wine_y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_nodes': 34, 'dropout_prob': 0, 'lr': 0.001, 'batch_size': 64}\n"
     ]
    }
   ],
   "source": [
    "print(best_model_params)\n",
    "\n",
    "model,history = train_model_white(white_wine_X_train, white_wine_y_train, best_model_params['num_nodes'], best_model_params['dropout_prob'], best_model_params['lr'], best_model_params['batch_size'], epochs,white_wine_X_valid,white_wine_y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      0.67      0.64       329\n",
      "           1       0.78      0.73      0.75       619\n",
      "           2       0.31      0.41      0.35        32\n",
      "\n",
      "    accuracy                           0.70       980\n",
      "   macro avg       0.57      0.60      0.58       980\n",
      "weighted avg       0.71      0.70      0.70       980\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Realizar previsões no conjunto de teste\n",
    "y_pred_probs = model.predict(white_wine_X_test)\n",
    "\n",
    "# Obter a classe com maior probabilidade\n",
    "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Avaliar o modelo\n",
    "print(classification_report(white_wine_y_test, y_pred_classes,zero_division=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## improved 0.61 - 0.46 = 0.26 , in comparisson to using all the qualities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
